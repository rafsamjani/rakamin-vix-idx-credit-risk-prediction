{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning, Feature Engineering, and Imbalance Handling\n",
    "\n",
    "In this notebook, we'll explore essential data science techniques including data cleaning, feature engineering, and handling imbalanced data - all crucial steps in the machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction to Data Cleaning](#introduction-to-data-cleaning)\n",
    "2. [Handling Missing Data](#handling-missing-data)\n",
    "3. [Detecting and Handling Outliers](#detecting-and-handling-outliers)\n",
    "4. [Dealing with Duplicates](#dealing-with-duplicates)\n",
    "5. [Data Type Conversion](#data-type-conversion)\n",
    "6. [String Manipulation](#string-manipulation)\n",
    "7. [Feature Engineering](#feature-engineering)\n",
    "8. [Handling Imbalanced Data](#handling-imbalanced-data)\n",
    "9. [Practical Example with Lending Club Data](#practical-example)\n",
    "10. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create a realistic Lending Club dataset with intentional issues for demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 8000\n",
    "\n",
    "# Create dataset with realistic characteristics for credit risk\n",
    "data = {\n",
    "    'loan_id': range(1, n_samples + 1),\n",
    "    'loan_amnt': np.random.normal(15000, 8000, n_samples),\n",
    "    'int_rate': np.random.normal(12, 4, n_samples),\n",
    "    'installment': np.random.normal(400, 200, n_samples),\n",
    "    'annual_inc': np.random.normal(70000, 35000, n_samples),\n",
    "    'dti': np.random.normal(15, 8, n_samples),\n",
    "    'fico_score': np.random.normal(680, 80, n_samples),\n",
    "    'emp_length': np.random.gamma(2, 2, n_samples),\n",
    "    'loan_status': np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),  # 15% default rate\n",
    "    'grade': pd.cut(np.random.normal(680, 80, n_samples), \n",
    "                    bins=[0, 580, 620, 660, 700, 740, 780, 850], \n",
    "                    labels=['G', 'F', 'E', 'D', 'C', 'B', 'A']),\n",
    "    'home_ownership': np.random.choice(['MORTGAGE', 'OWN', 'RENT', 'OTHER'], n_samples, p=[0.4, 0.2, 0.35, 0.05]),\n",
    "    'verification_status': np.random.choice(['Verified', 'Not Verified', 'Source Verified'], n_samples, p=[0.35, 0.5, 0.15]),\n",
    "    'purpose': np.random.choice(['debt_consolidation', 'credit_card', 'home_improvement', 'major_purchase', \n",
    "                                'small_business', 'other', 'vacation', 'car', 'moving', 'medical'], \n",
    "                               n_samples, p=[0.25, 0.2, 0.15, 0.1, 0.08, 0.07, 0.05, 0.05, 0.03, 0.02])\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce some intentional data quality issues for demonstration\n",
    "\n",
    "# 1. Missing values\n",
    "missing_indices = np.random.choice(df.index, size=int(0.05 * n_samples), replace=False)\n",
    "df.loc[missing_indices[:int(0.02 * n_samples)], 'annual_inc'] = np.nan\n",
    "df.loc[missing_indices[int(0.02 * n_samples):int(0.035 * n_samples)], 'dti'] = np.nan\n",
    "df.loc[missing_indices[int(0.035 * n_samples):], 'fico_score'] = np.nan\n",
    "\n",
    "# 2. Outliers\n",
    "outlier_indices = np.random.choice(df.index, size=int(0.02 * n_samples), replace=False)\n",
    "df.loc[outlier_indices[:int(0.01 * n_samples)], 'annual_inc'] = np.random.uniform(500000, 1000000, int(0.01 * n_samples))\n",
    "df.loc[outlier_indices[int(0.01 * n_samples):], 'fico_score'] = np.random.uniform(150, 250, int(0.01 * n_samples))\n",
    "\n",
    "# 3. Inconsistent values\n",
    "inconsistent_indices = np.random.choice(df.index, size=int(0.01 * n_samples), replace=False)\n",
    "df.loc[inconsistent_indices, 'emp_length'] = np.random.uniform(-5, -1, int(0.01 * n_samples))  # Negative employment length\n",
    "\n",
    "# 4. Duplicates\n",
    "duplicate_indices = np.random.choice(df.index, size=int(0.005 * n_samples), replace=False)\n",
    "df_duplicate = df.loc[duplicate_indices].copy()\n",
    "df_duplicate.index = range(len(df), len(df) + len(df_duplicate))\n",
    "df = pd.concat([df, df_duplicate], ignore_index=True)\n",
    "\n",
    "# 5. Invalid ranges\n",
    "df.loc[df['loan_amnt'] < 0, 'loan_amnt'] = np.abs(df.loc[df['loan_amnt'] < 0, 'loan_amnt'])\n",
    "df.loc[df['int_rate'] < 0, 'int_rate'] = np.abs(df.loc[df['int_rate'] < 0, 'int_rate'])\n",
    "df.loc[df['int_rate'] > 35, 'int_rate'] = 35  # Cap at realistic max\n",
    "df.loc[df['dti'] < 0, 'dti'] = 0\n",
    "df.loc[df['fico_score'] > 850, 'fico_score'] = 850\n",
    "df.loc[df['fico_score'] < 300, 'fico_score'] = 300\n",
    "\n",
    "print(\"Data Cleaning, Feature Engineering, and Imbalance Handling\")\n",
    "print(\"Simulated Lending Club Dataset with intentional data quality issues\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Default Rate: {(df['loan_status'] == 1).mean():.2%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data Cleaning\n",
    "\n",
    "Data cleaning is the process of detecting and correcting (or removing) corrupt, inaccurate, or irrelevant parts of a dataset. It's a critical step in the data science pipeline that ensures data quality and reliability."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initial data quality assessment\n",
    "print(\"Initial Data Quality Assessment:\")\n",
    "\n",
    "# Overview of the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(f\"\\nMissing values:\\n{missing_values[missing_values > 0]}\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows: {duplicates}\")\n",
    "\n",
    "# Data types\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "\n",
    "# Basic statistics with potential issues highlighted\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Identify potential issues\n",
    "print(f\"\\nPotential Data Quality Issues Identified:\")\n",
    "\n",
    "# Check for negative values in fields that shouldn't be negative\n",
    "negative_checks = {\n",
    "    'loan_amnt': (df['loan_amnt'] < 0).sum(),\n",
    "    'annual_inc': (df['annual_inc'] < 0).sum(),\n",
    "    'dti': (df['dti'] < 0).sum(),\n",
    "    'emp_length': (df['emp_length'] < 0).sum()\n",
    "}\n",
    "print(f\"Negative values in inappropriate fields: {negative_checks}\")\n",
    "\n",
    "# Check for unrealistic values\n",
    "unrealistic_checks = {\n",
    "    'fico_score_out_of_range': ((df['fico_score'] < 300) | (df['fico_score'] > 850)).sum(),\n",
    "    'int_rate_too_high': (df['int_rate'] > 35).sum(),\n",
    "    'dti_too_high': (df['dti'] > 100).sum()\n",
    "}\n",
    "print(f\"Unrealistic values: {unrealistic_checks}\")\n",
    "\n",
    "# Visualize data quality issues\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Missing values heatmap\n",
    "sns.heatmap(df.isnull(), yticklabels=False, cbar=True, cmap='viridis', ax=axes[0])\n",
    "axes[0].set_title('Missing Values Heatmap')\n",
    "\n",
    "# 2. Distribution of numerical features to identify outliers\n",
    "numerical_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score', 'emp_length']\n",
    "axes[1].boxplot([df[col] for col in numerical_cols], labels=numerical_cols)\n",
    "axes[1].set_title('Distribution of Numerical Features (Boxplot)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Missing value counts\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "if len(missing_counts) > 0:\n",
    "    axes[2].bar(missing_counts.index, missing_counts.values, color='red', alpha=0.7)\n",
    "    axes[2].set_title('Missing Values Count')\n",
    "    axes[2].set_ylabel('Count')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, 'No missing values', \n",
    "                 horizontalalignment='center', verticalalignment='center',\n",
    "                 transform=axes[2].transAxes)\n",
    "    axes[2].set_title('Missing Values Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Make a copy of the original dataset for comparison\n",
    "df_original = df.copy()\n",
    "print(f\"\\nOriginal dataset shape: {df_original.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "Missing data is a common problem in real-world datasets. We'll explore different strategies to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Handling Missing Data\n",
    "print(\"Handling Missing Data:\")\n",
    "\n",
    "# Check missing values again\n",
    "missing_data = df.isnull().sum().sort_values(ascending=False)\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percent\n",
    "})\n",
    "\n",
    "print(\"Missing Data Summary:\")\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# Visualization of missing data\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "missing_subset = missing_df[missing_df['Missing Count'] > 0]\n",
    "plt.bar(missing_subset.index, missing_subset['Missing Count'], color='skyblue')\n",
    "plt.title('Missing Values Count')\n",
    "plt.ylabel('Number of Missing Values')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(missing_subset.index, missing_subset['Missing Percentage'], color='lightcoral')\n",
    "plt.title('Missing Values Percentage')\n",
    "plt.ylabel('Percentage of Missing Values (%)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Different strategies for handling missing data\n",
    "df_missing_treatment = df.copy()\n",
    "\n",
    "print(f\"\\nDifferent Strategies for Handling Missing Data:\")\n",
    "\n",
    "# 1. Drop rows with missing values\n",
    "df_dropped = df_missing_treatment.dropna()\n",
    "print(f\"1. Drop rows with missing values: {df_dropped.shape[0]} rows ({len(df_dropped)/len(df_missing_treatment)*100:.2f}% of original)\")\n",
    "\n",
    "# 2. Mean/median/mode imputation\n",
    "df_imputed = df_missing_treatment.copy()\n",
    " \n",
    "# For numerical columns, use median (more robust to outliers)\n",
    "for col in ['annual_inc', 'dti', 'fico_score']:\n",
    "    if df_imputed[col].isnull().any():\n",
    "        median_val = df_imputed[col].median()\n",
    "        df_imputed[f'{col}_imputed'] = df_imputed[col].isnull().astype(int)  # Create indicator\n",
    "        df_imputed[col].fillna(median_val, inplace=True)\n",
    "        print(f\"   Imputed {col} with median: {median_val:.2f}\")\n",
    "\n",
    "print(f\"2. Mean/median imputation: {df_imputed.shape[0]} rows (no rows lost)\")\n",
    "\n",
    "# 3. KNN imputation for numerical variables\n",
    "df_knn = df_missing_treatment.copy()\n",
    "\n",
    "# Prepare data for KNN imputation (only numerical features)\n",
    "numerical_features = ['loan_amnt', 'int_rate', 'installment', 'annual_inc', 'dti', 'fico_score', 'emp_length']\n",
    "df_numerical = df_knn[numerical_features].copy()\n",
    "\n",
    "# Apply KNN imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_numerical_imputed = pd.DataFrame(\n",
    "    knn_imputer.fit_transform(df_numerical),\n",
    "    columns=numerical_features,\n",
    "    index=df_numerical.index\n",
    ")\n",
    "\n",
    "# Update the original dataframe\n",
    "for col in numerical_features:\n",
    "    df_knn[col] = df_numerical_imputed[col]\n",
    "\n",
    "print(f\"3. KNN imputation: {df_knn.shape[0]} rows (no rows lost)\")\n",
    "\n",
    "# 4. Advanced imputation - using machine learning to predict missing values\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df_ml_imputed = df_missing_treatment.copy()\n",
    "\n",
    "# Example: Impute 'annual_inc' based on other features\n",
    "if df_ml_imputed['annual_inc'].isnull().any():\n",
    "    # Features to predict annual income\n",
    "    features_for_income = ['loan_amnt', 'fico_score', 'int_rate', 'dti', 'emp_length']\n",
    "    \n",
    "    # Split data into known and unknown income\n",
    "    known_income = df_ml_imputed[df_ml_imputed['annual_inc'].notna()]\n",
    "    unknown_income = df_ml_imputed[df_ml_imputed['annual_inc'].isna()]\n",
    "    \n",
    "    if len(unknown_income) > 0 and len(known_income) > 0:\n",
    "        # Train model on known values\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(known_income[features_for_income], known_income['annual_inc'])\n",
    "        \n",
    "        # Predict missing values\n",
    "        predicted_income = model.predict(unknown_income[features_for_income])\n",
    "        df_ml_imputed.loc[df_ml_imputed['annual_inc'].isna(), 'annual_inc'] = predicted_income\n",
    "        \n",
    "        print(f\"4. ML-based imputation for annual_inc: {len(unknown_income)} values imputed\")\n",
    "\n",
    "# Visualization of imputation effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Original distribution of annual_inc\n",
    "axes[0, 0].hist(df['annual_inc'].dropna(), bins=50, alpha=0.7, label='Original (non-missing)', color='blue')\n",
    "axes[0, 0].set_title('Original Distribution of Annual Income')\n",
    "axes[0, 0].set_xlabel('Annual Income')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# After median imputation\n",
    "axes[0, 1].hist(df_imputed['annual_inc'], bins=50, alpha=0.7, label='After Median Imputation', color='orange')\n",
    "axes[0, 1].set_title('After Median Imputation')\n",
    "axes[0, 1].set_xlabel('Annual Income')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# After KNN imputation\n",
    "axes[0, 2].hist(df_knn['annual_inc'], bins=50, alpha=0.7, label='After KNN Imputation', color='green')\n",
    "axes[0, 2].set_title('After KNN Imputation')\n",
    "axes[0, 2].set_xlabel('Annual Income')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Missing value indicators\n",
    "if 'annual_inc_imputed' in df_imputed.columns:\n",
    "    missing_indicators = df_imputed['annual_inc_imputed'].value_counts()\n",
    "    axes[0, 3].pie(missing_indicators.values, labels=['Not Missing', 'Imputed'], autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 3].set_title('Distribution of Imputed Values')\n",
    "else:\n",
    "    axes[0, 3].text(0.5, 0.5, 'No missing indicators', \n",
    "                    horizontalalignment='center', verticalalignment='center',\n",
    "                    transform=axes[0, 3].transAxes)\n",
    "    axes[0, 3].set_title('Imputation Indicators')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Update working dataset to df_imputed for subsequent steps\n",
    "df = df_imputed.copy()\n",
    "print(f\"\\nUpdated dataset shape after missing value treatment: {df.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting and Handling Outliers\n",
    "\n",
    "Outliers can significantly impact model performance. We'll explore various methods to detect and handle outliers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Detecting and Handling Outliers\n",
    "print(\"Detecting and Handling Outliers:\")\n",
    "\n",
    "# Visualize outliers before treatment\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "numerical_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score', 'emp_length']\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    # Boxplot to visualize outliers\n",
    "    axes[i].boxplot(df[col])\n",
    "    axes[i].set_title(f'{col} - Box Plot')\n",
    "    axes[i].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical methods for outlier detection\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers.index, lower_bound, upper_bound\n",
    "\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
    "    z_scores = np.abs(stats.zscore(data[column].dropna()))\n",
    "    outlier_indices = data.index[np.abs(z_scores) > threshold]\n",
    "    return outlier_indices\n",
    "\n",
    "# Apply outlier detection methods\n",
    "outlier_info = {}\n",
    "\n",
    "for col in ['loan_amnt', 'annual_inc', 'fico_score', 'dti']:\n",
    "    iqr_outliers, lower, upper = detect_outliers_iqr(df, col)\n",
    "    zscore_outliers = detect_outliers_zscore(df, col)\n",
    "    \n",
    "    outlier_info[col] = {\n",
    "        'iqr_count': len(iqr_outliers),\n",
    "        'zscore_count': len(zscore_outliers),\n",
    "        'iqr_percentage': len(iqr_outliers) / len(df) * 100,\n",
    "        'zscore_percentage': len(zscore_outliers) / len(df) * 100,\n",
    "        'iqr_bounds': (lower, upper)\n",
    "    }\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_info).T\n",
    "print(\"Outlier Detection Results:\")\n",
    "print(outlier_df.round(2))\n",
    "\n",
    "# Isolation Forest for outlier detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Prepare data for Isolation Forest (only numerical)\n",
    "numerical_data = df[['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score', 'emp_length']].copy()\n",
    "\n",
    "# Remove any remaining NaN values\n",
    "numerical_data = numerical_data.dropna()\n",
    "\n",
    "# Apply Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "outlier_labels = iso_forest.fit_predict(numerical_data)\n",
    "\n",
    "# Add outlier labels to the original dataframe\n",
    "df['outlier_iso_forest'] = 0\n",
    "df.loc[numerical_data.index, 'outlier_iso_forest'] = (outlier_labels == -1).astype(int)\n",
    "\n",
    "iso_outlier_count = df['outlier_iso_forest'].sum()\n",
    "print(f\"\\nIsolation Forest detected {iso_outlier_count} outliers ({iso_outlier_count/len(df)*100:.2f}% of data)\")\n",
    "\n",
    "# Different strategies for handling outliers\n",
    "print(f\"\\nStrategies for Handling Outliers:\")\n",
    "\n",
    "# 1. Remove outliers\n",
    "df_no_outliers = df.copy()\n",
    "for col in ['loan_amnt', 'annual_inc', 'fico_score', 'dti']:\n",
    "    _, lower, upper = detect_outliers_iqr(df_no_outliers, col)\n",
    "    df_no_outliers = df_no_outliers[(df_no_outliers[col] >= lower) & (df_no_outliers[col] <= upper)]\n",
    "print(f\"1. Remove outliers using IQR: {df_no_outliers.shape[0]} rows remain ({len(df_no_outliers)/len(df)*100:.2f}% of original)\")\n",
    "\n",
    "# 2. Cap outliers (Winsorization)\n",
    "df_capped = df.copy()\n",
    "for col in ['loan_amnt', 'annual_inc', 'fico_score', 'dti']:\n",
    "    _, lower, upper = detect_outliers_iqr(df_capped, col)\n",
    "    df_capped[col] = df_capped[col].clip(lower=lower, upper=upper)\n",
    "print(f\"2. Cap outliers (Winsorization): {df_capped.shape[0]} rows (no rows lost)\")\n",
    "\n",
    "# 3. Transform the data (log transformation)\n",
    "df_transformed = df.copy()\n",
    "\n",
    "# Apply log transformation to highly skewed variables\n",
    "for col in ['loan_amnt', 'annual_inc']:\n",
    "    df_transformed[f'log_{col}'] = np.log1p(df_transformed[col])  # log(1+x) to handle zero values\n",
    "print(f\"3. Apply log transformation: New features created (log_loan_amnt, log_annual_inc)\")\n",
    "\n",
    "# Visualization of outlier treatment\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Original distribution\n",
    "axes[0].hist(df['annual_inc'], bins=50, alpha=0.7, label='Original', color='blue')\n",
    "axes[0].set_title('Original Annual Income Distribution')\n",
    "axes[0].set_xlabel('Annual Income')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# After removing outliers\n",
    "axes[1].hist(df_no_outliers['annual_inc'], bins=50, alpha=0.7, label='After Removing Outliers', color='red')\n",
    "axes[1].set_title('After Removing Outliers')\n",
    "axes[1].set_xlabel('Annual Income')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# After capping outliers\n",
    "axes[2].hist(df_capped['annual_inc'], bins=50, alpha=0.7, label='After Capping Outliers', color='green')\n",
    "axes[2].set_title('After Capping Outliers')\n",
    "axes[2].set_xlabel('Annual Income')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "\n",
    "# Log transformed\n",
    "if 'log_annual_inc' in df_transformed.columns:\n",
    "    axes[3].hist(df_transformed['log_annual_inc'], bins=50, alpha=0.7, label='Log Transformed', color='purple')\n",
    "    axes[3].set_title('Log Transformed Annual Income')\n",
    "    axes[3].set_xlabel('Log(Annual Income)')\n",
    "    axes[3].set_ylabel('Frequency')\n",
    "\n",
    "# Boxplot comparison\n",
    "box_data = [df['annual_inc'], df_capped['annual_inc']]\n",
    "box_labels = ['Original', 'Capped']\n",
    "axes[4].boxplot(box_data, labels=box_labels)\n",
    "axes[4].set_title('Boxplot Comparison (Original vs Capped)')\n",
    "axes[4].set_ylabel('Annual Income')\n",
    "\n",
    "# Outlier detection visualization\n",
    "normal_points = df[df['outlier_iso_forest'] == 0]\n",
    "outlier_points = df[df['outlier_iso_forest'] == 1]\n",
    "axes[5].scatter(normal_points['fico_score'], normal_points['int_rate'], \n",
    "                alpha=0.5, label='Normal', color='blue')\n",
    "axes[5].scatter(outlier_points['fico_score'], outlier_points['int_rate'], \n",
    "                alpha=0.7, label='Outliers', color='red')\n",
    "axes[5].set_title('Isolation Forest: Normal vs Outliers')\n",
    "axes[5].set_xlabel('FICO Score')\n",
    "axes[5].set_ylabel('Interest Rate')\n",
    "axes[5].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Update working dataset to df_capped for subsequent steps\n",
    "df = df_capped.copy()\n",
    "print(f\"\\nUpdated dataset shape after outlier treatment: {df.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Duplicates\n",
    "\n",
    "Duplicate records can skew analysis and model performance. We'll explore methods to identify and handle duplicates."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dealing with Duplicates\n",
    "print(\"Dealing with Duplicates:\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Check for duplicate values in specific columns\n",
    "for col in ['loan_id']:\n",
    "    duplicate_ids = df[col].duplicated().sum()\n",
    "    print(f\"Number of duplicate {col}: {duplicate_ids}\")\n",
    "\n",
    "# Display duplicate information\n",
    "if duplicate_rows > 0:\n",
    "    # Show the duplicated rows\n",
    "    duplicated_data = df[df.duplicated(keep=False)].sort_values(by='loan_id')\n",
    "    print(f\"\\nFirst few duplicated rows:\\n{duplicated_data.head(10)}\")\n",
    "    \n",
    "    # Visualize duplicates\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(['Unique', 'Duplicate'], [len(df) - duplicate_rows, duplicate_rows], \n",
    "            color=['lightgreen', 'orange'])\n",
    "    plt.title('Distribution of Unique vs Duplicate Rows')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Add value labels\n",
    "    plt.text(0, len(df) - duplicate_rows + len(df)*0.01, f'{len(df) - duplicate_rows}', \n",
    "             ha='center')\n",
    "    plt.text(1, duplicate_rows + len(df)*0.01, f'{duplicate_rows}', ha='center')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    duplicate_counts = df.duplicated().value_counts()\n",
    "    labels = ['Unique', 'Duplicate']\n",
    "    plt.pie(duplicate_counts.values, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "    plt.title('Proportion of Unique vs Duplicate Rows')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Different strategies for handling duplicates\n",
    "print(f\"\\nStrategies for Handling Duplicates:\")\n",
    "\n",
    "# 1. Remove all duplicates\n",
    "df_deduplicated = df.drop_duplicates()\n",
    "print(f\"1. Remove all duplicates: {df_deduplicated.shape[0]} rows remain\")\n",
    "\n",
    "# 2. Keep first occurrence (default behavior)\n",
    "df_keep_first = df.drop_duplicates(keep='first')\n",
    "print(f\"2. Keep first occurrence: {df_keep_first.shape[0]} rows remain\")\n",
    "\n",
    "# 3. Keep last occurrence\n",
    "df_keep_last = df.drop_duplicates(keep='last')\n",
    "print(f\"3. Keep last occurrence: {df_keep_last.shape[0]} rows remain\")\n",
    "\n",
    "# 4. Remove duplicates based on specific subset of columns\n",
    "df_subset_dedup = df.drop_duplicates(subset=['loan_amnt', 'fico_score', 'annual_inc'])\n",
    "print(f\"4. Remove duplicates based on loan_amnt, fico_score, annual_inc: {df_subset_dedup.shape[0]} rows remain\")\n",
    "\n",
    "# Visualization after deduplication\n",
    "original_size = len(df)\n",
    "dedup_size = len(df_deduplicated)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "methods = ['Original', 'After Deduplication']\n",
    "counts = [original_size, dedup_size]\n",
    "bars = plt.bar(methods, counts, color=['skyblue', 'lightgreen'])\n",
    "plt.title('Dataset Size Comparison: Before vs After Deduplication')\n",
    "plt.ylabel('Number of Rows')\n",
    "\n",
    "# Add value labels\n",
    "for bar, count in zip(bars, counts):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + original_size*0.01, \n",
    "             f'{count}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Calculate reduction\n",
    "reduction = original_size - dedup_size\n",
    "reduction_pct = reduction / original_size * 100\n",
    "print(f\"\\nDeduplication removed {reduction} rows ({reduction_pct:.2f}% of original dataset)\")\n",
    "\n",
    "# Update working dataset to df_deduplicated for subsequent steps\n",
    "df = df_deduplicated.copy()\n",
    "print(f\"Updated dataset shape after deduplication: {df.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type Conversion\n",
    "\n",
    "Proper data types are crucial for memory efficiency and correct operations. We'll cover how to convert and optimize data types."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data Type Conversion\n",
    "print(\"Data Type Conversion:\")\n",
    "\n",
    "# Current data types\n",
    "print(\"Current Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMemory usage before conversion: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Check unique values in categorical columns for possible optimization\n",
    "categorical_cols = ['grade', 'home_ownership', 'verification_status', 'purpose']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    unique_vals = df[col].nunique()\n",
    "    total_vals = len(df[col])\n",
    "    print(f\"{col}: {unique_vals} unique values out of {total_vals} total ({unique_vals/total_vals*100:.2f}%)\")\n",
    "\n",
    "# Convert categorical columns to 'category' type for memory efficiency\n",
    "df_optimized = df.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_optimized[col] = df_optimized[col].astype('category')\n",
    "\n",
    "# Optimize numerical columns\n",
    "# For integer columns, use the smallest possible integer type\n",
    "if 'loan_status' in df_optimized.columns:\n",
    "    df_optimized['loan_status'] = pd.to_numeric(df_optimized['loan_status'], downcast='integer')\n",
    "\n",
    "# For float columns, consider if float32 is sufficient instead of float64\n",
    "float_cols = ['loan_amnt', 'int_rate', 'installment', 'annual_inc', 'dti', 'fico_score', 'emp_length']\n",
    "for col in float_cols:\n",
    "    if col in df_optimized.columns:\n",
    "        # Only convert if it doesn't lose precision\n",
    "        original_dtype = df_optimized[col].dtype\n",
    "        converted = pd.to_numeric(df_optimized[col], downcast='float')\n",
    "        if converted.dtype != original_dtype:\n",
    "            df_optimized[col] = converted\n",
    "            print(f\"Converted {col} from {original_dtype} to {converted.dtype}\")\n",
    "\n",
    "# Memory usage comparison\n",
    "print(f\"\\nMemory usage after optimization: {df_optimized.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory saved: {(df.memory_usage(deep=True).sum() - df_optimized.memory_usage(deep=True).sum()) / 1024**2:.2f} MB ({((df.memory_usage(deep=True).sum() - df_optimized.memory_usage(deep=True).sum()) / df.memory_usage(deep=True).sum()) * 100:.2f}% reduction)\")\n",
    "\n",
    "# Check for columns that should be dates but are not\n",
    "print(f\"\\nLooking for date-related columns that need conversion...\")\n",
    "\n",
    "# If we had date columns, here's how we would convert them\n",
    "# Example: Adding a date column to demonstrate\n",
    "df_optimized['issue_date'] = pd.date_range(start='2010-01-01', periods=len(df_optimized), freq='H')[:len(df_optimized)]\n",
    "print(f\"Added example date column 'issue_date' with type: {df_optimized['issue_date'].dtype}\")\n",
    "\n",
    "# Convert to proper datetime if needed\n",
    "df_optimized['issue_date'] = pd.to_datetime(df_optimized['issue_date'])\n",
    "\n",
    "# Extract meaningful date features\n",
    "df_optimized['issue_year'] = df_optimized['issue_date'].dt.year\n",
    "df_optimized['issue_month'] = df_optimized['issue_date'].dt.month\n",
    "df_optimized['issue_day_of_week'] = df_optimized['issue_date'].dt.dayofweek\n",
    "df_optimized['issue_day_of_year'] = df_optimized['issue_date'].dt.dayofyear\n",
    "\n",
    "print(f\"Added date-derived features: issue_year, issue_month, issue_day_of_week, issue_day_of_year\")\n",
    "\n",
    "# Tidy data: Ensure each variable forms a column and each observation forms a row\n",
    "print(f\"\\nData tidiness check:\")\n",
    "print(f\"- Each column represents a variable: {list(df_optimized.columns)}\")\n",
    "print(f\"- Each row represents an observation: {df_optimized.shape[0]} observations\")\n",
    "print(f\"- Values are not spread across multiple columns: âœ“ (data appears to be in tidy format)\")\n",
    "\n",
    "# Visualization of data types\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "dtypes_counts = df.dtypes.value_counts()\n",
    "plt.pie(dtypes_counts.values, labels=dtypes_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Original Data Types Distribution')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "dtypes_counts_opt = df_optimized.dtypes.value_counts()\n",
    "plt.pie(dtypes_counts_opt.values, labels=dtypes_counts_opt.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Optimized Data Types Distribution')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "memory_comparison = [df.memory_usage(deep=True).sum() / 1024**2, \n",
    "                     df_optimized.memory_usage(deep=True).sum() / 1024**2]\n",
    "plt.bar(['Before', 'After'], memory_comparison, color=['red', 'green'], alpha=0.7)\n",
    "plt.title('Memory Usage Comparison')\n",
    "plt.ylabel('Memory (MB)')\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(memory_comparison):\n",
    "    plt.text(i, v + max(memory_comparison)*0.01, f'{v:.1f}MB', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Update working dataset\n",
    "df = df_optimized.copy()\n",
    "print(f\"\\nUpdated dataset shape after type conversion: {df.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation\n",
    "\n",
    "String manipulation is important for cleaning categorical data, creating new features from text, and standardizing formats."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# String Manipulation\n",
    "print(\"String Manipulation:\")\n",
    "\n",
    "# Check the categorical columns for potential string manipulation\n",
    "categorical_cols = ['grade', 'home_ownership', 'verification_status', 'purpose']\n",
    "\n",
    "print(\"Original categorical values:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}: {df[col].value_counts().to_dict()}\")\n",
    "\n",
    "# Create examples of string manipulation issues\n",
    "# Add some inconsistent string values to demonstrate cleaning\n",
    "df_string_demo = df.copy()\n",
    "\n",
    "# Introduce some inconsistent formatting\n",
    "inconsistent_indices = np.random.choice(df_string_demo.index, size=50, replace=False)\n",
    "df_string_demo.loc[inconsistent_indices[:25], 'purpose'] = df_string_demo.loc[inconsistent_indices[:25], 'purpose'].str.upper()\n",
    "df_string_demo.loc[inconsistent_indices[25:], 'home_ownership'] = df_string_demo.loc[inconsistent_indices[25:], 'home_ownership'].str.lower()\n",
    "\n",
    "print(f\"\\nAfter introducing inconsistent formatting:\")\n",
    "for col in ['purpose', 'home_ownership']:\n",
    "    print(f\"{col}: {df_string_demo[col].value_counts().head()}\")\n",
    "\n",
    "# Techniques for string manipulation\n",
    "print(f\"\\nString Manipulation Techniques Applied:\")\n",
    "\n",
    "# 1. Standardize text case\n",
    "df_string_demo['purpose'] = df_string_demo['purpose'].str.lower()\n",
    "df_string_demo['home_ownership'] = df_string_demo['home_ownership'].str.upper()\n",
    "print(\"1. Standardized text case (lowercase for purpose, uppercase for home_ownership)\")\n",
    "\n",
    "# 2. Strip whitespace\n",
    "df_string_demo['verification_status'] = df_string_demo['verification_status'].str.strip()\n",
    "print(\"2. Stripped whitespace from verification_status\")\n",
    "\n",
    "# 3. Replace specific values\n",
    "df_string_demo['home_ownership'] = df_string_demo['home_ownership'].replace('OWN', 'OWNED')  # Standardize naming\n",
    "print(\"3. Replaced 'OWN' with 'OWNED' in home_ownership\")\n",
    "\n",
    "# 4. Extract patterns from text\n",
    "df_string_demo['purpose_length'] = df_string_demo['purpose'].str.len()\n",
    "print(\"4. Extracted string length as 'purpose_length' feature\")\n",
    "\n",
    "# 5. Create binary features based on string content\n",
    "df_string_demo['purpose_includes_debt'] = df_string_demo['purpose'].str.contains('debt', case=False).astype(int)\n",
    "df_string_demo['purpose_includes_home'] = df_string_demo['purpose'].str.contains('home', case=False).astype(int)\n",
    "print(\"5. Created binary features based on string content\")\n",
    "\n",
    "# 6. Split strings to extract meaningful parts\n",
    "# Not applicable in our current example, but showing the concept\n",
    "\n",
    "# 7. Regular expressions for complex text cleaning\n",
    "import re\n",
    "\n",
    "# Example of using regex to clean text (though we don't have text fields with patterns to clean)\n",
    "# df_string_demo['purpose'] = df_string_demo['purpose'].str.replace(r'[0-9]+', '', regex=True)\n",
    "\n",
    "# Show results after string manipulation\n",
    "print(f\"\\nAfter string manipulation:\")\n",
    "for col in ['purpose', 'home_ownership']:\n",
    "    print(f\"{col}: {df_string_demo[col].value_counts().to_dict()}\")\n",
    "\n",
    "# Visualization of string manipulation\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "original_purpose_dist = df['purpose'].value_counts()[:5]\n",
    "plt.bar(range(len(original_purpose_dist)), original_purpose_dist.values, color='lightblue', alpha=0.7)\n",
    "plt.title('Original Purpose Distribution (Top 5)')\n",
    "plt.xticks(range(len(original_purpose_dist)), original_purpose_dist.index, rotation=45)\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "df_string_demo['purpose_length'].hist(bins=20, alpha=0.7, color='lightgreen')\n",
    "plt.title('Distribution of Purpose String Lengths')\n",
    "plt.xlabel('String Length')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "new_features = ['purpose_includes_debt', 'purpose_includes_home']\n",
    "new_feature_counts = []\n",
    "for feature in new_features:\n",
    "    counts = df_string_demo[feature].value_counts()\n",
    "    new_feature_counts.append(counts)\n",
    "    \n",
    "labels = ['No', 'Yes']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, [new_feature_counts[0][0], new_feature_counts[0][1]] if 0 in new_feature_counts[0].index and 1 in new_feature_counts[0].index else [0, 0], \n",
    "        width, label='Debt-related', alpha=0.7)\n",
    "plt.bar(x + width/2, [new_feature_counts[1][0], new_feature_counts[1][1]] if 0 in new_feature_counts[1].index and 1 in new_feature_counts[1].index else [0, 0], \n",
    "        width, label='Home-related', alpha=0.7)\n",
    "plt.title('Binary Features from String Patterns')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(x, labels)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Update working dataset\n",
    "df = df_string_demo.copy()\n",
    "print(f\"\\nUpdated dataset shape after string manipulation: {df.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Concatenation\n",
    "\n",
    "Data concatenation refers to combining multiple datasets or adding new observations to existing datasets."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data Concatenation\n",
    "print(\"Data Concatenation:\")\n",
    "\n",
    "# Create additional datasets for concatenation examples\n",
    "# 1. Adding additional rows (new observations)\n",
    "additional_data = {\n",
    "    'loan_id': range(df['loan_id'].max() + 1, df['loan_id'].max() + 1001),\n",
    "    'loan_amnt': np.random.normal(18000, 7000, 1000),\n",
    "    'int_rate': np.random.normal(11, 3, 1000),\n",
    "    'installment': np.random.normal(450, 180, 1000),\n",
    "    'annual_inc': np.random.normal(75000, 30000, 1000),\n",
    "    'dti': np.random.normal(14, 7, 1000),\n",
    "    'fico_score': np.random.normal(700, 70, 1000),\n",
    "    'emp_length': np.random.gamma(2.5, 2, 1000),\n",
    "    'loan_status': np.random.choice([0, 1], 1000, p=[0.88, 0.12]),\n",
    "    'grade': pd.cut(np.random.normal(700, 70, 1000), \n",
    "                    bins=[0, 580, 620, 660, 700, 740, 780, 850], \n",
    "                    labels=['G', 'F', 'E', 'D', 'C', 'B', 'A']),\n",
    "    'home_ownership': np.random.choice(['MORTGAGE', 'OWNED', 'RENT', 'OTHER'], 1000, p=[0.4, 0.2, 0.35, 0.05]),\n",
    "    'verification_status': np.random.choice(['VERIFIED', 'NOT VERIFIED', 'SOURCE VERIFIED'], 1000, p=[0.35, 0.5, 0.15]),\n",
    "    'purpose': np.random.choice(['debt_consolidation', 'credit_card', 'home_improvement', 'major_purchase', \n",
    "                                'small_business', 'other', 'vacation', 'car', 'moving', 'medical'], \n",
    "                               1000, p=[0.25, 0.2, 0.15, 0.1, 0.08, 0.07, 0.05, 0.05, 0.03, 0.02])\n",
    "}\n",
    "\n",
    "additional_df = pd.DataFrame(additional_data)\n",
    "\n",
    "# Apply same string processing to new data\n",
    "additional_df['purpose'] = additional_df['purpose'].str.lower()\n",
    "additional_df['home_ownership'] = additional_df['home_ownership'].str.upper()\n",
    "additional_df['verification_status'] = additional_df['verification_status'].str.upper()\n",
    "additional_df['purpose_length'] = additional_df['purpose'].str.len()\n",
    "additional_df['purpose_includes_debt'] = additional_df['purpose'].str.contains('debt', case=False).astype(int)\n",
    "additional_df['purpose_includes_home'] = additional_df['purpose'].str.contains('home', case=False).astype(int)\n",
    "\n",
    "# Set appropriate data types\n",
    "for col in ['grade', 'home_ownership', 'verification_status', 'purpose']:\n",
    "    additional_df[col] = additional_df[col].astype('category')\n",
    "\n",
    "# Concatenate datasets\n",
    "df_concatenated = pd.concat([df, additional_df], ignore_index=True)\n",
    "print(f\"1. Concatenated datasets (added {len(additional_df)} new rows)\")\n",
    "print(f\"   Original shape: {df.shape}\")\n",
    "print(f\"   Additional data shape: {additional_df.shape}\")\n",
    "print(f\"   Combined shape: {df_concatenated.shape}\")\n",
    "\n",
    "# 2. Adding new columns (features) from another dataframe\n",
    "new_features_df = pd.DataFrame({\n",
    "    'loan_id': df_concatenated['loan_id'],\n",
    "    'region': np.random.choice(['Northeast', 'Southeast', 'Midwest', 'Southwest', 'West'], len(df_concatenated)),\n",
    "    'loan_age_months': np.random.randint(1, 60, len(df_concatenated)),\n",
    "    'delinq_2yrs': np.random.poisson(0.5, len(df_concatenated))\n",
    "})\n",
    "\n",
    "# Concatenate along columns axis\n",
    "df_final = pd.concat([df_concatenated, new_features_df.drop('loan_id', axis=1)], axis=1)\n",
    "print(f\"2. Added new columns (region, loan_age_months, delinq_2yrs)\")\n",
    "print(f\"   Final shape after adding columns: {df_final.shape}\")\n",
    "\n",
    "# 3. Visualization of concatenation\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "sizes = [df.shape[0], additional_df.shape[0], df_concatenated.shape[0]]\n",
    "labels = ['Original', 'Added', 'Combined']\n",
    "plt.bar(labels, sizes, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "plt.title('Dataset Size Before and After Concatenation')\n",
    "plt.ylabel('Number of Rows')\n",
    "for i, v in enumerate(sizes):\n",
    "    plt.text(i, v + max(sizes)*0.01, f'{v}', ha='center')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "new_features_count = len(new_features_df.columns) - 1  # Exclude loan_id\n",
    "original_features_count = len(df.columns)\n",
    "final_features_count = len(df_final.columns)\n",
    "feature_counts = [original_features_count, new_features_count, final_features_count]\n",
    "feature_labels = ['Original Features', 'Added Features', 'Total Features']\n",
    "plt.bar(feature_labels, feature_counts, color=['gold', 'lightblue', 'orange'])\n",
    "plt.title('Feature Count Before and After Adding Columns')\n",
    "plt.ylabel('Number of Features')\n",
    "for i, v in enumerate(feature_counts):\n",
    "    plt.text(i, v + max(feature_counts)*0.01, f'{v}', ha='center')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "new_features_df['region'].value_counts().plot(kind='bar', color='lightblue', alpha=0.7)\n",
    "plt.title('Distribution of New Feature: Region')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Update working dataset\n",
    "df = df_final.copy()\n",
    "print(f\"\\nUpdated dataset shape after concatenation: {df.shape}\")\n",
    "print(f\"Dataset now contains {len(df.columns)} features\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is the process of using domain knowledge to extract features from raw data that help machine learning algorithms work more effectively."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "print(\"Feature Engineering:\")\n",
    "\n",
    "# Original features\n",
    "original_features = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score', 'emp_length']\n",
    "print(f\"Original features: {original_features}\")\n",
    "\n",
    "# 1. Mathematical transformations\n",
    "df['loan_to_income_ratio'] = df['loan_amnt'] / (df['annual_inc'] + 1)  # Adding 1 to avoid division by zero\n",
    "df['interest_cost_total'] = df['loan_amnt'] * (df['int_rate'] / 100)\n",
    "df['debt_to_income_ratio_squared'] = df['dti'] ** 2\n",
    "df['fico_score_squared'] = df['fico_score'] ** 2\n",
    "df['fico_score_log'] = np.log(df['fico_score'] + 1)  # Adding 1 to avoid log(0)\n",
    "\n",
    "print(\"1. Added mathematical transformations:\")\n",
    "print(\"   - loan_to_income_ratio: loan amount to annual income ratio\")\n",
    "print(\"   - interest_cost_total: total interest cost based on loan amount and rate\")\n",
    "print(\"   - debt_to_income_ratio_squared: squared DTI ratio\")\n",
    "print(\"   - fico_score_squared: squared FICO score\")\n",
    "print(\"   - fico_score_log: log-transformed FICO score\")\n",
    "\n",
    "# 2. Binning/Discretization\n",
    "df['fico_score_cat'] = pd.cut(df['fico_score'], \n",
    "                              bins=[0, 580, 620, 660, 700, 740, 780, 850], \n",
    "                              labels=['Very Poor', 'Poor', 'Fair', 'Good', 'Very Good', 'Excellent', 'Exceptional'])\n",
    "df['income_cat'] = pd.cut(df['annual_inc'], \n",
    "                          bins=[0, 30000, 50000, 70000, 90000, 150000, float('inf')],\n",
    "                          labels=['Very Low', 'Low', 'Medium', 'High', 'Very High', 'Very Very High'])\n",
    "\n",
    "print(f\"\\n2. Added categorical binned features:\")\n",
    "print(f\"   - fico_score_cat: binned FICO scores\")\n",
    "print(f\"   - income_cat: binned annual income\")\n",
    "\n",
    "# 3. Interaction features\n",
    "df['fico_grade_interaction'] = df['fico_score'] * pd.Categorical(df['grade']).codes\n",
    "df['income_dti_interaction'] = df['annual_inc'] * df['dti']\n",
    "df['loan_rate_interaction'] = df['loan_amnt'] * df['int_rate']\n",
    "\n",
    "print(f\"\\n3. Added interaction features:\")\n",
    "print(f\"   - fico_grade_interaction: interaction between FICO score and grade\")\n",
    "print(f\"   - income_dti_interaction: interaction between income and DTI\")\n",
    "print(f\"   - loan_rate_interaction: interaction between loan amount and rate\")\n",
    "\n",
    "# 4. Domain-specific features\n",
    "df['risk_score'] = (\n",
    "    (850 - df['fico_score']) / 100 +  # Lower FICO -> higher risk\n",
    "    df['dti'] / 10 +  # Higher DTI -> higher risk\n",
    "    df['int_rate'] / 10 +  # Higher interest -> higher risk\n",
    "    (df['grade'].cat.codes.max() - pd.Categorical(df['grade']).codes)  # Lower grade -> higher risk\n",
    ") / 4  # Normalize\n",
    "\n",
    "# Calculate installment to income ratio\n",
    "df['installment_to_income_ratio'] = df['installment'] / (df['annual_inc'] / 12 + 1)\n",
    "\n",
    # Employment length to income ratio\n",
    "df['emp_length_to_income_ratio'] = df['emp_length'] / (df['annual_inc'] / 50000 + 1)\n",
    "\n",
    "print(f\"\\n4. Added domain-specific features:\")\n",
    "print(f\"   - risk_score: composite risk score based on multiple factors\")\n",
    "print(f\"   - installment_to_income_ratio: monthly installment to monthly income ratio\")\n",
    "print(f\"   - emp_length_to_income_ratio: employment length adjusted for income\")\n",
    "\n",
    "# 5. One-hot encoding for categorical variables\n",
    "categorical_cols = ['grade', 'home_ownership', 'verification_status', 'purpose', 'fico_score_cat', 'income_cat']\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, prefix=categorical_cols, dtype=int)\n",
    "\n",
    "print(f\"\\n5. Applied one-hot encoding to categorical features:\")\n",
    "print(f\"   - Original categorical columns: {categorical_cols}\")\n",
    "print(f\"   - Added {len(df_encoded.columns) - len(df.columns)} new binary columns\")\n",
    "print(f\"   - New total features: {len(df_encoded.columns)}\")\n",
    "\n",
    "# 6. Scaling features (for algorithms sensitive to feature scales)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numeric_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score', 'emp_length',\n",
    "                'loan_to_income_ratio', 'interest_cost_total', 'installment_to_income_ratio', 'risk_score']\n",
    "\n",
    "# Only scale features that exist in the dataset\n",
    "available_numeric_cols = [col for col in numeric_cols if col in df_encoded.columns]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_encoded[available_numeric_cols] = scaler.fit_transform(df_encoded[available_numeric_cols])\n",
    "\n",
    "print(f\"\\n6. Applied standardization to numeric features: {available_numeric_cols}\")\n",
    "\n",
    "# Update working dataset\n",
    "df = df_encoded.copy()\n",
    "print(f\"\\nFinal dataset shape after feature engineering: {df.shape}\")\n",
    "print(f\"Features increased from original {len(original_features)} to {len(df.columns)} features\")\n",
    "\n",
    "# Visualization of feature engineering\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(df['loan_to_income_ratio'], bins=50, alpha=0.7, color='lightblue')\n",
    "plt.title('Distribution of New Feature: Loan to Income Ratio')\n",
    "plt.xlabel('Loan to Income Ratio')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(df['risk_score'], bins=50, alpha=0.7, color='lightgreen')\n",
    "plt.title('Distribution of New Feature: Risk Score')\n",
    "plt.xlabel('Risk Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.scatter(df['fico_score'], df['loan_to_income_ratio'], alpha=0.5, c=df['loan_status'], cmap='viridis')\n",
    "plt.title('New Feature vs Original Feature')\n",
    "plt.xlabel('FICO Score')\n",
    "plt.ylabel('Loan to Income Ratio')\n",
    "plt.colorbar(label='Loan Status')\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "# Show feature importance based on correlation with target\n",
    "correlations = df.select_dtypes(include=[np.number]).corrwith(df['loan_status']).abs().sort_values(ascending=False)[:10]\n",
    "plt.barh(range(len(correlations)), correlations.values)\n",
    "plt.yticks(range(len(correlations)), correlations.index, fontsize=8)\n",
    "plt.title('Top 10 Features by Correlation with Target')\n",
    "plt.xlabel('Absolute Correlation with Target')\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "feature_counts = [len(original_features), len(df.columns)-len(original_features), len(df.columns)]\n",
    "labels = ['Original Features', 'Engineered Features', 'Total Features']\n",
    "plt.bar(labels, feature_counts, color=['skyblue', 'lightgreen', 'lightcoral'])\n",
    "plt.title('Feature Count Summary')\n",
    "plt.ylabel('Count')\n",
    "for i, v in enumerate(feature_counts):\n",
    "    plt.text(i, v + max(feature_counts)*0.01, f'{v}', ha='center')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "df['fico_score_cat_Very Good'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Distribution of One-Hot Encoded Feature')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Imbalanced Data\n",
    "\n",
    "In many real-world datasets, the target classes are not equally represented. This is especially common in credit risk prediction where default rates are typically low."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Handling Imbalanced Data\n",
    "print(\"Handling Imbalanced Data:\")\n",
    "\n",
    "# First, let's check the class distribution in our target variable\n",
    "target_distribution = df['loan_status'].value_counts()\n",
    "print(f\"Original target distribution:\")\n",
    "print(target_distribution)\n",
    "print(f\"Class ratio: {target_distribution[0]}/{target_distribution[1]} = {target_distribution[0]/target_distribution[1]:.2f}:1\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(['loan_status'], axis=1)\n",
    "y = df['loan_status']\n",
    "\n",
    "# Select only numeric columns for some of the imbalanced data techniques\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "X_numeric = X[numeric_cols]\n",
    "\n",
    "# Make sure there are no infinite values\n",
    "X_numeric = X_numeric.replace([np.inf, -np.inf], np.nan)\n",
    "X_numeric = X_numeric.fillna(X_numeric.mean())\n",
    "\n",
    "# 1. Undersampling - Random Undersampling\n",
    "print(f\"\\n1. Random Undersampling:\")\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_under, y_under = undersampler.fit_resample(X_numeric, y)\n",
    "print(f\"   Before: {y.value_counts().to_dict()}\")\n",
    "print(f\"   After: {pd.Series(y_under).value_counts().to_dict()}\")\n",
    "\n",
    "# 2. Oversampling - SMOTE (Synthetic Minority Oversampling Technique)\n",
    "print(f\"\\n2. SMOTE Oversampling:\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X_numeric, y)\n",
    "print(f\"   Before: {y.value_counts().to_dict()}\")\n",
    "print(f\"   After: {pd.Series(y_smote).value_counts().to_dict()}\")\n",
    "\n",
    "# 3. Advanced Oversampling - ADASYN\n",
    "print(f\"\\n3. ADASYN Oversampling:\")\n",
    "adasyn = ADASYN(random_state=42, n_neighbors=5)\n",
    "X_adasyn, y_adasyn = adasyn.fit_resample(X_numeric, y)\n",
    "print(f\"   Before: {y.value_counts().to_dict()}\")\n",
    "print(f\"   After: {pd.Series(y_adasyn).value_counts().to_dict()}\")\n",
    "\n",
    "# 4. Combined approach - SMOTETomek\n",
    "print(f\"\\n4. SMOTETomek (Combined Oversampling and Undersampling):\")\n",
    "smotetomek = SMOTETomek(random_state=42)\n",
    "X_smtm, y_smtm = smotetomek.fit_resample(X_numeric, y)\n",
    "print(f\"   Before: {y.value_counts().to_dict()}\")\n",
    "print(f\"   After: {pd.Series(y_smtm).value_counts().to_dict()}\")\n",
    "\n",
    "# Visualization of different sampling techniques\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Original distribution\n",
    "original_counts = y.value_counts()\n",
    "axes[0].pie(original_counts.values, labels=[f'Paid ({original_counts[0]})', f'Default ({original_counts[1]})'], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Original Distribution')\n",
    "\n",
    "# After Random Undersampling\n",
    "under_counts = pd.Series(y_under).value_counts()\n",
    "axes[1].pie(under_counts.values, labels=[f'Paid ({under_counts[1]})', f'Default ({under_counts[0]})'], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "axes[1].set_title('After Random Undersampling')\n",
    "\n",
    "# After SMOTE\n",
    "smote_counts = pd.Series(y_smote).value_counts()\n",
    "axes[2].pie(smote_counts.values, labels=[f'Paid ({smote_counts[0]})', f'Default ({smote_counts[1]})'], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "axes[2].set_title('After SMOTE')\n",
    "\n",
    "# After ADASYN\n",
    "adasyn_counts = pd.Series(y_adasyn).value_counts()\n",
    "axes[3].pie(adasyn_counts.values, labels=[f'Paid ({adasyn_counts[0]})', f'Default ({adasyn_counts[1]})'], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "axes[3].set_title('After ADASYN')\n",
    "\n",
    "# After SMOTETomek\n",
    "smtm_counts = pd.Series(y_smtm).value_counts()\n",
    "axes[4].pie(smtm_counts.values, labels=[f'Paid ({smtm_counts[0]})', f'Default ({smtm_counts[1]})'], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "axes[4].set_title('After SMOTETomek')\n",
    "\n",
    "# Comparison bar chart\n",
    "sampling_methods = ['Original', 'Undersample', 'SMOTE', 'ADASYN', 'SMOTETomek']\n",
    "default_counts = [target_distribution[1], under_counts[1], smote_counts[1], adasyn_counts[1], smtm_counts[1]]\n",
    "paid_counts = [target_distribution[0], under_counts[0], smote_counts[0], adasyn_counts[0], smtm_counts[0]]\n",
    "\n",
    "x = np.arange(len(sampling_methods))\n",
    "width = 0.35\n",
    "axes[5].bar(x - width/2, paid_counts, width, label='Paid', alpha=0.7)\n",
    "axes[5].bar(x + width/2, default_counts, width, label='Default', alpha=0.7)\n",
    "axes[5].set_xlabel('Sampling Method')\n",
    "axes[5].set_ylabel('Count')\n",
    "axes[5].set_title('Comparison of Sampling Methods')\n",
    "axes[5].set_xticks(x)\n",
    "axes[5].set_xticklabels(sampling_methods, rotation=45)\n",
    "axes[5].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demonstrate model performance with different sampling techniques\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "print(f\"\\nComparing model performance with different sampling techniques:\")\n",
    "\n",
    "# Split original data\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "    X_numeric, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train a model on original data\n",
    "lr_orig = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_orig.fit(X_train_orig, y_train_orig)\n",
    "y_pred_orig = lr_orig.predict(X_test_orig)\n",
    "auc_orig = roc_auc_score(y_test_orig, lr_orig.predict_proba(X_test_orig)[:, 1])\n",
    "print(f\"\\nOriginal data - AUC: {auc_orig:.4f}\")\n",
    "print(f\"Original data - Classification Report:\")\n",
    "print(classification_report(y_test_orig, y_pred_orig))\n",
    "\n",
    "# Train a model on SMOTE data\n",
    "X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(\n",
    "    X_smote, y_smote, test_size=0.2, random_state=42, stratify=y_smote\n",
    ")\n",
    "\n",
    "lr_smote = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_smote.fit(X_train_smote, y_train_smote)\n",
    "y_pred_smote = lr_smote.predict(X_test_smote)\n",
    "auc_smote = roc_auc_score(y_test_smote, lr_smote.predict_proba(X_test_smote)[:, 1])\n",
    "print(f\"\\nSMOTE data - AUC: {auc_smote:.4f}\")\n",
    "print(f\"SMOTE data - Classification Report:\")\n",
    "print(classification_report(y_test_smote, y_pred_smote))\n",
    "\n",
    "# Results comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': ['Original', 'SMOTE'],\n",
    "    'AUC Score': [auc_orig, auc_smote],\n",
    "    'Default Precision': [\n",
    "        classification_report(y_test_orig, y_pred_orig, output_dict=True)['1']['precision'],\n",
    "        classification_report(y_test_smote, y_pred_smote, output_dict=True)['1']['precision']\n",
    "    ],\n",
    "    'Default Recall': [\n",
    "        classification_report(y_test_orig, y_pred_orig, output_dict=True)['1']['recall'],\n",
    "        classification_report(y_test_smote, y_pred_smote, output_dict=True)['1']['recall']\n",
    "    ]\n",
    "})\n",
    "print(f\"\\nComparison Summary:\")\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Update working dataset to SMOTE balanced data\n",
    "X_balanced = X_smote\n",
    "y_balanced = y_smote\n",
    "print(f\"\\nUsing SMOTE balanced dataset for further analysis\")\n",
    "print(f\"Balanced dataset shape: X={X_balanced.shape}, y={y_balanced.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curse of Dimensionality\n",
    "\n",
    "The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Curse of Dimensionality\n",
    "print(\"Curse of Dimensionality:\")\n",
    "\n",
    "# Explain the concept\n",
    "print(\"The curse of dimensionality affects machine learning algorithms in several ways:\")\n",
    "print(\"1. Data becomes sparse as dimensions increase\")\n",
    "print(\"2. Distance functions become less meaningful\")\n",
    "print(\"3. More data is needed to maintain the same density\")\n",
    "print(\"4. Computational complexity increases\")\n",
    "print(\"5. Model performance may deteriorate\")\n",
    "\n",
    "# Demonstrate how distance becomes less meaningful in high dimensions\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "dimensions = [2, 5, 10, 20, 50, 100]\n",
    "min_distances = []\n",
    "max_distances = []\n",
    "distance_ratios = []\n",
    "\n",
    "for dim in dimensions:\n",
    "    # Generate random points in high-dimensional space\n",
    "    points = np.random.rand(n_samples, dim)\n",
    "    \n",
    "    # Calculate pairwise distances (using first point as reference)\n",
    "    ref_point = points[0:1]\n",
    "    distances = np.sqrt(np.sum((points - ref_point)**2, axis=1))\n",
    "    \n",
    "    min_dist = np.min(distances[1:])  # Exclude distance to itself\n",
    "    max_dist = np.max(distances[1:])\n",
    "    \n",
    "    min_distances.append(min_dist)\n",
    "    max_distances.append(max_dist)\n",
    "    distance_ratios.append(max_dist / min_dist if min_dist != 0 else 1)\n",
    "\n",
    "# Visualization of curse of dimensionality\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].plot(dimensions, min_distances, marker='o', label='Min Distance', color='blue')\n",
    "axes[0].plot(dimensions, max_distances, marker='s', label='Max Distance', color='red')\n",
    "axes[0].set_xlabel('Number of Dimensions')\n",
    "axes[0].set_ylabel('Distance')\n",
    "axes[0].set_title('Minimum vs Maximum Distances in High Dimensions')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(dimensions, distance_ratios, marker='o', color='green')\n",
    "axes[1].set_xlabel('Number of Dimensions')\n",
    "axes[1].set_ylabel('Max Distance / Min Distance Ratio')\n",
    "axes[1].set_title('Distance Ratio Increase with Dimensions')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Show how data becomes sparse in high dimensions\n",
    "volume_ratios = []\n",
    "for dim in [2, 5, 10, 15, 20]:\n",
    "    # Volume of unit hypersphere relative to unit hypercube\n",
    "    # Volume of hypersphere: pi^(d/2) / (gamma(d/2 + 1))\n",
    "    from math import gamma\n",
    "    sphere_vol = (np.pi ** (dim/2)) / gamma(dim/2 + 1)\n",
    "    cube_vol = 2 ** dim  # Unit hypercube has volume 2^d\n",
    "    volume_ratio = sphere_vol / cube_vol\n",
    "    volume_ratios.append(volume_ratio)\n",
    "\n",
    "dims_sparse = [2, 5, 10, 15, 20]\n",
    "axes[2].bar([str(d) for d in dims_sparse], volume_ratios, color='orange', alpha=0.7)\n",
    "axes[2].set_xlabel('Number of Dimensions')\n",
    "axes[2].set_ylabel('Volume Ratio (Hypersphere/Cube)')\n",
    "axes[2].set_title('Volume Ratio Decreases in High Dimensions')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nEffects of dimensionality on distance measures:\")\n",
    "for dim, ratio in zip(dimensions, distance_ratios):\n",
    "    print(f\"  {dim} dimensions: Max distance is {ratio:.2f}x the min distance\")\n",
    "\n",
    "# Solutions to curse of dimensionality\n",
    "print(f\"\\nSolutions to Curse of Dimensionality:\")\n",
    "print(\"1. Feature Selection - Remove irrelevant features\")\n",
    "print(\"2. Feature Extraction - Transform to lower dimensionality\")\n",
    "print(\"3. Regularization - Prevent overfitting in high dimensions\")\n",
    "print(\"4. Increase sample size - More data to fill the space\")\n",
    "print(\"5. Use domain knowledge - Select meaningful features only\")\n",
    "\n",
    "# Demonstrate feature selection techniques\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select top k features using statistical tests\n",
    "k_best = 15  # Select top 15 features\n",
    "selector = SelectKBest(score_func=f_classif, k=k_best)\n",
    "X_selected = selector.fit_transform(X_balanced, y_balanced)\n",
    "\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "selected_features = X_balanced.columns[selected_indices]\n",
    "\n",
    "print(f\"\\nApplied feature selection:\")\n",
    "print(f\"  Reduced from {X_balanced.shape[1]} features to {X_selected.shape[1]} features\")\n",
    "print(f\"  Selected features: {list(selected_features[:10])}...\")  # Show first 10\n",
    "\n",
    "# Compare model performance before and after feature selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train_selected, X_test_selected = train_test_split(\n",
    "    X_selected, y_balanced, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train models\n",
    "rf_full = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf_full.fit(X_train, y_train)\n",
    "\n",
    "rf_selected = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "# Compare performance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "y_pred_full = rf_full.predict(X_test)\n",
    "y_pred_selected = rf_selected.predict(X_test_selected)\n",
    "\n",
    "print(f\"\\nModel Performance Comparison:\")\n",
    "print(f\"  Full feature set: Accuracy={accuracy_score(y_test, y_pred_full):.3f}, \"\n",
    "      f\"Precision={precision_score(y_test, y_pred_full):.3f}, \"\n",
    "      f\"Recall={recall_score(y_test, y_pred_full):.3f}\")\n",
    "print(f\"  Selected features: Accuracy={accuracy_score(y_test, y_pred_selected):.3f}, \"\n",
    "      f\"Precision={precision_score(y_test, y_pred_selected):.3f}, \"\n",
    "      f\"Recall={recall_score(y_test, y_pred_selected):.3f}\")\n",
    "\n",
    "# Visualization of feature importance\n",
    "feature_scores = selector.scores_\n",
    "top_features_idx = np.argsort(feature_scores)[-10:]  # Top 10 features\n",
    "top_feature_names = X_balanced.columns[top_features_idx]\n",
    "top_feature_scores = feature_scores[top_features_idx]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(top_feature_names)), top_feature_scores)\n",
    "plt.yticks(range(len(top_feature_names)), top_feature_names)\n",
    "plt.xlabel('Feature Score (F-statistic)')\n",
    "plt.title('Top 10 Features by Statistical Significance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example with Lending Club Data\n",
    "\n",
    "Now let's bring together all the concepts we've learned in a comprehensive example."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Practical Example with Lending Club Data\n",
    "print(\"Practical Example: Complete Data Cleaning and Preparation Pipeline\")\n",
    "\n",
    "# Create a simulated Lending Club dataset with realistic issues\n",
    "np.random.seed(123)\n",
    "n_samples = 5000\n",
    "\n",
    "# Generate realistic lending data\n",
    "data = {\n",
    "    'loan_amnt': np.random.normal(16000, 10000, n_samples),\n",
    "    'int_rate': np.random.normal(12, 5, n_samples),\n",
    "    'installment': np.random.normal(420, 250, n_samples),\n",
    "    'annual_inc': np.random.lognormal(np.log(65000), 0.6, n_samples),\n",
    "    'dti': np.random.gamma(2, 7, n_samples),\n",
    "    'fico_score': np.random.normal(690, 70, n_samples),\n",
    "    'emp_length': np.random.beta(2, 5, n_samples) * 15,\n",
    "    'loan_status': np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),  # 15% default rate\n",
    "    'grade': pd.cut(np.random.normal(690, 70, n_samples), \n",
    "                    bins=[0, 580, 620, 660, 700, 740, 780, 850], \n",
    "                    labels=['G', 'F', 'E', 'D', 'C', 'B', 'A']),\n",
    "    'home_ownership': np.random.choice(['MORTGAGE', 'OWN', 'RENT'], n_samples, p=[0.45, 0.15, 0.4]),\n",
    "    'verification_status': np.random.choice(['Verified', 'Not Verified', 'Source Verified'], n_samples, p=[0.3, 0.55, 0.15]),\n",
    "    'purpose': np.random.choice(['debt_consolidation', 'credit_card', 'home_improvement', 'major_purchase', \n",
    "                                'small_business', 'other', 'vacation', 'car', 'moving', 'medical'], \n",
    "                               n_samples, p=[0.25, 0.2, 0.15, 0.1, 0.08, 0.07, 0.05, 0.05, 0.03, 0.02])\n",
    "}\n",
    "\n",
    "# Create the DataFrame\n",
    "loan_df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce realistic data problems\n",
    "# 1. Missing values\n",
    "missing_indices = np.random.choice(loan_df.index, size=int(0.08 * n_samples), replace=False)\n",
    "loan_df.loc[missing_indices[:int(0.03 * n_samples)], 'annual_inc'] = np.nan\n",
    "loan_df.loc[missing_indices[int(0.03 * n_samples):int(0.05 * n_samples)], 'dti'] = np.nan\n",
    "loan_df.loc[missing_indices[int(0.05 * n_samples):int(0.07 * n_samples)], 'fico_score'] = np.nan\n",
    "loan_df.loc[missing_indices[int(0.07 * n_samples):], 'emp_length'] = np.nan\n",
    "\n",
    "# 2. Outliers\n",
    "outlier_indices = np.random.choice(loan_df.index, size=int(0.05 * n_samples), replace=False)\n",
    "loan_df.loc[outlier_indices[:int(0.025 * n_samples)], 'annual_inc'] = np.random.uniform(500000, 1000000, int(0.025 * n_samples))\n",
    "loan_df.loc[outlier_indices[int(0.025 * n_samples):], 'fico_score'] = np.random.uniform(150, 250, int(0.025 * n_samples))\n",
    "\n",
    "# 3. Inconsistent categorical values\n",
    "inconsistent_indices = np.random.choice(loan_df.index, size=50, replace=False)\n",
    "loan_df.loc[inconsistent_indices[:25], 'purpose'] = loan_df.loc[inconsistent_indices[:25], 'purpose'].str.upper()\n",
    "loan_df.loc[inconsistent_indices[25:], 'home_ownership'] = loan_df.loc[inconsistent_indices[25:], 'home_ownership'].str.lower()\n",
    "\n",
    "# 4. Invalid ranges\n",
    "loan_df.loc[loan_df['loan_amnt'] < 1000, 'loan_amnt'] = 1000  # Minimum loan amount\n",
    "loan_df.loc[loan_df['int_rate'] > 36, 'int_rate'] = 36  # Maximum interest rate\n",
    "loan_df.loc[loan_df['dti'] > 100, 'dti'] = 100  # Maximum DTI is 100\n",
    "loan_df.loc[loan_df['fico_score'] > 850, 'fico_score'] = 850\n",
    "loan_df.loc[loan_df['fico_score'] < 300, 'fico_score'] = 300\n",
    "\n",
    "# Create a copy for the pipeline\n",
    "pipeline_df = loan_df.copy()\n",
    "\n",
    "# Step 1: Data Cleaning\n",
    "print(f\"\\n=== STEP 1: DATA CLEANING ===\")\n",
    "print(f\"Before cleaning: {pipeline_df.shape}\")\n",
    "\n",
    "# a) Handle missing values\n",
    "print(f\"Missing values before: {pipeline_df.isnull().sum().sum()}\")\n",
    "\n",
    "# Use median for numerical, mode for categorical\n",
    "for col in ['annual_inc', 'dti', 'fico_score', 'emp_length']:\n",
    "    if col in pipeline_df.columns and pipeline_df[col].isnull().any():\n",
    "        if pipeline_df[col].dtype in ['int64', 'float64']:\n",
    "            pipeline_df[col].fillna(pipeline_df[col].median(), inplace=True)\n",
    "        else:\n",
    "            pipeline_df[col].fillna(pipeline_df[col].mode()[0], inplace=True)\n",
    "\n",
    "print(f\"Missing values after: {pipeline_df.isnull().sum().sum()}\")\n",
    "\n",
    "# b) Handle duplicates\n",
    "duplicates_before = pipeline_df.duplicated().sum()\n",
    "pipeline_df = pipeline_df.drop_duplicates()\n",
    "duplicates_after = pipeline_df.duplicated().sum()\n",
    "print(f\"Removed {duplicates_before - duplicates_after} duplicate rows\")\n",
    "\n",
    "# c) Handle inconsistent categorical values\n",
    "for col in ['purpose', 'home_ownership']:\n",
    "    pipeline_df[col] = pipeline_df[col].str.lower()\n",
    "\n",
    "print(f\"Shape after Step 1: {pipeline_df.shape}\")\n",
    "\n",
    "# Step 2: Outlier Detection and Treatment\n",
    "print(f\"\\n=== STEP 2: OUTLIER TREATMENT ===\")\n",
    "\n",
    "def cap_outliers(df, columns, method='iqr'):\n",
    "    \"\"\"Cap outliers using IQR or Z-score method\"\"\"\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if method == 'iqr':\n",
    "            Q1 = df_out[col].quantile(0.25)\n",
    "            Q3 = df_out[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        df_out[col] = df_out[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "# Apply outlier treatment\n",
    "outlier_cols = ['annual_inc', 'fico_score', 'dti', 'loan_amnt']\n",
    "pipeline_df = cap_outliers(pipeline_df, outlier_cols, method='iqr')\n",
    "\n",
    "print(f\"Applied IQR capping to: {outlier_cols}\")\n",
    "print(f\"Shape after Step 2: {pipeline_df.shape}\")\n",
    "\n",
    "# Step 3: Data Type Optimization\n",
    "print(f\"\\n=== STEP 3: DATA TYPE OPTIMIZATION ===\")\n",
    "\n",
    "# Convert categorical columns to category type\n",
    "categorical_cols = ['grade', 'home_ownership', 'verification_status', 'purpose']\n",
    "for col in categorical_cols:\n",
    "    pipeline_df[col] = pipeline_df[col].astype('category')\n",
    "\n",
    "# Optimize numerical types\n",
    "for col in ['loan_status']:\n",
    "    if col in pipeline_df.columns:\n",
    "        pipeline_df[col] = pd.to_numeric(pipeline_df[col], downcast='integer')\n",
    "\n",
    "print(f\"Optimized data types for categorical and integer columns\")\n",
    "print(f\"Memory usage before: {loan_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory usage after: {pipeline_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Step 4: Feature Engineering\n",
    "print(f\"\\n=== STEP 4: FEATURE ENGINEERING ===\")\n",
    "\n",
    "# Create new features\n",
    "pipeline_df['loan_to_income_ratio'] = pipeline_df['loan_amnt'] / (pipeline_df['annual_inc'] + 1)\n",
    "pipeline_df['interest_cost'] = pipeline_df['loan_amnt'] * (pipeline_df['int_rate'] / 100)\n",
    "pipeline_df['installment_to_income_ratio'] = pipeline_df['installment'] / (pipeline_df['annual_inc'] / 12 + 1)\n",
    "\n",
    "# Create binned categorical features\n",
    "pipeline_df['fico_score_binned'] = pd.cut(pipeline_df['fico_score'], \n",
    "                                          bins=[0, 600, 650, 700, 750, 850], \n",
    "                                          labels=['Poor', 'Fair', 'Good', 'Very Good', 'Exceptional'])\n",
    "pipeline_df['income_binned'] = pd.cut(pipeline_df['annual_inc'], \n",
    "                                     bins=[0, 40000, 60000, 80000, 120000, float('inf')],\n",
    "                                     labels=['Low', 'Lower Mid', 'Mid', 'Upper Mid', 'High'])\n",
    "\n",
    "# Interaction features\n",
    "pipeline_df['fico_grade_interaction'] = pipeline_df['fico_score'] * (pipeline_df['grade'].cat.codes + 1)\n",
    "\n",
    "print(f\"Created 5 new engineered features\")\n",
    "print(f\"Shape after Step 4: {pipeline_df.shape}\")\n",
    "\n",
    "# Step 5: Handling Imbalanced Data\n",
    "print(f\"\\n=== STEP 5: IMBALANCE HANDLING ===\")\n",
    "\n",
    "# Separate features and target\n",
    "X_pipeline = pipeline_df.drop(['loan_status'], axis=1)\n",
    "y_pipeline = pipeline_df['loan_status']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X_pipeline, drop_first=True)\n",
    "\n",
    "# Check target distribution\n",
    "print(f\"Target distribution before balancing: {y_pipeline.value_counts().to_dict()}\")\n",
    "print(f\"Target ratio: {y_pipeline.value_counts()[0]/y_pipeline.value_counts()[1]:.2f}:1\")\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_encoded, y_pipeline)\n",
    "\n",
    "print(f\"Target distribution after balancing: {pd.Series(y_balanced).value_counts().to_dict()}\")\n",
    "\n",
    "# Final shape\n",
    "print(f\"\\n=== FINAL RESULTS ===\")\n",
    "print(f\"Initial shape: {loan_df.shape}\")\n",
    "print(f\"After pipeline: {X_balanced.shape[0]} samples, {X_balanced.shape[1]} features\")\n",
    "print(f\"Target distribution balanced: {pd.Series(y_balanced).value_counts().to_dict()}\")\n",
    "\n",
    "# Model Performance Comparison\n",
    "print(f\"\\n=== MODEL PERFORMANCE COMPARISON ===\")\n",
    "\n",
    "# Prepare original dataset for comparison\n",
    "X_orig_encoded = pd.get_dummies(loan_df.drop(['loan_status'], axis=1), drop_first=True)\n",
    "y_orig = loan_df['loan_status']\n",
    "\n",
    "# Align feature sets\n",
    "missing_cols = set(X_encoded.columns) - set(X_orig_encoded.columns)\n",
    "for col in missing_cols:\n",
    "    X_orig_encoded[col] = 0\n",
    "\n",
    "# Ensure same column order\n",
    "X_orig_encoded = X_orig_encoded[X_encoded.columns]\n",
    "\n",
    "# Train models on both datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "    X_orig_encoded, y_orig, test_size=0.2, random_state=42, stratify=y_orig\n",
    ")\n",
    "\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(\n",
    "    X_balanced, y_balanced, test_size=0.2, random_state=42, stratify=y_balanced\n",
    ")\n",
    "\n",
    "# Train models\n",
    "rf_orig = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_orig.fit(X_train_orig, y_train_orig)\n",
    "\n",
    "rf_bal = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_bal.fit(X_train_bal, y_train_bal)\n",
    "\n",
    # Make predictions
    "y_pred_orig = rf_orig.predict(X_test_orig)\n",
    "y_pred_bal = rf_bal.predict(X_test_bal)\n",
    "\n",
    # Calculate metrics
    "orig_metrics = {\n",
    "    'accuracy': accuracy_score(y_test_orig, y_pred_orig),\n",
    "    'precision': precision_score(y_test_orig, y_pred_orig),\n",
    "    'recall': recall_score(y_test_orig, y_pred_orig),\n",
    "    'auc': roc_auc_score(y_test_orig, rf_orig.predict_proba(X_test_orig)[:, 1])\n",
    "}\n",
    "\n",
    "bal_metrics = {\n",
    "    'accuracy': accuracy_score(y_test_bal, y_pred_bal),\n",
    "    'precision': precision_score(y_test_bal, y_pred_bal),\n",
    "    'recall': recall_score(y_test_bal, y_pred_bal),\n",
    "    'auc': roc_auc_score(y_test_bal, rf_bal.predict_proba(X_test_bal)[:, 1])\n",
    "}\n",
    "\n",
    "print(f\"Original dataset - Accuracy: {orig_metrics['accuracy']:.3f}, Precision: {orig_metrics['precision']:.3f}, Recall: {orig_metrics['recall']:.3f}, AUC: {orig_metrics['auc']:.3f}\")\n",
    "print(f\"Balanced dataset - Accuracy: {bal_metrics['accuracy']:.3f}, Precision: {bal_metrics['precision']:.3f}, Recall: {bal_metrics['recall']:.3f}, AUC: {bal_metrics['auc']:.3f}\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# 1. Data size comparison\n",
    "sizes = [loan_df.shape[0], X_balanced.shape[0]]\n",
    "axes[0].bar(['Original', 'After Pipeline'], sizes, color=['skyblue', 'lightgreen'])\n",
    "axes[0].set_title('Sample Size Before and After Pipeline')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "for i, v in enumerate(sizes):\n",
    "    axes[0].text(i, v + max(sizes)*0.01, f'{v}', ha='center')\n",
    "\n",
    "# 2. Feature count comparison\n",
    "feature_counts = [X_orig_encoded.shape[1], X_balanced.shape[1]]\n",
    "axes[1].bar(['Original Features', 'Engineered Features'], feature_counts, color=['orange', 'lightcoral'])\n",
    "axes[1].set_title('Feature Count Before and After Engineering')\n",
    "axes[1].set_ylabel('Number of Features')\n",
    "for i, v in enumerate(feature_counts):\n",
    "    axes[1].text(i, v + max(feature_counts)*0.01, f'{v}', ha='center')\n",
    "\n",
    "# 3. Target distribution\n",
    "target_counts_orig = y_orig.value_counts()\n",
    "target_counts_bal = pd.Series(y_balanced).value_counts()\n",
    "axes[2].bar(['Paid (Original)', 'Default (Original)'], [target_counts_orig[0], target_counts_orig[1]], \n",
    "            color=['lightgreen', 'lightblue'], alpha=0.7, label='Original')\n",
    "axes[2].bar([2, 3], [target_counts_bal[0], target_counts_bal[1]], \n",
    "            color=['orange', 'red'], alpha=0.7, label='Balanced')\n",
    "axes[2].set_title('Target Distribution: Original vs Balanced')\n",
    "axes[2].set_ylabel('Count')\n",
    "axes[2].set_xticks([0, 1, 2, 3])\n",
    "axes[2].set_xticklabels(['Paid', 'Default', 'Paid', 'Default'])\n",
    "axes[2].legend()\n",
    "\n",
    "# 4. Model performance comparison\n",
    "metrics_names = list(orig_metrics.keys())\n",
    "orig_values = list(orig_metrics.values())\n",
    "bal_values = list(bal_metrics.values())\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "axes[3].bar(x - width/2, orig_values, width, label='Original', alpha=0.7)\n",
    "axes[3].bar(x + width/2, bal_values, width, label='Balanced', alpha=0.7)\n",
    "axes[3].set_xlabel('Metrics')\n",
    "axes[3].set_ylabel('Score')\n",
    "axes[3].set_title('Model Performance: Original vs Balanced Data')\n",
    "axes[3].set_xticks(x)\n",
    "axes[3].set_xticklabels(metrics_names)\n",
    "axes[3].legend()\n",
    "\n",
    "# 5. Feature importance from the balanced dataset model\n",
    "feature_importance = rf_bal.feature_importances_\n",
    "top_indices = np.argsort(feature_importance)[-10:]  # Top 10 most important\n",
    "top_features = [X_balanced.columns[i] for i in top_indices]\n",
    "top_importances = feature_importance[top_indices]\n",
    "axes[4].barh(top_features, top_importances)\n",
    "axes[4].set_xlabel('Feature Importance')\n",
    "axes[4].set_title('Top 10 Feature Importances (Balanced Dataset)')\n",
    "\n",
    "# 6. Missing value treatment visualization\n",
    "missing_before = loan_df.isnull().sum().sum()\n",
    "missing_after = 0  # All missing values were handled\n",
    "axes[5].bar(['Before Treatment', 'After Treatment'], [missing_before, missing_after], \n",
    "            color=['red', 'green'], alpha=0.7)\n",
    "axes[5].set_ylabel('Number of Missing Values')\n",
    "axes[5].set_title('Missing Values Before and After Treatment')\n",
    "for i, v in enumerate([missing_before, missing_after]):\n",
    "    axes[5].text(i, v + max([missing_before, missing_after])*0.01, f'{v}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nData cleaning, feature engineering, and imbalance handling pipeline completed successfully!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this comprehensive notebook, we've covered essential techniques for data cleaning, feature engineering, and handling imbalanced data. Here's a summary of what we've learned:\n",
    "\n",
    "## Data Cleaning:\n",
    "- **Missing Data**: We explored different strategies like deletion, mean/median imputation, KNN imputation, and ML-based imputation\n",
    "- **Outliers**: We learned to detect outliers using statistical methods (IQR, Z-score, Isolation Forest) and handle them through removal, capping, or transformation\n",
    "- **Duplicates**: We identified duplicate records and learned various strategies to handle them\n",
    "- **Data Type Conversion**: We optimized memory usage by converting to appropriate data types\n",
    "- **String Manipulation**: We cleaned and standardized categorical text data\n",
    "- **Data Concatenation**: We combined datasets and features appropriately\n",
    "\n",
    "## Feature Engineering:\n",
    "- **Mathematical Transformations**: Created ratios, squared terms, and logarithmic features\n",
    "- **Binning**: Converted continuous variables into categorical bins\n",
    "- **Interaction Features**: Combined multiple variables to create new features\n",
    "- **Domain-Specific Features**: Created features based on domain knowledge\n",
    "- **Encoding**: Applied one-hot encoding for categorical variables\n",
    "\n",
    "## Handling Imbalanced Data:\n",
    "- **Understanding the Problem**: Recognized how class imbalance affects model performance\n",
    "- **Undersampling**: Random and other techniques to reduce majority class\n",
    "- **Oversampling**: SMOTE, ADASYN, and other methods to increase minority class\n",
    "- **Combined Approaches**: SMOTETomek and other hybrid methods\n",
    "- **Impact Assessment**: Measured how balancing affects model performance\n",
    "\n",
    "## Curse of Dimensionality:\n",
    "- **Problem Recognition**: Understanding how high-dimensional spaces affect distance measures\n",
    "- **Solutions**: Feature selection, extraction, and regularization techniques\n",
    "- **Dimensionality Reduction**: Methods to work with fewer, more relevant features\n",
    "\n",
    "These techniques are fundamental to any data science project and are especially important in credit risk modeling where data quality and feature relevance directly impact business outcomes. The practical example demonstrated how to combine all these techniques in a real-world pipeline, showing the importance of each step in improving model performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 }
}