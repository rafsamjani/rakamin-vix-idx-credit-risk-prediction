{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data Exploration and Understanding\n",
    "\n",
    "**Author**: Rafsamjani Anugrah  \n",
    "**Date**: 2024  \n",
    "**Project**: Credit Risk Prediction - ID/X Partners  \n",
    "\n",
    "## Tujuan Notebook\n",
    "\n",
    "Notebook ini berfokus pada:\n",
    "1. Memuat dan memahami dataset loan data 2007-2014\n",
    "2. Analisis awal struktur data dan karakteristik\n",
    "3. Identifikasi masalah data quality\n",
    "4. Eksplorasi distribusi variabel target\n",
    "5. Persiapan untuk tahap cleaning selanjutnya\n",
    "\n",
    "## Dataset Information\n",
    "\n",
    "- **Source**: Lending Club Loan Data (2007-2014)\n",
    "- **Format**: CSV\n",
    "- **Purpose**: Memprediksi risiko kredit (default vs fully paid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set styling untuk visualisasi\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üìÖ Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek lokasi file dataset\n",
    "data_paths = [\n",
    "    '../data/raw/loan_data_2007_2014.csv',\n",
    "    '../../data/raw/loan_data_2007_2014.csv',\n",
    "    'data/raw/loan_data_2007_2014.csv',\n",
    "    'loan_data_2007_2014.csv'\n",
    "]\n",
    "\n",
    "data_path = None\n",
    "for path in data_paths:\n",
    "    if os.path.exists(path):\n",
    "        data_path = path\n",
    "        break\n",
    "\n",
    "if data_path:\n",
    "    print(f\"‚úÖ Dataset found at: {data_path}\")\n",
    "    \n",
    "    # Load dataset dengan parameter optimal\n",
    "    print(\"üìä Loading dataset...\")\n",
    "    \n",
    "    try:\n",
    "        # Load data dengan sampling untuk preview cepat (jika file besar)\n",
    "        df = pd.read_csv(data_path, low_memory=False)\n",
    "        \n",
    "        print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "        print(f\"üìè Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "        print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {e}\")\n",
    "        df = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Dataset not found in any of the expected paths:\")\n",
    "    for path in data_paths:\n",
    "        print(f\"   - {path}\")\n",
    "    print(\"\\nüí° Please ensure the dataset is available in the data/raw/ directory\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tampilkan informasi dasar dataset\n",
    "if df is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"DATASET OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìä Basic Statistics:\")\n",
    "    print(f\"   Total Records: {df.shape[0]:,}\")\n",
    "    print(f\"   Total Features: {df.shape[1]}\")\n",
    "    print(f\"   File Size: {os.path.getsize(data_path) / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Tampilkan sample data\n",
    "    print(f\"\\nüëÄ First 3 rows:\")\n",
    "    display(df.head(3))\n",
    "    \n",
    "    # Tampilkan tipe data\n",
    "    print(f\"\\nüî¢ Data Types:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"   {dtype}: {count} columns\")\n",
    "    \n",
    "    # Tampilkan informasi kolom\n",
    "    print(f\"\\nüìã Column Information:\")\n",
    "    display(df.info(verbose=False))\n",
    "else:\n",
    "    print(\"‚ùå No dataset available for exploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Column Analysis and Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"COLUMN ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Daftar semua kolom\n",
    "    print(f\"\\nüìù All Columns ({len(df.columns)} total):\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    # Group columns by logical categories\n",
    "    print(f\"\\nüóÇÔ∏è Column Categories:\")\n",
    "    \n",
    "    # Loan characteristics\n",
    "    loan_cols = [col for col in df.columns if any(x in col.lower() \n",
    "                for x in ['loan', 'amount', 'term', 'rate', 'purpose', 'grade'])]\n",
    "    print(f\"\\nüí∞ Loan Characteristics ({len(loan_cols)}):\")\n",
    "    for col in loan_cols:\n",
    "        print(f\"   - {col}\")\n",
    "    \n",
    "    # Borrower information\n",
    "    borrower_cols = [col for col in df.columns if any(x in col.lower() \n",
    "                   for x in ['annual', 'emp', 'home', 'verification', 'dti'])]\n",
    "    print(f\"\\nüë§ Borrower Information ({len(borrower_cols)}):\")\n",
    "    for col in borrower_cols:\n",
    "        print(f\"   - {col}\")\n",
    "    \n",
    "    # Credit history\n",
    "    credit_cols = [col for col in df.columns if any(x in col.lower() \n",
    "                 for x in ['fico', 'credit', 'delinq', 'revol', 'pub_rec', 'earliest'])]\n",
    "    print(f\"\\nüìà Credit History ({len(credit_cols)}):\")\n",
    "    for col in credit_cols:\n",
    "        print(f\"   - {col}\")\n",
    "    \n",
    "    # Status and dates\n",
    "    status_cols = [col for col in df.columns if any(x in col.lower() \n",
    "                 for x in ['status', 'issue', 'last', 'next'])]\n",
    "    print(f\"\\nüìÖ Status and Dates ({len(status_cols)}):\")\n",
    "    for col in status_cols:\n",
    "        print(f\"   - {col}\")\n",
    "else:\n",
    "    print(\"‚ùå No dataset available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis (Loan Status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'loan_status' in df.columns:\n",
    "    print(\"=\"*80)\n",
    "    print(\"TARGET VARIABLE ANALYSIS - LOAN STATUS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Analisis distribusi loan status\n",
    "    status_counts = df['loan_status'].value_counts()\n",
    "    status_percentages = (status_counts / len(df) * 100).round(2)\n",
    "    \n",
    "    print(f\"\\nüìä Loan Status Distribution:\")\n",
    "    print(\"=\"*50)\n",
    "    for status, count in status_counts.items():\n",
    "        percentage = status_percentages[status]\n",
    "        print(f\"   {status:25} : {count:>7,} ({percentage:>6.2f}%)\")\n",
    "    \n",
    "    # Visualisasi distribusi\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Bar chart\n",
    "    status_counts.plot(kind='bar', ax=ax1, color='skyblue')\n",
    "    ax1.set_title('Loan Status Distribution - Count', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Loan Status')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Pie chart\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(status_counts)))\n",
    "    wedges, texts, autotexts = ax2.pie(status_counts, labels=status_counts.index, \n",
    "                                     autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    ax2.set_title('Loan Status Proportion', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Horizontal bar chart untuk top 10\n",
    "    top_10_status = status_counts.head(10)\n",
    "    y_pos = range(len(top_10_status))\n",
    "    ax3.barh(y_pos, top_10_status.values, color='lightcoral')\n",
    "    ax3.set_yticks(y_pos)\n",
    "    ax3.set_yticklabels(top_10_status.index)\n",
    "    ax3.invert_yaxis()  # Labels read top-to-bottom\n",
    "    ax3.set_xlabel('Count')\n",
    "    ax3.set_title('Top 10 Loan Status Types', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Log scale untuk distribusi\n",
    "    ax4.bar(range(len(status_counts)), status_counts.values, color='gold')\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.set_xlabel('Loan Status (sorted)')\n",
    "    ax4.set_ylabel('Count (log scale)')\n",
    "    ax4.set_title('Loan Status Distribution (Log Scale)', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xticks([])  # Hide x-axis labels for clarity\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analisis khusus untuk completed loans\n",
    "    completed_loans = ['Fully Paid', 'Charged Off']\n",
    "    df_completed = df[df['loan_status'].isin(completed_loans)]\n",
    "    \n",
    "    print(f\"\\nüéØ Completed Loans Analysis:\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"   Completed loans (Fully Paid + Charged Off): {len(df_completed):,} ({len(df_completed)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Fully Paid: {len(df_completed[df_completed['loan_status'] == 'Fully Paid']):,}\")\n",
    "    print(f\"   Charged Off: {len(df_completed[df_completed['loan_status'] == 'Charged Off']):,}\")\n",
    "    \n",
    "    # Default rate calculation\n",
    "    if len(df_completed) > 0:\n",
    "        default_rate = (df_completed['loan_status'] == 'Charged Off').mean() * 100\n",
    "        print(f\"   Default rate among completed loans: {default_rate:.2f}%\")\n",
    "        print(f\"   Success rate among completed loans: {100-default_rate:.2f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Loan status column not found or no dataset available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"MISSING VALUES ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate missing values\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_percentages = (missing_counts / len(df) * 100).round(2)\n",
    "    \n",
    "    # Create missing values DataFrame\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Missing Count': missing_counts.values,\n",
    "        'Missing %': missing_percentages.values,\n",
    "        'Data Type': df.dtypes.values\n",
    "    })\n",
    "    \n",
    "    # Sort by missing percentage\n",
    "    missing_df = missing_df.sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    # Summary statistics\n",
    "    total_columns = len(df.columns)\n",
    "    columns_with_missing = (missing_counts > 0).sum()\n",
    "    columns_with_high_missing = (missing_percentages > 50).sum()\n",
    "    \n",
    "    print(f\"\\nüìä Missing Values Summary:\")\n",
    "    print(f\"   Total columns: {total_columns}\")\n",
    "    print(f\"   Columns with missing values: {columns_with_missing} ({columns_with_missing/total_columns*100:.1f}%)\")\n",
    "    print(f\"   Columns with >50% missing: {columns_with_high_missing}\")\n",
    "    print(f\"   Total missing values: {missing_counts.sum():,}\")\n",
    "    \n",
    "    # Display columns with missing values\n",
    "    missing_cols = missing_df[missing_df['Missing Count'] > 0]\n",
    "    \n",
    "    if len(missing_cols) > 0:\n",
    "        print(f\"\\nüìã Columns with Missing Values ({len(missing_cols)} columns):\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Format display\n",
    "        for _, row in missing_cols.iterrows():\n",
    "            missing_bar = '‚ñà' * int(row['Missing %'] / 2)  # Visual bar\n",
    "            print(f\"{row['Column']:<30} | {row['Missing Count']:>8,} | {row['Missing %']:>6.2f}% | {missing_bar}\")\n",
    "        \n",
    "        # Visualize missing values pattern\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Missing values by column (top 20)\n",
    "        top_missing = missing_cols.head(20)\n",
    "        ax1.barh(range(len(top_missing)), top_missing['Missing %'], color='salmon')\n",
    "        ax1.set_yticks(range(len(top_missing)))\n",
    "        ax1.set_yticklabels(top_missing['Column'], fontsize=8)\n",
    "        ax1.invert_yaxis()\n",
    "        ax1.set_xlabel('Missing Percentage (%)')\n",
    "        ax1.set_title('Top 20 Columns by Missing Values', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Missing percentage distribution\n",
    "        missing_bins = [0, 1, 5, 10, 25, 50, 75, 100]\n",
    "        missing_hist, _ = np.histogram(missing_percentages, bins=missing_bins)\n",
    "        \n",
    "        ax2.bar(range(len(missing_hist)-1), missing_hist[:-1], color='lightblue', edgecolor='navy')\n",
    "        ax2.set_xlabel('Missing Percentage Range')\n",
    "        ax2.set_ylabel('Number of Columns')\n",
    "        ax2.set_title('Distribution of Missing Values', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xticks(range(len(missing_bins)-1))\n",
    "        ax2.set_xticklabels([f'{missing_bins[i]}-{missing_bins[i+1]}%' for i in range(len(missing_bins)-1)], \n",
    "                          rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(missing_hist[:-1]):\n",
    "            if v > 0:\n",
    "                ax2.text(i, v + 0.5, str(int(v)), ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Recommendations for handling missing values\n",
    "        print(f\"\\nüí° Missing Values Handling Recommendations:\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        high_missing = missing_df[missing_df['Missing %'] > 50]\n",
    "        if len(high_missing) > 0:\n",
    "            print(f\"\\nüî¥ HIGH MISSING (>50%): Consider dropping these columns\")\n",
    "            for _, row in high_missing.iterrows():\n",
    "                print(f\"   - {row['Column']}: {row['Missing %']:.1f}%\")\n",
    "        \n",
    "        medium_missing = missing_df[(missing_df['Missing %'] > 10) & (missing_df['Missing %'] <= 50)]\n",
    "        if len(medium_missing) > 0:\n",
    "            print(f\"\\nüü° MEDIUM MISSING (10-50%): Requires careful imputation strategy\")\n",
    "            for _, row in medium_missing.head(5).iterrows():\n",
    "                print(f\"   - {row['Column']}: {row['Missing %']:.1f}% ({row['Data Type']})\")\n",
    "            if len(medium_missing) > 5:\n",
    "                print(f\"   ... and {len(medium_missing)-5} more columns\")\n",
    "        \n",
    "        low_missing = missing_df[(missing_df['Missing %'] > 0) & (missing_df['Missing %'] <= 10)]\n",
    "        if len(low_missing) > 0:\n",
    "            print(f\"\\nüü¢ LOW MISSING (0-10%): Simple imputation should work\")\n",
    "            for _, row in low_missing.head(5).iterrows():\n",
    "                print(f\"   - {row['Column']}: {row['Missing %']:.1f}% ({row['Data Type']})\")\n",
    "            if len(low_missing) > 5:\n",
    "                print(f\"   ... and {len(low_missing)-5} more columns\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\n‚úÖ No missing values found in the dataset!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No dataset available for missing values analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Types Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"DATA TYPES ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Analyze data types\n",
    "    dtype_analysis = pd.DataFrame({\n",
    "        'Data Type': df.dtypes.value_counts().index,\n",
    "        'Count': df.dtypes.value_counts().values\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nüìä Data Types Distribution:\")\n",
    "    print(\"=\"*40)\n",
    "    for _, row in dtype_analysis.iterrows():\n",
    "        percentage = row['Count'] / len(df.columns) * 100\n",
    "        print(f\"   {row['Data Type']:<20} : {row['Count']:>3} columns ({percentage:>5.1f}%)\")\n",
    "    \n",
    "    # Detailed analysis by data type\n",
    "    print(f\"\\nüîç Detailed Data Type Analysis:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Numerical columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"\\nüìä Numerical Columns ({len(numeric_cols)}):\")\n",
    "    \n",
    "    # Basic statistics for numerical columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        numeric_stats = df[numeric_cols].describe().T\n",
    "        print(f\"   Range of values:\")\n",
    "        for col in numeric_cols[:10]:  # Show first 10\n",
    "            min_val = numeric_stats.loc[col, 'min']\n",
    "            max_val = numeric_stats.loc[col, 'max']\n",
    "            mean_val = numeric_stats.loc[col, 'mean']\n",
    "            print(f\"     {col:<25}: {min_val:>12,.2f} to {max_val:>12,.2f} (avg: {mean_val:>8,.2f})\")\n",
    "        if len(numeric_cols) > 10:\n",
    "            print(f\"     ... and {len(numeric_cols)-10} more numerical columns\")\n",
    "    \n",
    "    # Categorical columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    print(f\"\\nüìù Categorical Columns ({len(categorical_cols)}):\")\n",
    "    \n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"   Cardinality analysis:\")\n",
    "        for col in categorical_cols[:15]:  # Show first 15\n",
    "            unique_count = df[col].nunique()\n",
    "            sample_values = df[col].dropna().unique()[:3]  # Show sample values\n",
    "            sample_str = ', '.join([str(v) for v in sample_values])\n",
    "            if len(sample_values) < df[col].nunique():\n",
    "                sample_str += ', ...'\n",
    "            print(f\"     {col:<25}: {unique_count:>3} unique values\")\n",
    "            print(f\"     {'':25}   Sample: {sample_str[:50]}\")\n",
    "        if len(categorical_cols) > 15:\n",
    "            print(f\"     ... and {len(categorical_cols)-15} more categorical columns\")\n",
    "    \n",
    "    # DateTime columns (if any)\n",
    "    datetime_cols = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "    print(f\"\\nüìÖ DateTime Columns ({len(datetime_cols)}):\")\n",
    "    for col in datetime_cols:\n",
    "        min_date = df[col].min()\n",
    "        max_date = df[col].max()\n",
    "        print(f\"     {col:<25}: {min_date} to {max_date}\")\n",
    "    \n",
    "    # Potential type conversion candidates\n",
    "    print(f\"\\nüîß Potential Type Conversions:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Look for columns that might need conversion\n",
    "    conversion_candidates = []\n",
    "    \n",
    "    # Check for percentage columns\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            sample_vals = df[col].dropna().head(5).astype(str)\n",
    "            # Check for percentage\n",
    "            if sample_vals.str.contains('%').any():\n",
    "                conversion_candidates.append((col, 'percentage', sample_vals.tolist()))\n",
    "            # Check for dates\n",
    "            elif sample_vals.str.match(r'\\w{3}-\\d{2}').all():\n",
    "                conversion_candidates.append((col, 'date', sample_vals.tolist()))\n",
    "            # Check for numeric with special characters\n",
    "            elif sample_vals.str.contains(r'\\$|,').any():\n",
    "                conversion_candidates.append((col, 'currency', sample_vals.tolist()))\n",
    "    \n",
    "    if conversion_candidates:\n",
    "        print(f\"   Found {len(conversion_candidates)} columns that may need type conversion:\")\n",
    "        for col, conv_type, samples in conversion_candidates[:10]:\n",
    "            print(f\"     - {col} ({conv_type}): {samples[:2]}\")\n",
    "        if len(conversion_candidates) > 10:\n",
    "            print(f\"     ... and {len(conversion_candidates)-10} more columns\")\n",
    "    else:\n",
    "        print(\"   No obvious type conversion candidates found.\")\n",
    "    \n",
    "    # Visualize data type distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Data type pie chart\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(dtype_counts)))\n",
    "    ax1.pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%', \n",
    "           colors=colors, startangle=90)\n",
    "    ax1.set_title('Data Type Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Column count by type\n",
    "    ax2.bar(range(len(dtype_counts)), dtype_counts.values, color=colors)\n",
    "    ax2.set_xlabel('Data Type')\n",
    "    ax2.set_ylabel('Number of Columns')\n",
    "    ax2.set_title('Column Count by Data Type', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xticks(range(len(dtype_counts)))\n",
    "    ax2.set_xticklabels(dtype_counts.index, rotation=45)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(dtype_counts.values):\n",
    "        ax2.text(i, v + 0.5, str(int(v)), ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No dataset available for data type analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initial Statistical Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"INITIAL STATISTICAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Numerical columns statistics\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nüìä Numerical Columns Statistics (Top 15 by range):\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats_df = df[numeric_cols].describe().T\n",
    "        \n",
    "        # Add additional statistics\n",
    "        stats_df['range'] = stats_df['max'] - stats_df['min']\n",
    "        stats_df['missing_pct'] = (df[numeric_cols].isnull().sum() / len(df) * 100)\n",
    "        stats_df['zeros_pct'] = ((df[numeric_cols] == 0).sum() / len(df) * 100)\n",
    "        \n",
    "        # Sort by range (descending)\n",
    "        stats_df = stats_df.sort_values('range', ascending=False)\n",
    "        \n",
    "        # Display top 15\n",
    "        display_cols = ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'range', 'missing_pct', 'zeros_pct']\n",
    "        \n",
    "        # Format for display\n",
    "        for col in stats_df.head(15).index:\n",
    "            row = stats_df.loc[col]\n",
    "            print(f\"\\nüî¢ {col}:\")\n",
    "            print(f\"   Count: {int(row['count']):,} | Missing: {row['missing_pct']:.1f}% | Zeros: {row['zeros_pct']:.1f}%\")\n",
    "            print(f\"   Range: {row['min']:,.2f} to {row['max']:,.2f} (spread: {row['range']:,.2f})\")\n",
    "            print(f\"   Central tendency: Mean={row['mean']:,.2f}, Median={row['50%']:,.2f}\")\n",
    "            print(f\"   Dispersion: Std={row['std']:,.2f}, IQR={row['75%']-row['25%']:,.2f}\")\n",
    "    \n",
    "    # Categorical columns statistics\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\n\\nüìù Categorical Columns Statistics:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        for col in categorical_cols[:10]:  # Analyze first 10 categorical columns\n",
    "            unique_count = df[col].nunique()\n",
    "            missing_count = df[col].isnull().sum()\n",
    "            most_common = df[col].mode().iloc[0] if not df[col].mode().empty else 'N/A'\n",
    "            most_common_pct = (df[col] == most_common).mean() * 100\n",
    "            \n",
    "            print(f\"\\nüìã {col}:\")\n",
    "            print(f\"   Unique values: {unique_count:,}\")\n",
    "            print(f\"   Missing: {missing_count:,} ({missing_count/len(df)*100:.1f}%)\")\n",
    "            print(f\"   Most common: '{most_common}' ({most_common_pct:.1f}%)\")\n",
    "            \n",
    "            # Show top categories\n",
    "            value_counts = df[col].value_counts().head(3)\n",
    "            print(f\"   Top 3 values:\")\n",
    "            for val, count in value_counts.items():\n",
    "                pct = count / len(df) * 100\n",
    "                print(f\"     '{val}': {count:,} ({pct:.1f}%)\")\n",
    "        \n",
    "        if len(categorical_cols) > 10:\n",
    "            print(f\"\\n   ... and {len(categorical_cols)-10} more categorical columns\")\n",
    "    \n",
    "    # Quick outlier detection\n",
    "    print(f\"\\n\\n‚ö†Ô∏è  Potential Outliers (based on IQR method):\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    outliers_detected = 0\n",
    "    for col in numeric_cols[:20]:  # Check first 20 numeric columns\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "        outlier_pct = outliers / len(df) * 100\n",
    "        \n",
    "        if outlier_pct > 1:  # More than 1% outliers\n",
    "            print(f\"   {col}: {outliers:,} outliers ({outlier_pct:.1f}%)\")\n",
    "            outliers_detected += 1\n",
    "    \n",
    "    if outliers_detected == 0:\n",
    "        print(\"   No significant outliers detected in the sample columns.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No dataset available for statistical analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"DATA EXPLORATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Key findings\n",
    "    print(f\"\\nüìä DATASET SUMMARY:\")\n",
    "    print(f\"   ‚Ä¢ Total Records: {df.shape[0]:,}\")\n",
    "    print(f\"   ‚Ä¢ Total Features: {df.shape[1]}\")\n",
    "    print(f\"   ‚Ä¢ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    if 'loan_status' in df.columns:\n",
    "        completed_loans = df[df['loan_status'].isin(['Fully Paid', 'Charged Off'])]\n",
    "        default_rate = (completed_loans['loan_status'] == 'Charged Off').mean() * 100 if len(completed_loans) > 0 else 0\n",
    "        print(f\"   ‚Ä¢ Completed Loans: {len(completed_loans):,} ({len(completed_loans)/len(df)*100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Default Rate: {default_rate:.2f}%\")\n",
    "    \n",
    "    # Data quality issues\n",
    "    missing_cols = (df.isnull().sum() > 0).sum()\n",
    "    high_missing_cols = ((df.isnull().sum() / len(df) * 100) > 50).sum()\n",
    "    \n",
    "    print(f\"\\nüîç DATA QUALITY ISSUES:\")\n",
    "    print(f\"   ‚Ä¢ Columns with missing values: {missing_cols} ({missing_cols/len(df.columns)*100:.1f}%)\")\n",
    "    print(f\"   ‚Ä¢ Columns with >50% missing: {high_missing_cols}\")\n",
    "    print(f\"   ‚Ä¢ Total missing values: {df.isnull().sum().sum():,}\")\n",
    "    \n",
    "    # Feature categories\n",
    "    numeric_count = len(df.select_dtypes(include=[np.number]).columns)\n",
    "    categorical_count = len(df.select_dtypes(include=['object', 'category']).columns)\n",
    "    \n",
    "    print(f\"\\nüìã FEATURE DISTRIBUTION:\")\n",
    "    print(f\"   ‚Ä¢ Numerical features: {numeric_count}\")\n",
    "    print(f\"   ‚Ä¢ Categorical features: {categorical_count}\")\n",
    "    print(f\"   ‚Ä¢ DateTime features: {len(df.select_dtypes(include=['datetime64']).columns)}\")\n",
    "    \n",
    "    print(f\"\\nüéØ NEXT STEPS FOR DATA CLEANING:\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"   1. Filter dataset to completed loans only (Fully Paid + Charged Off)\")\n",
    "    print(\"   2. Handle missing values based on importance and percentage\")\n",
    "    print(\"   3. Convert data types (percentages, dates, currencies)\")\n",
    "    print(\"   4. Create derived features (FICO average, financial ratios)\")\n",
    "    print(\"   5. Handle outliers in key numerical variables\")\n",
    "    print(\"   6. Encode categorical variables for modeling\")\n",
    "    \n",
    "    # Save basic exploration results\n",
    "    exploration_summary = {\n",
    "        'dataset_shape': df.shape,\n",
    "        'total_records': len(df),\n",
    "        'total_features': len(df.columns),\n",
    "        'missing_columns': missing_cols,\n",
    "        'high_missing_columns': high_missing_cols,\n",
    "        'numeric_features': numeric_count,\n",
    "        'categorical_features': categorical_count,\n",
    "        'completed_loans': len(completed_loans) if 'loan_status' in df.columns else 0,\n",
    "        'default_rate': default_rate if 'loan_status' in df.columns else 0,\n",
    "        'exploration_date': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Save summary to file\n",
    "    import json\n",
    "    try:\n",
    "        with open('../data/exploration_summary.json', 'w') as f:\n",
    "            json.dump(exploration_summary, f, indent=2, default=str)\n",
    "        print(f\"\\nüíæ Exploration summary saved to '../data/exploration_summary.json'\")\n",
    "    except:\n",
    "        print(f\"\\n‚ö†Ô∏è  Could not save exploration summary file\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data exploration completed successfully!\")\n",
    "    print(f\"üìù Ready to proceed to '02_data_cleaning.ipynb'\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No dataset available for summary\")\n",
    "    print(\"\\nüí° Please ensure the dataset is available and try again\")\n",
    "    print(\"   Expected location: data/raw/loan_data_2007_2014.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (credit-risk-env)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}