{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics and Data Analytics for Data Science\n",
    "\n",
    "In this notebook, we will explore fundamental statistical concepts and techniques that are essential for data science and exploratory data analysis (EDA). As a data scientist, understanding these concepts is crucial when carrying out exploratory analysis to find out about the related data that will be used in creating a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction to Descriptive Statistics](#introduction-to-descriptive-statistics)\n",
    "2. [Types of Data](#types-of-data)\n",
    "3. [Level of Measurement](#level-of-measurement)\n",
    "4. [Measure of Central Tendency](#measure-of-central-tendency)\n",
    "5. [Which is the Best Measure](#which-is-the-best-measure)\n",
    "6. [Measure of Dispersion](#measure-of-dispersion)\n",
    "7. [Measure of Asymmetry](#measure-of-asymmetry)\n",
    "8. [Probability Distribution](#probability-distribution)\n",
    "9. [Normal Distribution](#normal-distribution)\n",
    "10. [Univariate Analysis](#univariate-analysis)\n",
    "11. [Bivariate Analysis](#bivariate-analysis)\n",
    "12. [Multivariate Analysis](#multivariate-analysis)\n",
    "13. [Inferential Analysis](#inferential-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create sample data that mimics Lending Club Dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "\n",
    "data = {\n",
    "    'loan_amnt': np.random.normal(15000, 10000, n_samples),\n",
    "    'int_rate': np.random.normal(12, 4, n_samples),\n",
    "    'annual_inc': np.random.normal(75000, 30000, n_samples),\n",
    "    'dti': np.random.normal(15, 10, n_samples),\n",
    "    'fico_score': np.random.normal(700, 50, n_samples),\n",
    "    'emp_length': np.random.gamma(2, 2, n_samples),\n",
    "    'loan_status': np.random.choice([0, 1], n_samples, p=[0.8, 0.2])  # 0: Fully Paid, 1: Charged Off\n",
    "}\n",
    "\n",
    "# Ensure no negative values\n",
    "data['loan_amnt'] = np.abs(data['loan_amnt'])\n",
    "data['annual_inc'] = np.abs(data['annual_inc'])\n",
    "data['dti'] = np.abs(data['dti'])\n",
    "data['fico_score'] = np.clip(data['fico_score'], 300, 850)\n",
    "data['emp_length'] = np.clip(data['emp_length'], 0, 15)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Sample Lending Club Dataset\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(df.info())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Descriptive Statistics\n",
    "\n",
    "Descriptive statistics are used to summarize and describe the main features of a dataset. They provide simple summaries about the sample and the measures. Together with simple graphics analysis, they form the basis of virtually every quantitative analysis of data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Basic descriptive statistics\n",
    "print(\"Descriptive Statistics for the Lending Club Dataset\")\n",
    "print(df.describe())\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  Mean: {df[col].mean():.2f}\")\n",
    "    print(f\"  Median: {df[col].median():.2f}\")\n",
    "    print(f\"  Mode: {df[col].mode()[0]:.2f}\")\n",
    "    print(f\"  Std Dev: {df[col].std():.2f}\")\n",
    "    print(f\"  Variance: {df[col].var():.2f}\")\n",
    "    print(f\"  Skewness: {df[col].skew():.2f}\")\n",
    "    print(f\"  Kurtosis: {df[col].kurtosis():.2f}\")\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Data\n",
    "\n",
    "Data can be classified into different types based on their nature and characteristics:\n",
    "\n",
    "1. **Quantitative Data**: Numerical values that can be measured or counted\n",
    "   - **Discrete**: Countable values (e.g., number of loans)\n",
    "   - **Continuous**: Measurable values (e.g., loan amount, interest rate)\n",
    "\n",
    "2. **Qualitative Data**: Non-numerical values that describe qualities or characteristics\n",
    "   - **Nominal**: Categories without order (e.g., loan purpose: debt consolidation, credit card)\n",
    "   - **Ordinal**: Categories with order (e.g., credit grade: A, B, C, D, E)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Adding categorical variables to our sample data to demonstrate types of data\n",
    "df['grade'] = pd.cut(df['fico_score'], \n",
    "                     bins=[0, 580, 620, 660, 700, 740, 780, 850], \n",
    "                     labels=['G', 'F', 'E', 'D', 'C', 'B', 'A'])\n",
    "\n",
    "df['home_ownership'] = np.random.choice(['MORTGAGE', 'RENT', 'OWN'], n_samples, p=[0.4, 0.3, 0.3])\n",
    "\n",
    "df['purpose'] = np.random.choice(['debt_consolidation', 'credit_card', 'home_improvement', 'major_purchase', 'small_business'], \n",
    "                                n_samples, p=[0.3, 0.2, 0.2, 0.2, 0.1])\n",
    "\n",
    "# Show data types\n",
    "print(\"Data Types in the Dataset:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Count different types of data\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric Columns ({len(numeric_cols)}): {numeric_cols}\")\n",
    "print(f\"Categorical Columns ({len(categorical_cols)}): {categorical_cols}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level of Measurement\n",
    "\n",
    "The level of measurement refers to how precisely data is recorded. There are four levels:\n",
    "\n",
    "1. **Nominal Scale**: Categories without any quantitative value\n",
    "2. **Ordinal Scale**: Categories with a meaningful order\n",
    "3. **Interval Scale**: Ordered categories with equal intervals\n",
    "4. **Ratio Scale**: Interval scale with a true zero point"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Demonstrating different levels of measurement\n",
    "print(\"Levels of Measurement in our Dataset:\")\n",
    "print(\"\\n1. Nominal Scale:\")\n",
    "print(f\"   - Home Ownership: {df['home_ownership'].unique()}\")\n",
    "print(f\"   - Purpose: {df['purpose'].unique()}\")\n",
    "\n",
    "print(\"\\n2. Ordinal Scale:\")\n",
    "print(f\"   - Grade: {df['grade'].cat.categories.tolist()} (A is better than G)\")\n",
    "\n",
    "print(\"\\n3. Interval Scale:\")\n",
    "print(f\"   - FICO Score: Continuous scale with equal intervals\")\n",
    "\n",
    "print(\"\\n4. Ratio Scale:\")\n",
    "print(f\"   - Loan Amount: Has true zero, ratios make sense (e.g., $20,000 is twice $10,000)\")\n",
    "print(f\"   - Annual Income: Has true zero, ratios make sense\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure of Central Tendency\n",
    "\n",
    "Measures of central tendency are statistical measures that represent the center or typical value of a dataset. The three main measures are:\n",
    "\n",
    "1. **Mean**: The arithmetic average of the data\n",
    "2. **Median**: The middle value when data is arranged in order\n",
    "3. **Mode**: The most frequently occurring value in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculating measures of central tendency for key variables\n",
    "central_tendency = df[['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score']].describe()\n",
    "\n",
    "print(\"Measures of Central Tendency:\")\n",
    "for col in ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score']:\n",
    "    mean_val = df[col].mean()\n",
    "    median_val = df[col].median()\n",
    "    mode_val = df[col].mode()[0]\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Mean: {mean_val:.2f}\")\n",
    "    print(f\"  Median: {median_val:.2f}\")\n",
    "    print(f\"  Mode: {mode_val:.2f}\")\n",
    "\n",
    "# Visualizing central tendency\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score']):\n",
    "    axes[i].hist(df[col], bins=50, alpha=0.7, color='lightblue', edgecolor='black')\n",
    "    \n",
    "    mean_val = df[col].mean()\n",
    "    median_val = df[col].median()\n",
    "    mode_val = df[col].mode()[0]\n",
    "    \n",
    "    axes[i].axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.2f}')\n",
    "    axes[i].axvline(median_val, color='green', linestyle='--', label=f'Median: {median_val:.2f}')\n",
    "    axes[i].axvline(mode_val, color='orange', linestyle='--', label=f'Mode: {mode_val:.2f}')\n",
    "    \n",
    "    axes[i].set_title(f'{col} Distribution')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which is the Best Measure?\n",
    "\n",
    "The choice of central tendency measure depends on the distribution of the data:\n",
    "\n",
    "- **Mean**: Best for normally distributed data, sensitive to outliers\n",
    "- **Median**: Best for skewed data, robust against outliers\n",
    "- **Mode**: Best for categorical data or identifying most common value\n",
    "\n",
    "In our lending dataset, some variables like loan amount and annual income are often skewed, making the median a more representative measure."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Analyzing skewness to determine the best measure of central tendency\n",
    "print(\"Skewness Analysis - Determining the Best Measure:\")\n",
    "for col in ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score']:\n",
    "    skewness = df[col].skew()\n",
    "    mean_val = df[col].mean()\n",
    "    median_val = df[col].median()\n",
    "    mode_val = df[col].mode()[0]\n",
    "    \n",
    "    print(f\"\\n{col}: Skewness = {skewness:.2f}\")\n",
    "    \n",
    "    if abs(skewness) < 0.5:\n",
    "        best_measure = \"Mean (relatively symmetric)\"\n",
    "    elif skewness > 0.5:\n",
    "        best_measure = \"Median (right-skewed)\"\n",
    "    else:\n",
    "        best_measure = \"Median (left-skewed)\"\n",
    "    \n",
    "    print(f\"  Best measure: {best_measure}\")\n",
    "    print(f\"  Mean: {mean_val:.2f}, Median: {median_val:.2f}, Mode: {mode_val:.2f}\")\n",
    "    \n",
    "# Box plots to visualize skewness\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score']):\n",
    "    df.boxplot(column=col, ax=axes[i])\n",
    "    axes[i].set_title(f'{col} - Box Plot (Skewness: {df[col].skew():.2f})')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[-1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure of Dispersion\n",
    "\n",
    "Measures of dispersion describe the spread or variability of the data. Key measures include:\n",
    "\n",
    "1. **Range**: Difference between maximum and minimum values\n",
    "2. **Variance**: Average of squared deviations from the mean\n",
    "3. **Standard Deviation**: Square root of variance\n",
    "4. **Interquartile Range (IQR)**: Difference between 75th and 25th percentiles"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculating measures of dispersion\n",
    "print(\"Measures of Dispersion:\")\n",
    "for col in ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score']:\n",
    "    range_val = df[col].max() - df[col].min()\n",
    "    variance = df[col].var()\n",
    "    std_dev = df[col].std()\n",
    "    iqr = df[col].quantile(0.75) - df[col].quantile(0.25)\n",
    "    cv = (df[col].std() / df[col].mean()) * 100  # Coefficient of variation\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Range: {range_val:.2f}\")\n",
    "    print(f\"  Variance: {variance:.2f}\")\n",
    "    print(f\"  Standard Deviation: {std_dev:.2f}\")\n",
    "    print(f\"  IQR: {iqr:.2f}\")\n",
    "    print(f\"  Coefficient of Variation: {cv:.2f}%\")\n",
    "\n",
    "# Visualizing dispersion with violin plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score']):\n",
    "    sns.violinplot(y=df[col], ax=axes[i])\n",
    "    axes[i].set_title(f'{col} - Violin Plot')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[-1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure of Asymmetry\n",
    "\n",
    "Skewness measures the asymmetry of the distribution of values. A distribution is said to be:\n",
    "\n",
    "- **Symmetrical** (Skewness ≈ 0): The distribution is balanced on both sides of the center\n",
    "- **Positively Skewed** (Skewness > 0): The tail extends to the right\n",
    "- **Negatively Skewed** (Skewness < 0): The tail extends to the left\n",
    "\n",
    "Kurtosis measures the 'tailedness' of the distribution:\n",
    "\n",
    "- **Mesokurtic** (Kurtosis ≈ 3): Normal distribution\n",
    "- **Leptokurtic** (Kurtosis > 3): Heavy tails (more outliers)\n",
    "- **Platykurtic** (Kurtosis < 3): Light tails (fewer outliers)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Analyzing skewness and kurtosis\n",
    "print(\"Measure of Asymmetry (Skewness and Kurtosis):\")\n",
    "for col in ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score']:\n",
    "    skewness = df[col].skew()\n",
    "    kurtosis = df[col].kurtosis()\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Skewness: {skewness:.3f}\")\n",
    "    if abs(skewness) < 0.5:\n",
    "        print(f\"  Distribution: Approximately symmetric\")\n",
    "    elif skewness > 0.5:\n",
    "        print(f\"  Distribution: Positively skewed (right tail)\")\n",
    "    else:\n",
    "        print(f\"  Distribution: Negatively skewed (left tail)\")\n",
    "    \n",
    "    print(f\"  Kurtosis: {kurtosis:.3f}\")\n",
    "    if kurtosis > 0:\n",
    "        print(f\"  Distribution: Heavy tails (leptokurtic)\")\n",
    "    elif kurtosis < 0:\n",
    "        print(f\"  Distribution: Light tails (platykurtic)\")\n",
    "    else:\n",
    "        print(f\"  Distribution: Normal tails (mesokurtic)\")\n",
    "\n",
    "# Plotting histograms with skewness and kurtosis information\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score']):\n",
    "    axes[i].hist(df[col], bins=50, alpha=0.7, color='lightblue', edgecolor='black', density=True)\n",
    "    \n",
    "    # Add normal distribution curve for comparison\n",
    "    x = np.linspace(df[col].min(), df[col].max(), 1000)\n",
    "    normal_curve = norm.pdf(x, df[col].mean(), df[col].std())\n",
    "    axes[i].plot(x, normal_curve, color='red', linestyle='--', label='Normal Distribution')\n",
    "    \n",
    "    skewness = df[col].skew()\n",
    "    kurtosis = df[col].kurtosis()\n",
    "    \n",
    "    axes[i].set_title(f'{col} Distribution\\nSkewness: {skewness:.3f}, Kurtosis: {kurtosis:.3f}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[-1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Distribution\n",
    "\n",
    "A probability distribution describes how probabilities are distributed over the values of a random variable. Common types include:\n",
    "\n",
    "1. **Discrete Probability Distributions**: For discrete random variables\n",
    "   - Binomial, Poisson, Discrete Uniform\n",
    "\n",
    "2. **Continuous Probability Distributions**: For continuous random variables\n",
    "   - Normal, Exponential, Gamma, Beta"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Demonstrating different probability distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Normal distribution (Continuous)\n",
    "x = np.linspace(-3, 3, 1000)\n",
    "y = norm.pdf(x)\n",
    "axes[0,0].plot(x, y, 'b-', label='Normal Distribution')\n",
    "axes[0,0].set_title('Normal Distribution')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "axes[0,0].legend()\n",
    "\n",
    "# 2. Probability mass function for default rate (Discrete)\n",
    "default_counts = df['loan_status'].value_counts()\n",
    "axes[0,1].bar(default_counts.index, default_counts.values/len(df), color=['green', 'red'], alpha=0.7)\n",
    "axes[0,1].set_title('Default Status Probability Distribution')\n",
    "axes[0,1].set_xlabel('Loan Status (0=Fully Paid, 1=Default)')\n",
    "axes[0,1].set_ylabel('Probability')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "for i, v in enumerate(default_counts.values/len(df)):\n",
    "    axes[0,1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# 3. Histogram of loan amount with fitted distribution\n",
    "axes[0,2].hist(df['loan_amnt'], bins=50, density=True, alpha=0.7, color='lightblue', label='Actual Data')\n",
    "x_l = np.linspace(df['loan_amnt'].min(), df['loan_amnt'].max(), 1000)\n",
    "fitted_normal = norm.pdf(x_l, df['loan_amnt'].mean(), df['loan_amnt'].std())\n",
    "axes[0,2].plot(x_l, fitted_normal, 'r-', label='Fitted Normal')\n",
    "axes[0,2].set_title('Loan Amount Distribution')\n",
    "axes[0,2].set_xlabel('Loan Amount')\n",
    "axes[0,2].set_ylabel('Density')\n",
    "axes[0,2].legend()\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Grade distribution (Categorical)\n",
    "grade_counts = df['grade'].value_counts().sort_index()\n",
    "axes[1,0].bar(grade_counts.index.astype(str), grade_counts.values/len(df), color=sns.color_palette(\"husl\", len(grade_counts)), alpha=0.7)\n",
    "axes[1,0].set_title('Credit Grade Probability Distribution')\n",
    "axes[1,0].set_xlabel('Credit Grade')\n",
    "axes[1,0].set_ylabel('Probability')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "for i, v in enumerate(grade_counts.values/len(df)):\n",
    "    axes[1,0].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "# 5. Interest rate distribution\n",
    "axes[1,1].hist(df['int_rate'], bins=50, density=True, alpha=0.7, color='lightgreen', label='Actual Data')\n",
    "axes[1,1].set_title('Interest Rate Distribution')\n",
    "axes[1,1].set_xlabel('Interest Rate (%)')\n",
    "axes[1,1].set_ylabel('Density')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. FICO score distribution\n",
    "axes[1,2].hist(df['fico_score'], bins=50, density=True, alpha=0.7, color='lightcoral', label='Actual Data')\n",
    "axes[1,2].set_title('FICO Score Distribution')\n",
    "axes[1,2].set_xlabel('FICO Score')\n",
    "axes[1,2].set_ylabel('Density')\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculating probability ranges\n",
    "print(\"Probability Calculations:\")\n",
    "print(f\"1. Probability of default: {df['loan_status'].mean():.3f}\")\n",
    "print(f\"2. Probability of loan amount > $20,000: {(df['loan_amnt'] > 20000).mean():.3f}\")\n",
    "print(f\"3. Probability of FICO score > 700: {(df['fico_score'] > 700).mean():.3f}\")\n",
    "print(f\"4. Probability of interest rate > 15%: {(df['int_rate'] > 15).mean():.3f}\")\n",
    "print(f\"5. Probability of DTI > 20%: {(df['dti'] > 20).mean():.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution\n",
    "\n",
    "The normal (Gaussian) distribution is one of the most important probability distributions in statistics. It's characterized by its bell-shaped curve and is defined by two parameters: mean (μ) and standard deviation (σ).\n",
    "\n",
    "Properties of normal distribution:\n",
    "1. Symmetrical around the mean\n",
    "2. Mean, median, and mode are equal\n",
    "3. Approximately 68% of values lie within 1σ of the mean\n",
    "4. Approximately 95% of values lie within 2σ of the mean\n",
    "5. Approximately 99.7% of values lie within 3σ of the mean (Empirical Rule)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Demonstrating normal distribution on our data\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "for i, col in enumerate(['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score']):\n",
    "    row = i // 3\n",
    "    col_idx = i % 3\n",
    "    \n",
    "    axes[row, col_idx].hist(df[col], bins=50, density=True, alpha=0.7, color='lightblue', label='Actual Data')\n",
    "    \n",
    "    # Add normal distribution curve\n",
    "    x = np.linspace(df[col].min(), df[col].max(), 1000)\n",
    "    normal_curve = norm.pdf(x, df[col].mean(), df[col].std())\n",
    "    axes[row, col_idx].plot(x, normal_curve, color='red', linestyle='--', label='Normal Distribution')\n",
    "    \n",
    "    # Add mean and std lines\n",
    "    mean_val = df[col].mean()\n",
    "    std_val = df[col].std()\n",
    "    axes[row, col_idx].axvline(mean_val, color='green', linestyle='-', label='Mean')\n",
    "    axes[row, col_idx].axvline(mean_val + std_val, color='orange', linestyle='--', label='+1σ')\n",
    "    axes[row, col_idx].axvline(mean_val - std_val, color='orange', linestyle='--', label='-1σ')\n",
    "    axes[row, col_idx].axvline(mean_val + 2*std_val, color='purple', linestyle=':', label='+2σ')\n",
    "    axes[row, col_idx].axvline(mean_val - 2*std_val, color='purple', linestyle=':', label='-2σ')\n",
    "    \n",
    "    axes[row, col_idx].set_title(f'{col} - Normal Distribution Comparison\\n(μ={df[col].mean():.2f}, σ={df[col].std():.2f})')\n",
    "    axes[row, col_idx].set_xlabel(col)\n",
    "    axes[row, col_idx].set_ylabel('Density')\n",
    "    axes[row, col_idx].legend()\n",
    "    axes[row, col_idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove the last subplot if needed\n",
    "axes[1, 2].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Empirical rule demonstration\n",
    "print(\"Empirical Rule Verification:\")\n",
    "for col in ['fico_score']:  # Using FICO score as it's closest to normal\n",
    "    mean_val = df[col].mean()\n",
    "    std_val = df[col].std()\n",
    "    \n",
    "    within_1std = ((df[col] >= mean_val - std_val) & (df[col] <= mean_val + std_val)).mean()\n",
    "    within_2std = ((df[col] >= mean_val - 2*std_val) & (df[col] <= mean_val + 2*std_val)).mean()\n",
    "    within_3std = ((df[col] >= mean_val - 3*std_val) & (df[col] <= mean_val + 3*std_val)).mean()\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Within 1σ: {within_1std:.3f} (Expected: 0.68)\")\n",
    "    print(f\"  Within 2σ: {within_2std:.3f} (Expected: 0.95)\")\n",
    "    print(f\"  Within 3σ: {within_3std:.3f} (Expected: 0.997)\")\n",
    "    \n",
    "# Z-score calculation\n",
    "print(\"\\nZ-Score Example for FICO Score:\")\n",
    "df['fico_zscore'] = (df['fico_score'] - df['fico_score'].mean()) / df['fico_score'].std()\n",
    "print(f\"  Sample Z-scores - Mean: {df['fico_zscore'].mean():.3f}, Std: {df['fico_zscore'].std():.3f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Analysis\n",
    "\n",
    "Univariate analysis involves examining one variable at a time to understand its distribution, central tendency, and dispersion. This is the foundation of statistical analysis and helps identify patterns, outliers, and the shape of data distribution."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Univariate Analysis\n",
    "print(\"Univariate Analysis Results:\")\n",
    "\n",
    "# Summary statistics for numerical variables\n",
    "numerical_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score', 'emp_length']\n",
    "\n",
    "for col in numerical_cols:\n",
    "    print(f\"\\n{col.upper()}\")\n",
    "    print(f\"  Mean: {df[col].mean():.2f}\")\n",
    "    print(f\"  Median: {df[col].median():.2f}\")\n",
    "    print(f\"  Std: {df[col].std():.2f}\")\n",
    "    print(f\"  Min: {df[col].min():.2f}\")\n",
    "    print(f\"  Max: {df[col].max():.2f}\")\n",
    "    print(f\"  25%: {df[col].quantile(0.25):.2f}\")\n",
    "    print(f\"  75%: {df[col].quantile(0.75):.2f}\")\n",
    "    print(f\"  Skewness: {df[col].skew():.2f}\")\n",
    "    print(f\"  Kurtosis: {df[col].kurtosis():.2f}\")\n",
    "    \n",
    "# Visualizations for univariate analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    axes[i].hist(df[col], bins=30, color='lightblue', edgecolor='black', alpha=0.7)\n",
    "    axes[i].set_title(f'{col} Distribution')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# For categorical variables\n",
    "categorical_cols = ['grade', 'home_ownership', 'purpose', 'loan_status']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    value_counts = df[col].value_counts()\n",
    "    axes[i].bar(value_counts.index.astype(str), value_counts.values, color=sns.color_palette(\"husl\", len(value_counts)), alpha=0.7)\n",
    "    axes[i].set_title(f'{col} Distribution')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, v in enumerate(value_counts.values):\n",
    "        axes[i].text(j, v + max(value_counts.values)*0.01, str(v), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Analysis\n",
    "\n",
    "Bivariate analysis examines the relationship between two variables. It helps understand correlations, associations, and dependencies between variables. Techniques include:\n",
    "\n",
    "1. Correlation analysis\n",
    "2. Scatter plots\n",
    "3. Cross-tabulation for categorical variables\n",
    "4. Box plots comparing categories"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Bivariate Analysis\n",
    "print(\"Bivariate Analysis Results:\")\n",
    "\n",
    "# Correlation matrix\n",
    "corr_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score', 'loan_status']\n",
    "corr_matrix = df[corr_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True, fmt='.3f')\n",
    "plt.title('Correlation Matrix - Numerical Variables')\n",
    "plt.show()\n",
    "\n",
    "# Strongest correlations\n",
    "print(\"\\nStrongest Correlations:\")\n",
    "corrs = corr_matrix.unstack().sort_values(key=lambda x: abs(x), ascending=False)\n",
    "corrs = corrs[corrs.index.get_level_values(0) != corrs.index.get_level_values(1)]  # Remove self-correlations\n",
    "print(corrs.head(10))\n",
    "\n",
    "# Scatter plots for key relationships\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# 1. FICO Score vs Interest Rate\n",
    "axes[0].scatter(df['fico_score'], df['int_rate'], alpha=0.5, color='blue')\n",
    "axes[0].set_xlabel('FICO Score')\n",
    "axes[0].set_ylabel('Interest Rate (%)')\n",
    "axes[0].set_title('FICO Score vs Interest Rate')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Annual Income vs Loan Amount\n",
    "axes[1].scatter(df['annual_inc'], df['loan_amnt'], alpha=0.5, color='green')\n",
    "axes[1].set_xlabel('Annual Income')\n",
    "axes[1].set_ylabel('Loan Amount')\n",
    "axes[1].set_title('Annual Income vs Loan Amount')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. DTI vs Interest Rate\n",
    "axes[2].scatter(df['dti'], df['int_rate'], alpha=0.5, color='red')\n",
    "axes[2].set_xlabel('Debt-to-Income Ratio')\n",
    "axes[2].set_ylabel('Interest Rate (%)')\n",
    "axes[2].set_title('DTI vs Interest Rate')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. FICO Score vs Loan Status\n",
    "sns.boxplot(data=df, x='loan_status', y='fico_score', ax=axes[3])\n",
    "axes[3].set_title('FICO Score by Loan Status')\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Interest Rate vs Loan Status\n",
    "sns.boxplot(data=df, x='loan_status', y='int_rate', ax=axes[4])\n",
    "axes[4].set_title('Interest Rate by Loan Status')\n",
    "axes[4].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Loan Amount by Grade\n",
    "df.boxplot(column='loan_amnt', by='grade', ax=axes[5])\n",
    "axes[5].set_title('Loan Amount by Grade')\n",
    "axes[5].set_xlabel('Grade')\n",
    "axes[5].set_ylabel('Loan Amount')\n",
    "axes[5].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cross-tabulation for categorical variables\n",
    "print(\"\\nCross-tabulation Examples:\")\n",
    "ct_grade_status = pd.crosstab(df['grade'], df['loan_status'], normalize='index')\n",
    "print(\"\\nDefault Rate by Credit Grade:\")\n",
    "print(ct_grade_status)\n",
    "\n",
    "ct_home_status = pd.crosstab(df['home_ownership'], df['loan_status'], normalize='index')\n",
    "print(\"\\nDefault Rate by Home Ownership:\")\n",
    "print(ct_home_status)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Analysis\n",
    "\n",
    "Multivariate analysis examines relationships among multiple variables simultaneously. It's more complex than bivariate analysis and includes techniques like:\n",
    "\n",
    "1. Multiple regression\n",
    "2. Principal Component Analysis (PCA)\n",
    "3. Cluster analysis\n",
    "4. Factor analysis\n",
    "5. Multivariate analysis of variance (MANOVA)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Multivariate Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Prepare data for multivariate analysis\n",
    "features_for_analysis = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score']\n",
    "X = df[features_for_analysis].copy()\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=features_for_analysis)\n",
    "\n",
    "# Principal Component Analysis (PCA)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot PCA results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['loan_status'], cmap='viridis', alpha=0.6)\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.title('PCA: Loan Data Projection')\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "# Show how original features contribute to principal components\n",
    "plt.subplot(1, 2, 2)\n",
    "components_df = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=features_for_analysis)\n",
    "sns.heatmap(components_df, annot=True, cmap='coolwarm', center=0, fmt='.3f')\n",
    "plt.title('PCA Components')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"PCA Explained Variance Ratio:\")\n",
    "print(f\"First 2 components explain {(pca.explained_variance_ratio_[:2].sum()):.2%} of the variance\")\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='Set1', alpha=0.6)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('K-means Clusters (PCA space)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(df['fico_score'], df['int_rate'], c=clusters, cmap='Set1', alpha=0.6)\n",
    "plt.xlabel('FICO Score')\n",
    "plt.ylabel('Interest Rate')\n",
    "plt.title('K-means Clusters (Feature space)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "cluster_summary = pd.DataFrame({\n",
    "    'cluster': clusters,\n",
    "    'fico_score': df['fico_score'],\n",
    "    'int_rate': df['int_rate'],\n",
    "    'annual_inc': df['annual_inc'],\n",
    "    'loan_status': df['loan_status']\n",
    "}).groupby('cluster').mean()\n",
    "\n",
    "sns.heatmap(cluster_summary.T, annot=True, cmap='viridis', fmt='.2f')\n",
    "plt.title('Cluster Characteristics')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCluster Characteristics:\")\n",
    "print(cluster_summary)\n",
    "\n",
    "# Multiple regression example (using statsmodels for detailed stats)\n",
    "print(\"\\nMultivariate Analysis: Multiple Regression\")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Predict interest rate based on multiple features\n",
    "X_reg = df[['fico_score', 'annual_inc', 'dti', 'loan_amnt']].copy()\n",
    "y_reg = df['int_rate']\n",
    "\n",
    "# Standardize features\n",
    "X_reg_scaled = StandardScaler().fit_transform(X_reg)\n",
    "X_reg_scaled_df = pd.DataFrame(X_reg_scaled, columns=X_reg.columns)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_reg_scaled_df, y_reg)\n",
    "\n",
    "# Model performance\n",
    "r2 = model.score(X_reg_scaled_df, y_reg)\n",
    "predictions = model.predict(X_reg_scaled_df)\n",
    "mse = np.mean((y_reg - predictions) ** 2)\n",
    "\n",
    "print(f\"R² Score: {r2:.3f}\")\n",
    "print(f\"Mean Squared Error: {mse:.3f}\")\n",
    "\n",
    "# Feature coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X_reg.columns,\n",
    "    'Coefficient': model.coef_,\n",
    "    'Abs_Coefficient': np.abs(model.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Coefficients (Standardized):\")\n",
    "print(coefficients)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferential Analysis\n",
    "\n",
    "Inferential analysis uses sample data to make inferences about a larger population. It involves hypothesis testing, confidence intervals, and statistical significance testing. In the context of our credit risk analysis, inferential statistics help us understand whether observed patterns in our sample data are likely to exist in the broader population of borrowers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Inferential Analysis\n",
    "from scipy import stats\n",
    "\n",
    "print(\"Inferential Analysis Results:\")\n",
    "\n",
    "# 1. Confidence Intervals\n",
    "confidence_level = 0.95\n",
    "z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "\n",
    "for col in ['fico_score', 'int_rate', 'annual_inc']:\n",
    "    sample_mean = df[col].mean()\n",
    "    sample_std = df[col].std()\n",
    "    sample_size = len(df)\n",
    "    margin_error = z_score * (sample_std / np.sqrt(sample_size))\n",
    "    \n",
    "    ci_lower = sample_mean - margin_error\n",
    "    ci_upper = sample_mean + margin_error\n",
    "    \n",
    "    print(f\"\\n{col} - {confidence_level*100}% Confidence Interval:\")\n",
    "    print(f\"  Sample Mean: {sample_mean:.2f}\")\n",
    "    print(f\"  CI: [{ci_lower:.2f}, {ci_upper:.2f}]\")\n",
    "    print(f\"  Margin of Error: ±{margin_error:.2f}\")\n",
    "\n",
    "# 2. Hypothesis Testing - Do A-grade borrowers have significantly higher FICO scores?\n",
    "print(\"\\nHypothesis Testing Example:\")\n",
    "print(\"H0: Mean FICO score of A-grade borrowers = Mean FICO score of G-grade borrowers\")\n",
    "print(\"H1: Mean FICO score of A-grade borrowers > Mean FICO score of G-grade borrowers\")\n",
    "\n",
    "# Get FICO scores for A and G grades\n",
    "fico_a = df[df['grade'] == 'A']['fico_score']\n",
    "fico_g = df[df['grade'] == 'G']['fico_score']\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_value = stats.ttest_ind(fico_a, fico_g)\n",
    "\n",
    "print(f\"\\nT-Test Results (A-grade vs G-grade FICO scores):\")\n",
    "print(f\"  A-grade mean FICO: {fico_a.mean():.2f}\")\n",
    "print(f\"  G-grade mean FICO: {fico_g.mean():.2f}\")\n",
    "print(f\"  T-statistic: {t_stat:.3f}\")\n",
    "print(f\"  P-value: {p_value:.3e}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"  Result: Reject H0 - A-grade borrowers have significantly higher FICO scores\")\n",
    "else:\n",
    "    print(\"  Result: Fail to reject H0 - No significant difference in FICO scores\")\n",
    "\n",
    "# 3. Hypothesis Testing - Do default rates differ by home ownership?\n",
    "print(\"\\nHypothesis Testing Example 2:\")\n",
    "print(\"H0: Default rates are the same across home ownership types\")\n",
    "print(\"H1: Default rates differ by home ownership types\")\n",
    "\n",
    "ct_home_status = pd.crosstab(df['home_ownership'], df['loan_status'])\n",
    "chi2, p, dof, expected = stats.chi2_contingency(ct_home_status)\n",
    "\n",
    "print(f\"\\nChi-square Test Results:\")\n",
    "print(f\"  Chi-square statistic: {chi2:.3f}\")\n",
    "print(f\"  P-value: {p:.3e}\")\n",
    "print(f\"  Degrees of freedom: {dof}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"  Result: Reject H0 - Default rates differ significantly by home ownership\")\n",
    "else:\n",
    "    print(\"  Result: Fail to reject H0 - No significant difference in default rates\")\n",
    "    \n",
    "# Visualize inferential results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# 1. Confidence intervals visualization\n",
    "means = [df['fico_score'].mean(), df['int_rate'].mean(), df['annual_inc'].mean()]\n",
    "categorical_names = ['FICO Score', 'Interest Rate', 'Annual Income']\n",
    "\n",
    "for i, col in enumerate(['fico_score', 'int_rate', 'annual_inc']):\n",
    "    sample_mean = df[col].mean()\n",
    "    sample_std = df[col].std()\n",
    "    sample_size = len(df)\n",
    "    margin_error = z_score * (sample_std / np.sqrt(sample_size))\n",
    "    \n",
    "    axes[0].bar(i, sample_mean, yerr=margin_error, capsize=10, alpha=0.7, color=['blue', 'red', 'green'][i])\n",
    "    \n",
    "axes[0].set_xticks(range(len(categorical_names)))\n",
    "axes[0].set_xticklabels(categorical_names)\n",
    "axes[0].set_title('Confidence Intervals')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Grade vs FICO boxplot\n",
    "df.boxplot(column='fico_score', by='grade', ax=axes[1])\n",
    "axes[1].set_title('FICO Score by Grade (Hypothesis Test 1)')\n",
    "axes[1].set_xlabel('Grade')\n",
    "axes[1].set_ylabel('FICO Score')\n",
    "\n",
    # 3. Default rates by home ownership\n",
    "default_rates = df.groupby('home_ownership')['loan_status'].mean()\n",
    "axes[2].bar(default_rates.index, default_rates.values, color=['purple', 'orange', 'brown'], alpha=0.7)\n",
    "axes[2].set_title('Default Rates by Home Ownership Type')\n",
    "axes[2].set_xlabel('Home Ownership')\n",
    "axes[2].set_ylabel('Default Rate')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "for i, v in enumerate(default_rates.values):\n",
    "    axes[2].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nStatistical Summary:\")\n",
    "print(f\"- Sample size: {len(df)}\")\n",
    "print(f\"- Default rate: {df['loan_status'].mean():.2%}\")\n",
    "print(f\"- Average FICO score: {df['fico_score'].mean():.1f}\")\n",
    "print(f\"- Average interest rate: {df['int_rate'].mean():.2f}%\")\n",
    "print(f\"- Average loan amount: ${df['loan_amnt'].mean():,.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this comprehensive statistics and data analytics notebook, we've explored various fundamental concepts that are crucial for data science:\n",
    "\n",
    "1. **Descriptive Statistics**: We calculated and visualized measures of central tendency (mean, median, mode) and dispersion (variance, standard deviation, IQR) for our lending dataset.\n",
    "\n",
    "2. **Types of Data**: We identified quantitative (loan amount, interest rate) and qualitative (grade, purpose) variables in our dataset.\n",
    "\n",
    "3. **Level of Measurement**: We categorized variables according to nominal, ordinal, interval, and ratio scales.\n",
    "\n",
    "4. **Measures of Central Tendency**: We determined which measure (mean vs median) is most appropriate based on the skewness of each distribution.\n",
    "\n",
    "5. **Measures of Dispersion**: We calculated and visualized range, variance, standard deviation, and IQR.\n",
    "\n",
    "6. **Measures of Asymmetry**: We analyzed skewness and kurtosis for each variable.\n",
    "\n",
    "7. **Probability Distribution**: We examined the distribution of our variables and compared them to theoretical distributions.\n",
    "\n",
    "8. **Normal Distribution**: We tested how closely our variables follow normal distribution using the empirical rule.\n",
    "\n",
    "9. **Univariate Analysis**: We analyzed each variable individually to understand its distribution.\n",
    "\n",
    "10. **Bivariate Analysis**: We examined relationships between pairs of variables using correlation and visualization.\n",
    "\n",
    "11. **Multivariate Analysis**: We applied PCA and clustering to understand patterns across multiple variables simultaneously.\n",
    "\n",
    "12. **Inferential Analysis**: We performed hypothesis testing and calculated confidence intervals to make inferences about the population.\n",
    "\n",
    "These statistical techniques form the foundation for exploratory data analysis (EDA) and are essential for creating predictive models in data science. They help ensure that models are built on solid statistical understanding of the underlying data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 }
}