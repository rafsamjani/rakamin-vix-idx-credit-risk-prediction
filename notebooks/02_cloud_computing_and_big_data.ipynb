{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Computing and Big Data for Data Science\n",
    "\n",
    "In this notebook, we'll explore the fundamental concepts of cloud computing and big data implementations in the context of data science. These technologies have revolutionized how organizations process, store, and analyze massive datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Cloud Computing Definition](#cloud-computing-definition)\n",
    "2. [Importance of Cloud Data Services](#importance-of-cloud-data-services)\n",
    "3. [Big Data Implementation Examples](#big-data-implementation-examples)\n",
    "4. [Data Warehousing Concepts](#data-warehousing-concepts)\n",
    "5. [Big Data Processing Principles](#big-data-processing-principles)\n",
    "6. [Application to Lending Club Dataset](#application-to-lending-club-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Computing Definition\n",
    "\n",
    "Cloud computing is the delivery of computing services—including servers, storage, databases, networking, software, analytics, and intelligence—over the Internet (\"the cloud\"). It offers faster innovation, flexible resources, and economies of scale."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create sample data to simulate cloud computing and big data scenarios\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "\n",
    "# Simulate Lending Club data (to connect with our main project)\n",
    "data = {\n",
    "    'loan_id': range(1, n_samples + 1),\n",
    "    'loan_amnt': np.random.normal(15000, 10000, n_samples),\n",
    "    'int_rate': np.random.normal(12, 4, n_samples),\n",
    "    'annual_inc': np.random.normal(75000, 30000, n_samples),\n",
    "    'dti': np.random.normal(15, 10, n_samples),\n",
    "    'fico_score': np.random.normal(700, 50, n_samples),\n",
    "    'emp_length': np.random.gamma(2, 2, n_samples),\n",
    "    'loan_status': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),  # 0: Fully Paid, 1: Charged Off\n",
    "    'grade': pd.cut(np.random.normal(700, 50, n_samples), \n",
    "                    bins=[0, 580, 620, 660, 700, 740, 780, 850], \n",
    "                    labels=['G', 'F', 'E', 'D', 'C', 'B', 'A']),\n",
    "    'home_ownership': np.random.choice(['MORTGAGE', 'RENT', 'OWN'], n_samples, p=[0.4, 0.3, 0.3]),\n",
    "    'purpose': np.random.choice(['debt_consolidation', 'credit_card', 'home_improvement', 'major_purchase', 'small_business'], \n",
    "                                n_samples, p=[0.3, 0.2, 0.2, 0.2, 0.1]),\n",
    "    'region': np.random.choice(['Northeast', 'Southeast', 'Midwest', 'Southwest', 'West'], n_samples),\n",
    "    'application_date': pd.date_range('2010-01-01', periods=n_samples, freq='H')[:n_samples]\n",
    "}\n",
    "\n",
    "# Ensure no negative values and realistic ranges\n",
    "data['loan_amnt'] = np.abs(data['loan_amnt'])\n",
    "data['annual_inc'] = np.abs(data['annual_inc'])\n",
    "data['dti'] = np.abs(data['dti'])\n",
    "data['fico_score'] = np.clip(data['fico_score'], 300, 850)\n",
    "data['emp_length'] = np.clip(data['emp_length'], 0, 15)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Cloud Computing and Big Data - Sample Lending Club Dataset\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset Shape: {df.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance of Cloud Data Services\n",
    "\n",
    "Cloud data services have become crucial for data science teams due to several key advantages:\n",
    "\n",
    "1. **Scalability**: Automatically scale resources up or down based on demand\n",
    "2. **Cost-effectiveness**: Pay only for what you use, reducing infrastructure costs\n",
    "3. **Flexibility**: Access to a wide range of data processing and machine learning tools\n",
    "4. **Reliability**: Built-in redundancy and disaster recovery\n",
    "5. **Collaboration**: Easy sharing of data and models across teams\n",
    "6. **Innovation**: Access to cutting-edge tools and services"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Benefits of Cloud Computing\n",
    "benefits = [\n",
    "    'Scalability',\n",
    "    'Cost-effectiveness',\n",
    "    'Flexibility',\n",
    "    'Reliability',\n",
    "    'Collaboration',\n",
    "    'Innovation'\n",
    "]\n",
    "importance_scores = [9, 8, 8, 9, 7, 8]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(x=benefits, y=importance_scores, \n",
    "           text=importance_scores, textposition='auto',\n",
    "           marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD'])\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Importance of Cloud Computing Benefits for Data Science',\n",
    "    xaxis_title='Benefits',\n",
    "    yaxis_title='Importance Score (1-10)',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Cloud service provider comparison\n",
    "providers = ['AWS', 'Google Cloud', 'Microsoft Azure']\n",
    "data_science_features = [12, 10, 9]\n",
    "ml_services = [15, 12, 11]\n",
    "storage_options = [8, 7, 8]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Data Science Tools', x=providers, y=data_science_features),\n",
    "    go.Bar(name='ML Services', x=providers, y=ml_services),\n",
    "    go.Bar(name='Storage Options', x=providers, y=storage_options)\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Cloud Service Providers - Data Science Capabilities',\n",
    "    xaxis_title='Cloud Provider',\n",
    "    yaxis_title='Number of Services/Features',\n",
    "    barmode='group',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Cloud Computing for Data Science Services:\")\n",
    "print(\"\\n1. Data Storage Solutions:\")\n",
    "print(\"   - Amazon S3 (Simple Storage Service) - Object storage\")\n",
    "print(\"   - Google Cloud Storage - Scalable storage solution\")\n",
    "print(\"   - Azure Blob Storage - Unstructured data storage\")\n",
    "\n",
    "print(\"\\n2. Data Processing Services:\")\n",
    "print(\"   - AWS EMR (Elastic MapReduce) - Managed Hadoop framework\")\n",
    "print(\"   - Google Dataproc - Managed Spark and Hadoop\")\n",
    "print(\"   - Azure HDInsight - Managed analytics service\")\n",
    "\n",
    "print(\"\\n3. Data Analytics Services:\")\n",
    "print(\"   - AWS Redshift - Data warehousing\")\n",
    "print(\"   - Google BigQuery - Serverless data warehouse\")\n",
    "print(\"   - Azure Synapse - Enterprise data analytics\")\n",
    "\n",
    "print(\"\\n4. Machine Learning Services:\")\n",
    "print(\"   - AWS SageMaker - End-to-end ML platform\")\n",
    "print(\"   - Google AI Platform - ML development and deployment\")\n",
    "print(\"   - Azure ML Studio - Drag-and-drop ML development\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Implementation Examples\n",
    "\n",
    "Big data implementations across different industries have transformed how organizations operate and make decisions. Here are some key examples:\n",
    "\n",
    "1. **Retail and Customer Analytics**: Personalization, inventory management, demand forecasting\n",
    "2. **Manufacturing and Predictive Maintenance**: Equipment monitoring, predictive maintenance, quality control\n",
    "3. **Telecommunications and Network Optimization**: Network performance, customer experience, fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization: Big Data Use Cases by Industry\n",
    "industries = ['Retail', 'Manufacturing', 'Telecom', 'Finance', 'Healthcare', 'Transportation']\n",
    "big_data_adoption = [8.5, 7.8, 8.9, 9.2, 7.0, 6.5]\n",
    "roi_scores = [7.5, 8.0, 8.5, 8.8, 6.5, 7.0]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, \n",
    "                    subplot_titles=('Big Data Adoption by Industry', 'ROI Scores by Industry'))\n",
    "\n",
    "fig.add_trace(go.Bar(x=industries, y=big_data_adoption, name='Adoption',\n",
    "                     marker_color='lightblue'), row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(x=industries, y=roi_scores, name='ROI Score',\n",
    "                     marker_color='lightgreen'), row=1, col=2)\n",
    "\n",
    "fig.update_layout(title='Big Data Implementation Across Industries',\n",
    "                  height=500)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Detailed explanation of each implementation\n",
    "print(\"Big Data Implementation Examples:\")\n",
    "\n",
    "print(\"\\n1. Retail and Customer Analytics\")\n",
    "print(\"   - Personalization: Analyzing customer behavior, purchase history, and preferences\")\n",
    "print(\"     to offer personalized recommendations and targeted marketing.\")\n",
    "print(\"   - Inventory Management: Predicting demand patterns to optimize inventory levels\")\n",
    "print(\"     and reduce waste.\")\n",
    "print(\"   - Customer Segmentation: Grouping customers based on behavior, demographics, and\")\n",
    "print(\"     preferences for targeted marketing strategies.\")\n",
    "\n",
    "# Simulate retail data scenario\n",
    "retail_customers = 10000\n",
    "purchase_freq = np.random.exponential(0.5, retail_customers)\n",
    "avg_purchase = np.random.normal(100, 50, retail_customers)\n",
    "retail_df = pd.DataFrame({'customer_id': range(retail_customers),\n",
    "                         'purchase_freq': purchase_freq,\n",
    "                         'avg_purchase': np.abs(avg_purchase)})\n",
    "\n",
    "fig = px.scatter(retail_df.sample(500), x='purchase_freq', y='avg_purchase', \n",
    "                 title='Simulated Retail Customer Data: Purchase Frequency vs Average Purchase',\n",
    "                 labels={'purchase_freq': 'Purchase Frequency', 'avg_purchase': 'Average Purchase Amount'},\n",
    "                 opacity=0.6)\n",
    "fig.update_layout(height=400)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n2. Manufacturing and Predictive Maintenance\")\n",
    "print(\"   - Equipment Monitoring: Collecting real-time sensor data from machines\")\n",
    "print(\"     to detect anomalies and predict failures.\")\n",
    "print(\"   - Predictive Maintenance: Using ML models to predict when equipment\")\n",
    "print(\"     will fail before it happens, reducing downtime.\")\n",
    "print(\"   - Quality Control: Analyzing production data to identify defects\")\n",
    "print(\"     early in the manufacturing process.\")\n",
    "\n",
    "# Simulate manufacturing data scenario\n",
    "sensors = ['Temp', 'Vibration', 'Pressure', 'Humidity', 'Flow']\n",
    "sensor_readings = {sensor: np.random.normal(50, 10, 5000) for sensor in sensors}\n",
    "manufacturing_df = pd.DataFrame(sensor_readings)\n",
    "\n",
    "# Add anomaly detection simulation\n",
    "manufacturing_df['anomaly_score'] = np.random.exponential(1, 5000)\n",
    "manufacturing_df['failure_risk'] = (manufacturing_df['anomaly_score'] > 3).astype(int)\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, \n",
    "                    subplot_titles=('Temperature', 'Vibration', 'Pressure', 'Anomaly Detection'))\n",
    "\n",
    "for i, sensor in enumerate(['Temp', 'Vibration', 'Pressure']):\n",
    "    row = (i // 2) + 1\n",
    "    col = (i % 2) + 1\n",
    "    fig.add_trace(go.Scatter(y=manufacturing_df[sensor].iloc[:100], \n",
    "                             mode='lines', name=sensor), row=row, col=col)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=manufacturing_df['anomaly_score'], nbinsx=50, \n",
    "                           name='Anomaly Score'), row=2, col=2)\n",
    "\n",
    "fig.update_layout(title='Simulated Manufacturing Sensor Data Analysis', height=600)\n",
    "fig.show()\n",
    "\n",
    "print(\"\\n3. Telecommunications and Network Optimization\")\n",
    "print(\"   - Network Performance: Analyzing network traffic patterns to optimize\")\n",
    "print(\"     routing and prevent congestion.\")\n",
    "print(\"   - Customer Experience: Monitoring service quality and customer usage\")\n",
    "print(\"     patterns to improve service delivery.\")\n",
    "print(\"   - Fraud Detection: Identifying unusual usage patterns that might\")\n",
    "print(\"     indicate fraudulent activity.\")\n",
    "\n",
    "# Simulate telecom data scenario\n",
    "time_range = pd.date_range('2023-01-01', periods=8760, freq='H')  # One year of hourly data\n",
    "traffic_volume = 1000 + 500 * np.sin(np.arange(8760) * 2 * np.pi / (24 * 7)) + np.random.normal(0, 100, 8760)  # Weekly pattern\n",
    "latency = 20 + 5 * np.random.exponential(1, 8760)\n",
    "bandwidth_usage = 50 + 20 * np.random.beta(2, 5, 8760)\n",
    "\n",
    "telecom_df = pd.DataFrame({\n",
    "    'timestamp': time_range,\n",
    "    'traffic_volume': np.abs(traffic_volume),\n",
    "    'latency': np.abs(latency),\n",
    "    'bandwidth_usage': np.abs(bandwidth_usage)\n",
    "})\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, \n",
    "                    subplot_titles=('Traffic Volume', 'Latency', 'Bandwidth Usage', 'Network Performance'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=telecom_df['timestamp'][:168], y=telecom_df['traffic_volume'][:168], \n",
    "                         mode='lines', name='Traffic'), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=telecom_df['timestamp'][:168], y=telecom_df['latency'][:168], \n",
    "                         mode='lines', name='Latency'), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=telecom_df['timestamp'][:168], y=telecom_df['bandwidth_usage'][:168], \n",
    "                         mode='lines', name='Bandwidth'), row=2, col=1)\n",
    "fig.add_trace(go.Scatter3d(x=telecom_df['traffic_volume'][:500], \n",
    "                           y=telecom_df['latency'][:500], \n",
    "                           z=telecom_df['bandwidth_usage'][:500], \n",
    "                           mode='markers',\n",
    "                           marker=dict(size=3),\n",
    "                           name='Performance'), row=2, col=2)\n",
    "\n",
    "fig.update_layout(title='Simulated Telecom Network Data Analysis', height=700)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Warehousing Concepts\n",
    "\n",
    "Data warehousing is the foundation for business intelligence and analytics. It involves storing large volumes of data from multiple sources in a way that makes it easy to analyze and report on. Key concepts include:\n",
    "\n",
    "1. **Fact Tables**: Store quantitative data about business transactions\n",
    "2. **Dimension Tables**: Store descriptive attributes that provide context to facts\n",
    "3. **Star Schema**: A logical arrangement of tables centered around a fact table\n",
    "4. **Snowflake Schema**: A normalized form of star schema\n",
    "5. **Galaxy Schema (Fact Constellation)**: Multiple fact tables sharing dimension tables\n",
    "6. **Principles of Massively Parallel Processing (MPP) Databases**: Optimized for analytical queries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create a simulated data warehouse for Lending Club\n",
    "\n",
    "# Dimension Tables\n",
    "dates_dim = pd.DataFrame({\n",
    "    'date_id': range(len(df)),\n",
    "    'date': df['application_date'],\n",
    "    'year': df['application_date'].dt.year,\n",
    "    'month': df['application_date'].dt.month,\n",
    "    'day': df['application_date'].dt.day,\n",
    "    'quarter': df['application_date'].dt.quarter\n",
    "})\n",
    "\n",
    "customer_dim = pd.DataFrame({\n",
    "    'customer_id': df['loan_id'],\n",
    "    'fico_score': df['fico_score'],\n",
    "    'annual_income': df['annual_inc'],\n",
    "    'home_ownership': df['home_ownership'],\n",
    "    'employment_length': df['emp_length'],\n",
    "    'region': df['region']\n",
    "})\n",
    "\n",
    "loan_dim = pd.DataFrame({\n",
    "    'loan_id': df['loan_id'],\n",
    "    'loan_amount': df['loan_amnt'],\n",
    "    'interest_rate': df['int_rate'],\n",
    "    'grade': df['grade'],\n",
    "    'purpose': df['purpose'],\n",
    "    'debt_to_income': df['dti']\n",
    "})\n",
    "\n",
    "# Fact Table\n",
    "fact_table = pd.DataFrame({\n",
    "    'date_id': range(len(df)),\n",
    "    'customer_id': df['loan_id'],\n",
    "    'loan_id': df['loan_id'],\n",
    "    'loan_amount': df['loan_amnt'],\n",
    "    'interest_rate': df['int_rate'],\n",
    "    'default_status': df['loan_status'],\n",
    "    'application_date_key': range(len(df))\n",
    "})\n",
    "\n",
    "print(\"Data Warehousing Example - Lending Club Data Warehouse\")\n",
    "print(\"\\nDates Dimension Table:\")\n",
    "print(dates_dim.head())\n",
    "print(f\"Shape: {dates_dim.shape}\")\n",
    "\n",
    "print(\"\\nCustomer Dimension Table:\")\n",
    "print(customer_dim.head())\n",
    "print(f\"Shape: {customer_dim.shape}\")\n",
    "\n",
    "print(\"\\nLoan Dimension Table:\")\n",
    "print(loan_dim.head())\n",
    "print(f\"Shape: {loan_dim.shape}\")\n",
    "\n",
    "print(\"\\nFact Table:\")\n",
    "print(fact_table.head())\n",
    "print(f\"Shape: {fact_table.shape}\")\n",
    "\n",
    "# Visualization of Star Schema\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Create positions for the star schema\n",
    "center = (0, 0)  # Fact table position\n",
    "dimensions = [\n",
    "    (-1, 1, \"Dates Dimension\"),\n",
    "    (1, 1, \"Customer Dimension\"),\n",
    "    (1, -1, \"Loan Dimension\"),\n",
    "    (-1, -1, \"Other Dimensions\")\n",
    "]\n",
    "\n",
    "# Plot fact table\n",
    "fig.add_trace(go.Scatter(x=[center[0]], y=[center[1]], \n",
    "                         mode='markers+text', \n",
    "                         text=['Fact Table'],\n",
    "                         textposition='middle center',\n",
    "                         marker=dict(size=40, color='red', symbol='square'),\n",
    "                         name='Fact Table'))\n",
    "\n",
    "# Plot dimension tables\n",
    "for x, y, name in dimensions:\n",
    "    fig.add_trace(go.Scatter(x=[x], y=[y], \n",
    "                             mode='markers+text', \n",
    "                             text=[name],\n",
    "                             textposition='middle center',\n",
    "                             marker=dict(size=30, color='blue', symbol='circle'),\n",
    "                             name=name))\n",
    "    \n",
    "    # Connect dimension to fact table\n",
    "    fig.add_trace(go.Scatter(x=[center[0], x], y=[center[1], y],\n",
    "                             mode='lines',\n",
    "                             line=dict(color='gray', width=2),\n",
    "                             showlegend=False))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Star Schema Visualization - Lending Club Data Warehouse',\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Explain the schemas\n",
    "print(\"\\nSchema Explanations:\")\n",
    "print(\"\\n1. Star Schema:\")\n",
    "print(\"   - Central fact table surrounded by dimension tables\")\n",
    "print(\"   - Denormalized structure for fast query performance\")\n",
    "print(\"   - Simple joins for analysis\")\n",
    "\n",
    "print(\"\\n2. Snowflake Schema:\")\n",
    "print(\"   - Normalized version of star schema\")\n",
    "print(\"   - Dimension tables further normalized\")\n",
    "print(\"   - Reduced data redundancy\")\n",
    "\n",
    "print(\"\\n3. Galaxy Schema:\")\n",
    "print(\"   - Multiple fact tables sharing dimension tables\")\n",
    "print(\"   - Used when multiple business processes are related\")\n",
    "print(\"   - Example: Loan applications + Collections + Payments\")\n",
    "\n",
    "# Demonstrate MPP principles with simulated query performance\n",
    "import time\n",
    "\n",
    "# Simulate different data processing approaches\n",
    "data_sizes = [1000, 5000, 10000, 15000, 20000]\n",
    "traditional_processing = [0.1, 0.8, 2.5, 5.0, 8.5]  # Times in seconds\n",
    "mpp_processing = [0.1, 0.2, 0.4, 0.6, 0.9]  # Times in seconds with parallel processing\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=data_sizes, y=traditional_processing, \n",
    "                         mode='lines+markers', name='Traditional Processing',\n",
    "                         line=dict(color='red')))\n",
    "fig.add_trace(go.Scatter(x=data_sizes, y=mpp_processing, \n",
    "                         mode='lines+markers', name='MPP Processing',\n",
    "                         line=dict(color='green')))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Performance Comparison: Traditional vs MPP Processing',\n",
    "    xaxis_title='Data Size',\n",
    "    yaxis_title='Processing Time (seconds)',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nMPP Database Principles:\")\n",
    "print(\"1. Parallel Processing: Distribute work across multiple nodes\")\n",
    "print(\"2. Data Distribution: Partition data across nodes for parallel access\")\n",
    "print(\"3. Query Optimization: Optimize queries to leverage parallelization\")\n",
    "print(\"4. Columnar Storage: Store data in columns for analytical queries\")\n",
    "print(\"5. Compression: Reduce storage requirements and improve I/O\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Processing Principles\n",
    "\n",
    "Big data is characterized by the 5 V's: Volume, Velocity, Variety, Veracity, and Value. Processing big data across multiple nodes or servers requires specialized frameworks and principles:\n",
    "\n",
    "1. **Distributed Computing**: Breaking down large problems into smaller tasks\n",
    "2. **MapReduce**: Processing model for large datasets\n",
    "3. **Hadoop Ecosystem**: Framework for distributed storage and processing\n",
    "4. **Apache Spark**: In-memory computing for big data\n",
    "5. **Real-time Processing**: Stream processing for immediate insights"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualization of Big Data Processing Concepts\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Volume Growth', 'Velocity Processing', 'Variety Sources', '5 V\\'s of Big Data'),\n",
    "    specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# 1. Volume Growth\n",
    "years = list(range(2015, 2025))\n",
    "data_volume = [1, 1.8, 3.2, 5.1, 7.9, 12.0, 17.5, 25.0, 35.0, 48.0]  # in zettabytes\n",
    "fig.add_trace(go.Scatter(x=years, y=data_volume, mode='lines+markers', name='Data Volume'), row=1, col=1)\n",
    "\n",
    "# 2. Velocity Processing\n",
    "processing_types = ['Batch', 'Near Real-time', 'Real-time', 'Stream']\n",
    "processing_time = [3600, 60, 5, 0.1]  # seconds\n",
    "fig.add_trace(go.Bar(x=processing_types, y=processing_time, name='Processing Speed'), row=1, col=2)\n",
    "\n",
    "# 3. Variety Sources\n",
    "data_types = ['Structured', 'Semi-structured', 'Unstructured']\n",
    "percentage = [25, 35, 40]\n",
    "fig.add_trace(go.Bar(x=data_types, y=percentage, name='Data Type Distribution'), row=2, col=1)\n",
    "\n",
    "# 4. 5 V's of Big Data\n",
    "v_names = ['Volume', 'Velocity', 'Variety', 'Veracity', 'Value']\n",
    "v_importance = [9, 8, 8, 7, 9]\n",
    "fig.add_trace(go.Bar(x=v_names, y=v_importance, name='Importance (1-10)'), row=2, col=2)\n",
    "\n",
    "fig.update_layout(height=700, title_text=\"Big Data Processing Concepts\")\n",
    "fig.show()\n",
    "\n",
    "# Simulate distributed processing\n",
    "print(\"Big Data Processing Principles Demonstration:\")\n",
    "print(\"\\n1. Distributed Computing Example (Pseudo-code):\")\n",
    "print(\"   # Instead of processing all data on one machine:\")\n",
    "print(\"   single_machine_result = process_large_dataset(data)\")\n",
    "print(\"   \")\n",
    "print(\"   # We distribute it across multiple nodes:\")\n",
    "print(\"   partitions = split_data(data, num_nodes)\")\n",
    "print(\"   results = []\")\n",
    "print(\"   for node_idx in range(num_nodes):\")\n",
    "print(\"       result = process_partition(partitions[node_idx])\")\n",
    "print(\"       results.append(result)\")\n",
    "print(\"   final_result = combine_results(results)\")\n",
    "\n",
    "print(\"\\n2. MapReduce Concept:\")\n",
    "print(\"   - Map: Apply function to each data element independently\")\n",
    "print(\"   - Reduce: Combine results from map phase\")\n",
    "print(\"   Example for counting loan defaults by grade:\")\n",
    "print(\"   Map: (A, 1), (B, 0), (A, 1), (C, 1) -> (A, [1, 1]), (B, [0]), (C, [1])\")\n",
    "print(\"   Reduce: (A, sum([1, 1])), (B, sum([0])), (C, sum([1])) -> (A, 2), (B, 0), (C, 1)\")\n",
    "\n",
    "# Demonstrate concept with our data\n",
    "print(f\"\\n3. Demonstrating with our Lending Club dataset:\")\n",
    "default_counts_by_grade = df.groupby('grade')['loan_status'].sum()\n",
    "total_counts_by_grade = df.groupby('grade')['loan_status'].count()\n",
    "default_rates_by_grade = (default_counts_by_grade / total_counts_by_grade) * 100\n",
    "\n",
    "print(\"   Default Rates by Grade:\")\n",
    "for grade, rate in default_rates_by_grade.items():\n",
    "    print(f\"     Grade {grade}: {rate:.2f}%\")\n",
    "\n",
    "# Visualization of MapReduce concept\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Original data\n",
    "df_sample = df.head(100)\n",
    "axes[0].scatter(df_sample['fico_score'], df_sample['int_rate'], \n",
    "                c=df_sample['grade'].cat.codes, cmap='Set1', alpha=0.7)\n",
    "axes[0].set_title('Original Dataset')\n",
    "axes[0].set_xlabel('FICO Score')\n",
    "axes[0].set_ylabel('Interest Rate')\n",
    "\n",
    "# After mapping (grouping by grade)\n",
    "grade_means = df_sample.groupby('grade')[['fico_score', 'int_rate']].mean()\n",
    "axes[1].scatter(grade_means['fico_score'], grade_means['int_rate'], \n",
    "                s=200, c=range(len(grade_means)), cmap='Set1')\n",
    "axes[1].set_title('After Mapping (Grouped by Grade)')\n",
    "axes[1].set_xlabel('Avg FICO Score')\n",
    "axes[1].set_ylabel('Avg Interest Rate')\n",
    "\n",
    "# After reducing (aggregated values)\n",
    "axes[2].bar(default_rates_by_grade.index, default_rates_by_grade.values, \n",
    "            color=sns.color_palette(\"husl\", len(default_rates_by_grade)), alpha=0.7)\n",
    "axes[2].set_title('After Reducing (Default Rates)')\n",
    "axes[2].set_xlabel('Grade')\n",
    "axes[2].set_ylabel('Default Rate (%)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Big Data Technologies\n",
    "technologies = {\n",
    "    'Hadoop': ['HDFS', 'MapReduce', 'YARN'],\n",
    "    'Spark': ['Spark Core', 'Spark SQL', 'MLlib', 'Spark Streaming'],\n",
    "    'NoSQL': ['MongoDB', 'Cassandra', 'HBase'],\n",
    "    'Cloud Platforms': ['AWS', 'GCP', 'Azure'],\n",
    "    'Streaming': ['Kafka', 'Storm', 'Flink']\n",
    "}\n",
    "\n",
    "print(\"\\n4. Big Data Technologies:\")\n",
    "for tech, subtechs in technologies.items():\n",
    "    print(f\"   {tech}: {', '.join(subtechs)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to Lending Club Dataset\n",
    "\n",
    "Now let's apply these cloud computing and big data concepts to our Lending Club dataset. We'll demonstrate how these technologies would be used in a real-world scenario to analyze loan data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cloud and Big Data Application to Lending Club Dataset\n",
    "\n",
    "print(\"Cloud and Big Data Application to Lending Club Dataset\")\n",
    "\n",
    "# 1. Data Pipeline Simulation\n",
    "print(\"\\n1. Data Pipeline - Simulating Real-World Processing\")\n",
    "\n",
    "# Simulate data coming from different sources\n",
    "print(\"   - Credit Bureaus: FICO scores, credit history\")\n",
    "print(\"   - Internal Loans: Application data, payment history\")\n",
    "print(\"   - Employment Verification: Income, employment duration\")\n",
    "print(\"   - Social Media: Additional risk indicators\")\n",
    "\n",
    "# Create simulated pipeline metrics\n",
    "pipeline_metrics = {\n",
    "    'data_sources': ['Credit Bureau', 'Internal Systems', 'Employment', 'External APIs'],\n",
    "    'records_per_minute': [50000, 100000, 25000, 10000],\n",
    "    'data_volume_tb_per_month': [2.5, 5.0, 1.2, 0.5]\n",
    "}\n",
    "\n",
    "pipeline_df = pd.DataFrame(pipeline_metrics)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('Records Processed per Minute', 'Data Volume per Month'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "fig.add_trace(go.Bar(x=pipeline_df['data_sources'], y=pipeline_df['records_per_minute'], \n",
    "                     name='Records/Minute'), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=pipeline_df['data_sources'], y=pipeline_df['data_volume_tb_per_month'], \n",
    "                     name='Volume (TB/month)'), row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=400, title_text=\"Data Pipeline Metrics - Lending Club\")\n",
    "fig.show()\n",
    "\n",
    "# 2. Real-time Processing Simulation\n",
    "print(\"\\n2. Real-time Processing - Credit Risk Assessment\")\n",
    "\n",
    "# Simulate real-time processing of loan applications\n",
    "time_steps = list(range(1, 11))  # 10 time units\n",
    "applications_per_unit = [100, 150, 120, 180, 200, 160, 140, 190, 170, 210]\n",
    "processed_per_unit = [95, 145, 115, 175, 190, 155, 135, 185, 165, 200]  # With some processing delay\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=time_steps, y=applications_per_unit, \n",
    "                         mode='lines+markers', name='Incoming Applications',\n",
    "                         line=dict(color='blue')))\n",
    "fig.add_trace(go.Scatter(x=time_steps, y=processed_per_unit, \n",
    "                         mode='lines+markers', name='Processed Applications',\n",
    "                         line=dict(color='green')))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Real-time Loan Application Processing',\n",
    "    xaxis_title='Time Unit',\n",
    "    yaxis_title='Number of Applications',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# 3. Scalability demonstration\n",
    "print(\"\\n3. Scalability - Cloud Resource Adjustment\")\n",
    "\n",
    "# Simulate resource usage during different periods\n",
    "hours = list(range(24))\n",
    "base_traffic = np.sin(np.array(hours) * 2 * np.pi / 24)  # Daily pattern\n",
    "weekend_factor = np.where([(h in [0, 1, 2, 3, 4]) for h in hours], 0.7, 1.0)  # Weekend vs weekday\n",
    "traffic_load = (1 + base_traffic) * weekend_factor * 100\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=hours, y=traffic_load, mode='lines+markers', \n",
    "                         name='Traffic Load',\n",
    "                         line=dict(color='purple', width=3),\n",
    "                         fill='tozeroy'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Daily Traffic Load - Auto-scaling Simulation',\n",
    "    xaxis_title='Hour of Day',\n",
    "    yaxis_title='Traffic Load (Arbitrary Units)',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# 4. Data Lake Architecture\n",
    "print(\"\\n4. Data Lake Architecture for Lending Club\")\n",
    "print(\"   Raw Zone: Original application files, credit bureau reports\")\n",
    "print(\"   Processed Zone: Cleaned and standardized data\")\n",
    "print(\"   Curated Zone: Aggregated and model-ready datasets\")\n",
    "print(\"   Consumption Zone: Ready for reporting and analytics\")\n",
    "\n",
    "# Visualize the data lake architecture\n",
    "zones = ['Raw Zone', 'Processed Zone', 'Curated Zone', 'Consumption Zone']\n",
    "data_types = [\n",
    "    ['Application Forms', 'Credit Reports', 'Bank Statements'],\n",
    "    ['Standardized Data', 'Data Quality Checks', 'Anomaly Detection'],\n",
    "    ['Feature Sets', 'Model Inputs', 'Aggregations'],\n",
    "    ['Dashboards', 'Reports', 'Model Predictions']\n",
    "]\n",
    "volume = [100, 70, 40, 20]  # Data volume decreases as it gets more refined\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(x=zones, y=volume, \n",
    "           text=volume, textposition='auto',\n",
    "           marker_color=['#FF9999', '#66B2FF', '#99FF99', '#FFD700'])\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Data Lake Architecture - Volume by Zone',\n",
    "    xaxis_title='Data Lake Zone',\n",
    "    yaxis_title='Relative Data Volume',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# 5. Cost Analysis\n",
    "print(\"\\n5. Cost Analysis - Traditional vs Cloud Approach\")\n",
    "\n",
    "workloads = ['Data Storage', 'Compute', 'Analytics', 'ML Training', 'Data Transfer']\n",
    "traditional_cost = [50000, 30000, 20000, 15000, 10000]\n",
    "cloud_cost = [15000, 10000, 8000, 5000, 3000]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Traditional', x=workloads, y=traditional_cost, marker_color='red'),\n",
    "    go.Bar(name='Cloud', x=workloads, y=cloud_cost, marker_color='green')\n",
    "])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Cost Comparison: Traditional vs Cloud Infrastructure',\n",
    "    xaxis_title='Workload',\n",
    "    yaxis_title='Monthly Cost (USD)',\n",
    "    barmode='group',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Calculate cost savings\n",
    "traditional_total = sum(traditional_cost)\n",
    "cloud_total = sum(cloud_cost)\n",
    "savings = traditional_total - cloud_total\n",
    "savings_percentage = (savings / traditional_total) * 100\n",
    "\n",
    "print(f\"   Traditional Approach Cost: ${traditional_total:,}/month\")\n",
    "print(f\"   Cloud Approach Cost: ${cloud_total:,}/month\")\n",
    "print(f\"   Monthly Savings: ${savings:,} ({savings_percentage:.1f}%)\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\nCloud Computing and Big Data Benefits for Lending Club Analysis:\")\n",
    "print(\"- Scalability to handle growing loan volumes\")\n",
    "print(\"- Real-time processing for instant credit decisions\")\n",
    "print(\"- Cost-effectiveness compared to traditional infrastructure\")\n",
    "print(\"- Advanced analytics capabilities for risk assessment\")\n",
    "print(\"- Integration with multiple data sources for comprehensive profiles\")\n",
    "print(\"- Machine learning capabilities for predictive modeling\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this comprehensive notebook on cloud computing and big data, we've explored:\n",
    "\n",
    "1. **Cloud Computing Definition**: Understanding cloud computing as the delivery of computing services over the Internet.\n",
    "\n",
    "2. **Importance of Cloud Data Services**: The benefits of scalability, cost-effectiveness, flexibility, reliability, collaboration, and innovation for data science teams.\n",
    "\n",
    "3. **Big Data Implementation Examples**: How different industries leverage big data for customer analytics, predictive maintenance, and network optimization.\n",
    "\n",
    "4. **Data Warehousing Concepts**: Fact tables, dimension tables, star schema, snowflake schema, and MPP databases for efficient data analysis.\n",
    "\n",
    "5. **Big Data Processing Principles**: Distributed computing, MapReduce, and technologies for handling large volumes of data.\n",
    "\n",
    "6. **Application to Lending Club Dataset**: Demonstrating how these concepts apply to real-world financial data analysis.\n",
    "\n",
    "These technologies and concepts are crucial for modern data science projects, especially when dealing with large-scale datasets like the Lending Club data. Cloud computing and big data technologies enable data scientists to process, analyze, and gain insights from massive datasets that would be impossible to handle with traditional computing approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 }
}