{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Data Cleaning and Preprocessing\n",
    "\n",
    "**Author**: Rafsamjani Anugrah  \n",
    "**Date**: 2024  \n",
    "**Project**: Credit Risk Prediction - ID/X Partners  \n",
    "\n",
    "## Tujuan Notebook\n",
    "\n",
    "Notebook ini berfokus pada membersihkan dan preprocessing data:\n",
    "1. Filter dataset untuk loan yang selesai (Fully Paid/Charged Off)\n",
    "2. Handle missing values dengan strategi yang tepat\n",
    "3. Konversi tipe data (percentages, dates, currencies)\n",
    "4. Create derived features (FICO average, financial ratios)\n",
    "5. Handle outliers\n",
    "6. Encoding categorical variables\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Dataset sudah dieksplorasi di notebook 01\n",
    "- Environment sudah di-setup dengan libraries yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set styling\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"ğŸ”§ Data Cleaning Tools Loaded!\")\n",
    "print(f\"ğŸ“… Cleaning started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_paths = [\n",
    "    '../data/raw/loan_data_2007_2014.csv',\n",
    "    '../../data/raw/loan_data_2007_2014.csv',\n",
    "    'data/raw/loan_data_2007_2014.csv',\n",
    "    'loan_data_2007_2014.csv'\n",
    "]\n",
    "\n",
    "data_path = None\n",
    "for path in data_paths:\n",
    "    if os.path.exists(path):\n",
    "        data_path = path\n",
    "        break\n",
    "\n",
    "if data_path:\n",
    "    print(f\"âœ… Loading dataset from: {data_path}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    df = pd.read_csv(data_path, low_memory=False)\n",
    "    \n",
    "    print(f\"ğŸ“Š Dataset loaded successfully!\")\n",
    "    print(f\"   Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "    \n",
    "    # Create backup of original data\n",
    "    df_original = df.copy()\n",
    "    print(f\"ğŸ’¾ Original data backup created\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Dataset not found!\")\n",
    "    print(\"Please ensure loan_data_2007_2014.csv is in the data/raw/ directory\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter for Completed Loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and 'loan_status' in df.columns:\n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 1: FILTER COMPLETED LOANS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Analisis loan status sebelum filtering\n",
    "    print(\"\\nğŸ“Š Original Loan Status Distribution:\")\n",
    "    status_counts = df['loan_status'].value_counts()\n",
    "    for status, count in status_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        print(f\"   {status:<25}: {count:>7,} ({percentage:>6.2f}%)\")\n",
    "    \n",
    "    # Define completed loans\n",
    "    completed_loans = ['Fully Paid', 'Charged Off']\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Filtering for completed loans: {', '.join(completed_loans)}\")\n",
    "    \n",
    "    # Filter dataset\n",
    "    df_filtered = df[df['loan_status'].isin(completed_loans)].copy()\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Filtering Results:\")\n",
    "    print(f\"   Original dataset: {len(df):,} records\")\n",
    "    print(f\"   Completed loans:   {len(df_filtered):,} records\")\n",
    "    print(f\"   Records removed:   {len(df) - len(df_filtered):,} ({(len(df) - len(df_filtered))/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Analysis of completed loans\n",
    "    completed_status_counts = df_filtered['loan_status'].value_counts()\n",
    "    print(f\"\\nğŸ“Š Completed Loans Distribution:\")\n",
    "    for status, count in completed_status_counts.items():\n",
    "        percentage = count / len(df_filtered) * 100\n",
    "        print(f\"   {status:<25}: {count:>7,} ({percentage:>6.2f}%)\")\n",
    "    \n",
    "    # Calculate default rate\n",
    "    default_rate = (df_filtered['loan_status'] == 'Charged Off').mean() * 100\n",
    "    print(f\"\\nğŸ“ˆ Default Rate in Completed Loans: {default_rate:.2f}%\")\n",
    "    print(f\"ğŸ“ˆ Success Rate in Completed Loans: {100-default_rate:.2f}%\")\n",
    "    \n",
    "    # Create binary target variable\n",
    "    df_filtered['loan_status_binary'] = (df_filtered['loan_status'] == 'Charged Off').astype(int)\n",
    "    \n",
    "    print(f\"\\nâœ… Target variable created:\")\n",
    "    print(f\"   0 = Fully Paid (Good loans): {(df_filtered['loan_status_binary'] == 0).sum():,}\")\n",
    "    print(f\"   1 = Charged Off (Bad loans): {(df_filtered['loan_status_binary'] == 1).sum():,}\")\n",
    "    \n",
    "    # Check class imbalance\n",
    "    imbalance_ratio = (df_filtered['loan_status_binary'] == 0).sum() / (df_filtered['loan_status_binary'] == 1).sum()\n",
    "    print(f\"\\nâš–ï¸  Class Imbalance Ratio: {imbalance_ratio:.1f}:1 (Non-default:Default)\")\n",
    "    \n",
    "    # Update main dataframe\n",
    "    df = df_filtered\n",
    "    print(f\"\\nğŸ”„ Main dataframe updated with filtered data\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Dataset not available or loan_status column missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 2: DATA TYPE CONVERSION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create a copy for cleaning\n",
    "    df_clean = df.copy()\n",
    "    conversions_made = 0\n",
    "    \n",
    "    print(\"\\nğŸ”„ Converting data types...\")\n",
    "    \n",
    "    # 1. Convert percentage columns\n",
    "    percentage_cols = ['int_rate', 'revol_util']\n",
    "    print(f\"\\nğŸ“Š Converting percentage columns:\")\n",
    "    \n",
    "    for col in percentage_cols:\n",
    "        if col in df_clean.columns:\n",
    "            print(f\"   Processing {col}...\")\n",
    "            \n",
    "            # Check original data\n",
    "            sample_original = df_clean[col].dropna().head(3).astype(str).tolist()\n",
    "            print(f\"     Original sample: {sample_original}\")\n",
    "            \n",
    "            # Convert\n",
    "            df_clean[col] = df_clean[col].astype(str).str.replace('%', '')\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "            \n",
    "            # Check converted data\n",
    "            sample_converted = df_clean[col].dropna().head(3).tolist()\n",
    "            print(f\"     Converted sample: {sample_converted}\")\n",
    "            print(f\"     Data type: {df_clean[col].dtype}\")\n",
    "            \n",
    "            conversions_made += 1\n",
    "        else:\n",
    "            print(f\"   âš ï¸  Column {col} not found\")\n",
    "    \n",
    "    # 2. Convert date columns\n",
    "    date_cols = ['issue_d', 'earliest_cr_line', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d']\n",
    "    print(f\"\\nğŸ“… Converting date columns:\")\n",
    "    \n",
    "    for col in date_cols:\n",
    "        if col in df_clean.columns:\n",
    "            print(f\"   Processing {col}...\")\n",
    "            \n",
    "            # Check original data\n",
    "            sample_original = df_clean[col].dropna().head(3).astype(str).tolist()\n",
    "            print(f\"     Original sample: {sample_original}\")\n",
    "            \n",
    "            # Convert using pandas datetime with format specification\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], format='%b-%y', errors='coerce')\n",
    "            \n",
    "            # Check converted data\n",
    "            if df_clean[col].notna().any():\n",
    "                sample_converted = df_clean[col].dropna().head(3).dt.strftime('%Y-%m-%d').tolist()\n",
    "                print(f\"     Converted sample: {sample_converted}\")\n",
    "                print(f\"     Data type: {df_clean[col].dtype}\")\n",
    "                print(f\"     Date range: {df_clean[col].min()} to {df_clean[col].max()}\")\n",
    "                conversions_made += 1\n",
    "            else:\n",
    "                print(f\"     âš ï¸  No valid dates found in {col}\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  Column {col} not found\")\n",
    "    \n",
    "    # 3. Convert employment length to numeric\n",
    "    if 'emp_length' in df_clean.columns:\n",
    "        print(f\"\\nğŸ‘¤ Converting employment length:\")\n",
    "        \n",
    "        # Check original data\n",
    "        print(f\"   Original unique values: {df_clean['emp_length'].value_counts().index.tolist()}\")\n",
    "        \n",
    "        # Create mapping\n",
    "        emp_length_mapping = {\n",
    "            '< 1 year': 0.5,\n",
    "            '1 year': 1,\n",
    "            '2 years': 2,\n",
    "            '3 years': 3,\n",
    "            '4 years': 4,\n",
    "            '5 years': 5,\n",
    "            '6 years': 6,\n",
    "            '7 years': 7,\n",
    "            '8 years': 8,\n",
    "            '9 years': 9,\n",
    "            '10+ years': 10,\n",
    "            'n/a': np.nan\n",
    "        }\n",
    "        \n",
    "        # Convert\n",
    "        df_clean['emp_length_numeric'] = df_clean['emp_length'].map(emp_length_mapping)\n",
    "        \n",
    "        print(f\"   Converted unique values: {df_clean['emp_length_numeric'].value_counts().index.tolist()}\")\n",
    "        print(f\"   Data type: {df_clean['emp_length_numeric'].dtype}\")\n",
    "        conversions_made += 1\n",
    "    \n",
    "    # 4. Convert loan term to numeric\n",
    "    if 'term' in df_clean.columns:\n",
    "        print(f\"\\nğŸ’° Converting loan term:\")\n",
    "        \n",
    "        # Check original data\n",
    "        print(f\"   Original unique values: {df_clean['term'].value_counts().index.tolist()}\")\n",
    "        \n",
    "        # Extract numeric value\n",
    "        df_clean['term_months'] = df_clean['term'].str.extract('(\\d+)').astype(float)\n",
    "        \n",
    "        print(f\"   Converted unique values: {df_clean['term_months'].value_counts().index.tolist()}\")\n",
    "        print(f\"   Data type: {df_clean['term_months'].dtype}\")\n",
    "        conversions_made += 1\n",
    "    \n",
    "    # 5. Handle zip code (convert to first 3 digits for region analysis)\n",
    "    if 'zip_code' in df_clean.columns:\n",
    "        print(f\"\\nğŸ“ Processing zip code:\")\n",
    "        \n",
    "        # Extract first 3 digits\n",
    "        df_clean['zip_code_3'] = df_clean['zip_code'].str[:3]\n",
    "        \n",
    "        print(f\"   Converted zip codes to first 3 digits for region analysis\")\n",
    "        print(f\"   Unique regions: {df_clean['zip_code_3'].nunique()}\")\n",
    "        conversions_made += 1\n",
    "    \n",
    "    print(f\"\\nâœ… Data type conversion completed!\")\n",
    "    print(f\"   Total conversions made: {conversions_made}\")\n",
    "    print(f\"   Dataset shape after conversion: {df_clean.shape}\")\n",
    "    \n",
    "    # Update main dataframe\n",
    "    df = df_clean\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 3: HANDLE MISSING VALUES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Analyze missing values\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_percentages = (missing_counts / len(df) * 100)\n",
    "    \n",
    "    # Create missing values DataFrame\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Missing Count': missing_counts.values,\n",
    "        'Missing %': missing_percentages.values,\n",
    "        'Data Type': df.dtypes.values\n",
    "    })\n",
    "    \n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Missing Values Analysis:\")\n",
    "    print(f\"   Columns with missing values: {len(missing_df)}\")\n",
    "    print(f\"   Total missing values: {missing_counts.sum():,}\")\n",
    "    \n",
    "    # Strategy 1: Drop columns with >50% missing\n",
    "    print(f\"\\nğŸ—‘ï¸  Strategy 1: Drop columns with >50% missing values\")\n",
    "    high_missing_cols = missing_df[missing_df['Missing %'] > 50]['Column'].tolist()\n",
    "    \n",
    "    if high_missing_cols:\n",
    "        print(f\"   Dropping {len(high_missing_cols)} columns:\")\n",
    "        for col in high_missing_cols:\n",
    "            missing_pct = missing_df[missing_df['Column'] == col]['Missing %'].iloc[0]\n",
    "            print(f\"     - {col}: {missing_pct:.1f}% missing\")\n",
    "        \n",
    "        df.drop(columns=high_missing_cols, inplace=True)\n",
    "        print(f\"   âœ… Dropped {len(high_missing_cols)} columns\")\n",
    "    else:\n",
    "        print(\"   âœ… No columns with >50% missing values\")\n",
    "    \n",
    "    # Strategy 2: Handle important columns with missing values\n",
    "    print(f\"\\nğŸ”§ Strategy 2: Handle important columns with missing values\")\n",
    "    \n",
    "    # Update missing_df after dropping columns\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_percentages = (missing_counts / len(df) * 100)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Missing Count': missing_counts.values,\n",
    "        'Missing %': missing_percentages.values,\n",
    "        'Data Type': df.dtypes.values\n",
    "    })\n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    # Define important columns and their handling strategy\n",
    "    important_cols_strategies = {\n",
    "        'emp_title': {'strategy': 'fill_unknown', 'fill_value': 'Unknown'},\n",
    "        'emp_length_numeric': {'strategy': 'fill_median'},\n",
    "        'annual_inc': {'strategy': 'fill_median'},\n",
    "        'dti': {'strategy': 'fill_median'},\n",
    "        'delinq_2yrs': {'strategy': 'fill_zero'},\n",
    "        'inq_last_6mths': {'strategy': 'fill_zero'},\n",
    "        'open_acc': {'strategy': 'fill_median'},\n",
    "        'pub_rec': {'strategy': 'fill_zero'},\n",
    "        'revol_bal': {'strategy': 'fill_median'},\n",
    "        'revol_util': {'strategy': 'fill_median'},\n",
    "        'total_acc': {'strategy': 'fill_median'},\n",
    "        'collections_12_mths_ex_med': {'strategy': 'fill_zero'},\n",
    "        'acc_now_delinq': {'strategy': 'fill_zero'},\n",
    "        'tot_coll_amt': {'strategy': 'fill_zero'},\n",
    "        'tot_cur_bal': {'strategy': 'fill_median'},\n",
    "        'total_rev_hi_lim': {'strategy': 'fill_median'},\n",
    "    }\n",
    "    \n",
    "    # Apply strategies\n",
    "    handled_cols = 0\n",
    "    \n",
    "    for col, strategy_info in important_cols_strategies.items():\n",
    "        if col in df.columns and df[col].isnull().sum() > 0:\n",
    "            strategy = strategy_info['strategy']\n",
    "            missing_count = df[col].isnull().sum()\n",
    "            missing_pct = missing_count / len(df) * 100\n",
    "            \n",
    "            print(f\"   ğŸ“ Processing {col}: {missing_count:,} ({missing_pct:.1f}%) missing\")\n",
    "            \n",
    "            if strategy == 'fill_unknown':\n",
    "                df[col] = df[col].fillna(strategy_info['fill_value'])\n",
    "                print(f\"     Filled with '{strategy_info['fill_value']}'\")\n",
    "            \n",
    "            elif strategy == 'fill_median':\n",
    "                median_val = df[col].median()\n",
    "                df[col] = df[col].fillna(median_val)\n",
    "                print(f\"     Filled with median: {median_val}\")\n",
    "            \n",
    "            elif strategy == 'fill_zero':\n",
    "                df[col] = df[col].fillna(0)\n",
    "                print(f\"     Filled with 0\")\n",
    "            \n",
    "            elif strategy == 'fill_mode':\n",
    "                mode_val = df[col].mode().iloc[0] if not df[col].mode().empty else 'Unknown'\n",
    "                df[col] = df[col].fillna(mode_val)\n",
    "                print(f\"     Filled with mode: {mode_val}\")\n",
    "            \n",
    "            handled_cols += 1\n",
    "    \n",
    "    # Strategy 3: Handle remaining categorical columns\n",
    "    print(f\"\\nğŸ“ Strategy 3: Handle remaining categorical columns\")\n",
    "    \n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    remaining_categorical_missing = [col for col in categorical_cols if df[col].isnull().sum() > 0]\n",
    "    \n",
    "    for col in remaining_categorical_missing:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        missing_pct = missing_count / len(df) * 100\n",
    "        print(f\"   ğŸ“ Processing {col}: {missing_count:,} ({missing_pct:.1f}%) missing\")\n",
    "        \n",
    "        # Fill with mode or 'Unknown'\n",
    "        if df[col].nunique() < 20:  # Low cardinality\n",
    "            fill_value = df[col].mode().iloc[0] if not df[col].mode().empty else 'Unknown'\n",
    "        else:  # High cardinality\n",
    "            fill_value = 'Unknown'\n",
    "        \n",
    "        df[col] = df[col].fillna(fill_value)\n",
    "        print(f\"     Filled with: {fill_value}\")\n",
    "        handled_cols += 1\n",
    "    \n",
    "    # Strategy 4: Handle remaining numerical columns\n",
    "    print(f\"\\nğŸ“Š Strategy 4: Handle remaining numerical columns\")\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    remaining_numerical_missing = [col for col in numerical_cols if df[col].isnull().sum() > 0]\n",
    "    \n",
    "    for col in remaining_numerical_missing:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        missing_pct = missing_count / len(df) * 100\n",
    "        print(f\"   ğŸ“Š Processing {col}: {missing_count:,} ({missing_pct:.1f}%) missing\")\n",
    "        \n",
    "        # Fill with median for numerical columns\n",
    "        fill_value = df[col].median()\n",
    "        df[col] = df[col].fillna(fill_value)\n",
    "        print(f\"     Filled with median: {fill_value}\")\n",
    "        handled_cols += 1\n",
    "    \n",
    "    # Final check\n",
    "    final_missing = df.isnull().sum().sum()\n",
    "    print(f\"\\nâœ… Missing values handling completed!\")\n",
    "    print(f\"   Columns handled: {handled_cols}\")\n",
    "    print(f\"   Remaining missing values: {final_missing:,}\")\n",
    "    print(f\"   Dataset shape: {df.shape}\")\n",
    "    \n",
    "    if final_missing == 0:\n",
    "        print(f\"   ğŸ‰ No missing values remaining!\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Still has {final_missing} missing values\")\n",
    "        remaining_missing_cols = df.columns[df.isnull().any()].tolist()\n",
    "        print(f\"   Remaining columns with missing: {remaining_missing_cols}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 4: FEATURE ENGINEERING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    engineered_features = 0\n",
    "    \n",
    "    print(\"\\nğŸ”§ Creating engineered features...\")\n",
    "    \n",
    "    # 1. FICO Score Features\n",
    "    fico_cols = [col for col in df.columns if 'fico' in col.lower()]\n",
    "    if 'fico_range_low' in df.columns and 'fico_range_high' in df.columns:\n",
    "        print(f\"\\nğŸ“ˆ Creating FICO score features:\")\n",
    "        \n",
    "        # FICO Average\n",
    "        df['fico_avg'] = (df['fico_range_low'] + df['fico_range_high']) / 2\n",
    "        print(f\"   âœ… fico_avg: Average FICO score\")\n",
    "        print(f\"      Range: {df['fico_avg'].min():.0f} - {df['fico_avg'].max():.0f}\")\n",
    "        engineered_features += 1\n",
    "        \n",
    "        # FICO Score Categories\n",
    "        df['fico_category'] = pd.cut(\n",
    "            df['fico_avg'],\n",
    "            bins=[0, 680, 720, 760, 850],\n",
    "            labels=['Fair (680-)', 'Good (680-720)', 'Very Good (720-760)', 'Excellent (760+)']\n",
    "        )\n",
    "        print(f\"   âœ… fico_category: FICO score categories\")\n",
    "        print(f\"      Categories: {df['fico_category'].value_counts().index.tolist()}\")\n",
    "        engineered_features += 1\n",
    "        \n",
    "        # Low FICO Flag\n",
    "        df['low_fico_flag'] = (df['fico_avg'] < 680).astype(int)\n",
    "        print(f\"   âœ… low_fico_flag: FICO < 680\")\n",
    "        print(f\"      Low FICO loans: {df['low_fico_flag'].sum():,} ({df['low_fico_flag'].mean()*100:.1f}%)\")\n",
    "        engineered_features += 1\n",
    "    \n",
    "    # 2. Financial Ratio Features\n",
    "    if all(col in df.columns for col in ['loan_amnt', 'annual_inc']):\n",
    "        print(f\"\\nğŸ’° Creating financial ratio features:\")\n",
    "        \n",
    "        # Loan to Income Ratio\n",
    "        df['loan_to_income_ratio'] = df['loan_amnt'] / (df['annual_inc'] / 12)\n",
    "        print(f\"   âœ… loan_to_income_ratio: Loan amount / monthly income\")\n",
    "        print(f\"      Range: {df['loan_to_income_ratio'].min():.1f} - {df['loan_to_income_ratio'].max():.1f}\")\n",
    "        engineered_features += 1\n",
    "        \n",
    "        # Monthly Debt Burden\n",
    "        if 'installment' in df.columns:\n",
    "            df['monthly_debt_burden'] = df['installment'] / (df['annual_inc'] / 12)\n",
    "            print(f\"   âœ… monthly_debt_burden: Monthly payment / monthly income\")\n",
    "            print(f\"      Range: {df['monthly_debt_burden'].min():.3f} - {df['monthly_debt_burden'].max():.3f}\")\n",
    "            engineered_features += 1\n",
    "    \n",
    "    # 3. Enhanced DTI Features\n",
    "    if all(col in df.columns for col in ['dti', 'installment', 'annual_inc']):\n",
    "        print(f\"\\nğŸ“Š Creating enhanced DTI features:\")\n",
    "        \n",
    "        # Effective DTI (including new loan)\n",
    "        monthly_income = df['annual_inc'] / 12\n",
    "        new_payment_dti = (df['installment'] / monthly_income) * 100\n",
    "        df['effective_dti'] = df['dti'] + new_payment_dti\n",
    "        print(f\"   âœ… effective_dti: Current DTI + new loan payment ratio\")\n",
    "        print(f\"      Range: {df['effective_dti'].min():.1f} - {df['effective_dti'].max():.1f}\")\n",
    "        engineered_features += 1\n",
    "        \n",
    "        # High DTI Flags\n",
    "        df['high_dti_flag'] = (df['dti'] > 30).astype(int)\n",
    "        df['very_high_dti_flag'] = (df['dti'] > 40).astype(int)\n",
    "        print(f\"   âœ… high_dti_flag: DTI > 30% ({df['high_dti_flag'].sum():,} loans)\")\n",
    "        print(f\"   âœ… very_high_dti_flag: DTI > 40% ({df['very_high_dti_flag'].sum():,} loans)\")\n",
    "        engineered_features += 2\n",
    "    \n",
    "    # 4. Credit History Features\n",
    "    if 'earliest_cr_line' in df.columns and 'issue_d' in df.columns:\n",
    "        print(f\"\\nğŸ“… Creating credit history features:\")\n",
    "        \n",
    "        # Credit Age in Years\n",
    "        df['credit_age_years'] = (df['issue_d'] - df['earliest_cr_line']).dt.days / 365.25\n",
    "        df['credit_age_years'] = df['credit_age_years'].fillna(0)  # Handle missing dates\n",
    "        print(f\"   âœ… credit_age_years: Length of credit history in years\")\n",
    "        print(f\"      Range: {df['credit_age_years'].min():.1f} - {df['credit_age_years'].max():.1f}\")\n",
    "        engineered_features += 1\n",
    "        \n",
    "        # Credit Age Categories\n",
    "        df['credit_age_category'] = pd.cut(\n",
    "            df['credit_age_years'],\n",
    "            bins=[0, 5, 10, 20, 100],\n",
    "            labels=['<5 years', '5-10 years', '10-20 years', '20+ years']\n",
    "        )\n",
    "        print(f\"   âœ… credit_age_category: Credit history length categories\")\n",
    "        engineered_features += 1\n",
    "    \n",
    "    # 5. Employment Stability Features\n",
    "    if 'emp_length_numeric' in df.columns:\n",
    "        print(f\"\\nğŸ‘¤ Creating employment stability features:\")\n",
    "        \n",
    "        # Employment Stability Score\n",
    "        if 'home_ownership' in df.columns:\n",
    "            df['employment_stability_score'] = (\n",
    "                df['emp_length_numeric'] * 7 +  # Employment length weight\n",
    "                (df['home_ownership'].isin(['MORTGAGE', 'OWN']) * 20) +  # Home ownership bonus\n",
    "                (df.get('verification_status', '').isin(['Source Verified', 'Verified']) * 13)  # Verification bonus\n",
    "            )\n",
    "            print(f\"   âœ… employment_stability_score: Combined employment stability metric\")\n",
    "            print(f\"      Range: {df['employment_stability_score'].min():.0f} - {df['employment_stability_score'].max():.0f}\")\n",
    "            engineered_features += 1\n",
    "        \n",
    "        # Stable Employment Flag\n",
    "        df['stable_employment_flag'] = (df['emp_length_numeric'] >= 5).astype(int)\n",
    "        print(f\"   âœ… stable_employment_flag: 5+ years employment ({df['stable_employment_flag'].sum():,} loans)\")\n",
    "        engineered_features += 1\n",
    "    \n",
    "    # 6. Credit Utilization Features\n",
    "    if 'revol_util' in df.columns:\n",
    "        print(f\"\\nğŸ’³ Creating credit utilization features:\")\n",
    "        \n",
    "        # High Utilization Flags\n",
    "        df['high_utilization_flag'] = (df['revol_util'] > 80).astype(int)\n",
    "        df['very_high_utilization_flag'] = (df['revol_util'] > 90).astype(int)\n",
    "        print(f\"   âœ… high_utilization_flag: Revolving utilization > 80% ({df['high_utilization_flag'].sum():,} loans)\")\n",
    "        print(f\"   âœ… very_high_utilization_flag: Revolving utilization > 90% ({df['very_high_utilization_flag'].sum():,} loans)\")\n",
    "        engineered_features += 2\n",
    "        \n",
    "        # Utilization Categories\n",
    "        df['utilization_category'] = pd.cut(\n",
    "            df['revol_util'],\n",
    "            bins=[0, 30, 60, 80, 100],\n",
    "            labels=['Low (0-30%)', 'Medium (30-60%)', 'High (60-80%)', 'Very High (80-100%)']\n",
    "        )\n",
    "        print(f\"   âœ… utilization_category: Credit utilization categories\")\n",
    "        engineered_features += 1\n",
    "    \n",
    "    # 7. Delinquency Risk Features\n",
    "    delinquency_cols = [col for col in df.columns if 'delinq' in col.lower() or 'pub_rec' in col.lower()]\n",
    "    if 'delinq_2yrs' in df.columns:\n",
    "        print(f\"\\nâš ï¸  Creating delinquency risk features:\")\n",
    "        \n",
    "        # Recent Delinquency Flag\n",
    "        df['recent_delinquency_flag'] = (df['delinq_2yrs'] > 0).astype(int)\n",
    "        print(f\"   âœ… recent_delinquency_flag: Any delinquency in last 2 years ({df['recent_delinquency_flag'].sum():,} loans)\")\n",
    "        engineered_features += 1\n",
    "        \n",
    "        # Multiple Delinquencies Flag\n",
    "        df['multiple_delinquencies_flag'] = (df['delinq_2yrs'] > 1).astype(int)\n",
    "        print(f\"   âœ… multiple_delinquencies_flag: 2+ delinquencies in last 2 years ({df['multiple_delinquencies_flag'].sum():,} loans)\")\n",
    "        engineered_features += 1\n",
    "    \n",
    "    # 8. Inquiry Risk Features\n",
    "    if 'inq_last_6mths' in df.columns:\n",
    "        print(f\"\\nğŸ” Creating inquiry risk features:\")\n",
    "        \n",
    "        # Recent Inquiries Flag\n",
    "        df['recent_inquiries_flag'] = (df['inq_last_6mths'] > 0).astype(int)\n",
    "        print(f\"   âœ… recent_inquiries_flag: Any inquiries in last 6 months ({df['recent_inquiries_flag'].sum():,} loans)\")\n",
    "        engineered_features += 1\n",
    "        \n",
    "        # High Inquiries Flag\n",
    "        df['high_inquiries_flag'] = (df['inq_last_6mths'] > 3).astype(int)\n",
    "        print(f\"   âœ… high_inquiries_flag: 4+ inquiries in last 6 months ({df['high_inquiries_flag'].sum():,} loans)\")\n",
    "        engineered_features += 1\n",
    "    \n",
    "    print(f\"\\nâœ… Feature engineering completed!\")\n",
    "    print(f\"   Total engineered features created: {engineered_features}\")\n",
    "    print(f\"   New dataset shape: {df.shape}\")\n",
    "    print(f\"   Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Show new features summary\n",
    "    if engineered_features > 0:\n",
    "        print(f\"\\nğŸ“‹ New Features Summary:\")\n",
    "        # Get all newly created columns (not in original dataset)\n",
    "        original_cols = set(df_original.columns)\n",
    "        current_cols = set(df.columns)\n",
    "        new_cols = current_cols - original_cols\n",
    "        \n",
    "        print(f\"   Total new features: {len(new_cols)}\")\n",
    "        for i, col in enumerate(sorted(list(new_cols)), 1):\n",
    "            if df[col].dtype in ['object', 'category']:\n",
    "                unique_vals = df[col].nunique()\n",
    "                print(f\"   {i:2d}. {col:<30} (categorical, {unique_vals} unique values)\")\n",
    "            else:\n",
    "                min_val = df[col].min()\n",
    "                max_val = df[col].max()\n",
    "                print(f\"   {i:2d}. {col:<30} (numeric, {min_val:.2f} to {max_val:.2f})\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"STEP 5: HANDLE OUTLIERS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Define outlier handling strategies\n",
    "    outlier_strategies = {\n",
    "        'annual_inc': {'method': 'cap', 'upper_percentile': 99.5, 'min_value': 20000, 'max_value': 500000},\n",
    "        'loan_amnt': {'method': 'cap', 'min_value': 1000, 'max_value': 35000},\n",
    "        'dti': {'method': 'cap', 'min_value': 0, 'max_value': 50},\n",
    "        'revol_util': {'method': 'cap', 'min_value': 0, 'max_value': 120},  # Allow some >100% for overlimit cases\n",
    "        'int_rate': {'method': 'cap', 'min_value': 0, 'max_value': 30},\n",
    "        'loan_to_income_ratio': {'method': 'cap', 'max_value': 10},  # Max 10x monthly income\n",
    "        'monthly_debt_burden': {'method': 'cap', 'max_value': 0.5},  # Max 50% of monthly income\n",
    "        'effective_dti': {'method': 'cap', 'max_value': 60},  # Max 60% including new loan\n",
    "    }\n",
    "    \n",
    "    outliers_handled = 0\n",
    "    \n",
    "    print(\"\\nğŸ” Analyzing and handling outliers...\")\n",
    "    \n",
    "    for col, strategy in outlier_strategies.items():\n",
    "        if col in df.columns:\n",
    "            method = strategy['method']\n",
    "            \n",
    "            print(f\"\\nğŸ“Š Processing {col}:\")\n",
    "            \n",
    "            # Show original statistics\n",
    "            original_min = df[col].min()\n",
    "            original_max = df[col].max()\n",
    "            original_mean = df[col].mean()\n",
    "            original_median = df[col].median()\n",
    "            \n",
    "            print(f\"   Original range: {original_min:,.2f} to {original_max:,.2f}\")\n",
    "            print(f\"   Original mean: {original_mean:,.2f}, median: {original_median:,.2f}\")\n",
    "            \n",
    "            if method == 'cap':\n",
    "                # Calculate bounds\n",
    "                if 'upper_percentile' in strategy:\n",
    "                    upper_bound = df[col].quantile(strategy['upper_percentile'] / 100)\n",
    "                elif 'max_value' in strategy:\n",
    "                    upper_bound = strategy['max_value']\n",
    "                else:\n",
    "                    upper_bound = df[col].quantile(0.99)\n",
    "                \n",
    "                if 'lower_percentile' in strategy:\n",
    "                    lower_bound = df[col].quantile(strategy['lower_percentile'] / 100)\n",
    "                elif 'min_value' in strategy:\n",
    "                    lower_bound = strategy['min_value']\n",
    "                else:\n",
    "                    lower_bound = df[col].quantile(0.01)\n",
    "                \n",
    "                # Count outliers\n",
    "                upper_outliers = (df[col] > upper_bound).sum()\n",
    "                lower_outliers = (df[col] < lower_bound).sum()\n",
    "                total_outliers = upper_outliers + lower_outliers\n",
    "                \n",
    "                print(f\"   Outlier bounds: {lower_bound:,.2f} to {upper_bound:,.2f}\")\n",
    "                print(f\"   Outliers detected: {total_outliers:,} ({total_outliers/len(df)*100:.2f}%)\")\n",
    "                print(f\"     Lower outliers: {lower_outliers:,}\")\n",
    "                print(f\"     Upper outliers: {upper_outliers:,}\")\n",
    "                \n",
    "                # Apply capping\n",
    "                df[col] = df[col].clip(lower_bound, upper_bound)\n",
    "                \n",
    "                # Show new statistics\n",
    "                new_min = df[col].min()\n",
    "                new_max = df[col].max()\n",
    "                new_mean = df[col].mean()\n",
    "                new_median = df[col].median()\n",
    "                \n",
    "                print(f\"   New range: {new_min:,.2f} to {new_max:,.2f}\")\n",
    "                print(f\"   New mean: {new_mean:,.2f}, median: {new_median:,.2f}\")\n",
    "                print(f\"   âœ… Outliers handled with capping method\")\n",
    "                \n",
    "                outliers_handled += total_outliers\n",
    "    \n",
    "    # Additional outlier detection using IQR method for important features\n",
    "    print(f\"\\nğŸ“ˆ Additional outlier check using IQR method:\")\n",
    "    \n",
    "    important_features = ['annual_inc', 'loan_amnt', 'dti', 'int_rate', 'fico_avg']\n",
    "    iqr_outliers = 0\n",
    "    \n",
    "    for col in important_features:\n",
    "        if col in df.columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 3 * IQR  # More conservative (3*IQR instead of 1.5)\n",
    "            upper_bound = Q3 + 3 * IQR\n",
    "            \n",
    "            outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()\n",
    "            outlier_pct = outliers / len(df) * 100\n",
    "            \n",
    "            if outlier_pct > 0.1:  # Only show if significant\n",
    "                print(f\"   {col}: {outliers:,} outliers ({outlier_pct:.2f}%)\")\n",
    "                iqr_outliers += outliers\n",
    "    \n",
    "    if iqr_outliers == 0:\n",
    "        print(\"   No significant outliers detected with IQR method\")\n",
    "    \n",
    "    print(f\"\\nâœ… Outlier handling completed!\")\n",
    "    print(f\"   Total outliers processed: {outliers_handled:,}\")\n",
    "    print(f\"   Dataset shape: {df.shape}\")\n",
    "    \n",
    "    # Visualize before/after for key variables\n",
    "    key_vars = ['annual_inc', 'loan_amnt', 'dti']\n",
    "    key_vars = [var for var in key_vars if var in df.columns]\n",
    "    \n",
    "    if key_vars:\n",
    "        fig, axes = plt.subplots(len(key_vars), 2, figsize=(15, 5*len(key_vars)))\n",
    "        if len(key_vars) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, var in enumerate(key_vars):\n",
    "            # Compare with original if available\n",
    "            if var in df_original.columns:\n",
    "                # Original data\n",
    "                axes[i][0].hist(df_original[var].dropna(), bins=50, alpha=0.7, color='red', label='Original')\n",
    "                axes[i][0].set_title(f'{var} - Original Distribution')\n",
    "                axes[i][0].set_xlabel(var)\n",
    "                axes[i][0].set_ylabel('Frequency')\n",
    "                axes[i][0].legend()\n",
    "                \n",
    "                # Cleaned data\n",
    "                axes[i][1].hist(df[var].dropna(), bins=50, alpha=0.7, color='green', label='Cleaned')\n",
    "                axes[i][1].set_title(f'{var} - Cleaned Distribution')\n",
    "                axes[i][1].set_xlabel(var)\n",
    "                axes[i][1].set_ylabel('Frequency')\n",
    "                axes[i][1].legend()\n",
    "            else:\n",
    "                # Just show cleaned data\n",
    "                axes[i][0].hist(df[var].dropna(), bins=50, alpha=0.7, color='blue')\n",
    "                axes[i][0].set_title(f'{var} - Distribution')\n",
    "                axes[i][0].set_xlabel(var)\n",
    "                axes[i][0].set_ylabel('Frequency')\n",
    "                \n",
    "                # Box plot\n",
    "                axes[i][1].boxplot(df[var].dropna())\n",
    "                axes[i][1].set_title(f'{var} - Box Plot')\n",
    "                axes[i][1].set_ylabel(var)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Quality Check and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"DATA CLEANING - FINAL SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nğŸ“Š FINAL DATASET SUMMARY:\")\n",
    "    print(f\"   Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "    print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Target variable check\n",
    "    if 'loan_status_binary' in df.columns:\n",
    "        target_counts = df['loan_status_binary'].value_counts()\n",
    "        default_rate = target_counts[1] / len(df) * 100\n",
    "        print(f\"   Target variable (loan_status_binary):\")\n",
    "        print(f\"     Fully Paid (0): {target_counts[0]:,} ({target_counts[0]/len(df)*100:.1f}%)\")\n",
    "        print(f\"     Charged Off (1): {target_counts[1]:,} ({default_rate:.1f}%)\")\n",
    "        print(f\"     Default rate: {default_rate:.2f}%\")\n",
    "    \n",
    "    # Missing values check\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    cols_with_missing = df.isnull().any().sum()\n",
    "    print(f\"   Missing values: {total_missing:,} total in {cols_with_missing} columns\")\n",
    "    \n",
    "    # Data types summary\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    print(f\"   Data types:\")\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"     {dtype}: {count} columns\")\n",
    "    \n",
    "    # Key engineered features\n",
    "    if 'fico_avg' in df.columns:\n",
    "        print(f\"   FICO scores: {df['fico_avg'].min():.0f} - {df['fico_avg'].max():.0f}\")\n",
    "    if 'loan_to_income_ratio' in df.columns:\n",
    "        print(f\"   Loan-to-income ratios: {df['loan_to_income_ratio'].min():.1f} - {df['loan_to_income_ratio'].max():.1f}\")\n",
    "    if 'effective_dti' in df.columns:\n",
    "        print(f\"   Effective DTI: {df['effective_dti'].min():.1f}% - {df['effective_dti'].max():.1f}%\")\n",
    "    \n",
    "    # Quality metrics\n",
    "    print(f\"\\nâœ… DATA QUALITY METRICS:\")\n",
    "    print(f\"   Completeness: {((1 - total_missing/(df.shape[0]*df.shape[1]))*100):.1f}%\")\n",
    "    print(f\"   Duplicate rows: {df.duplicated().sum():,}\")\n",
    "    \n",
    "    # Check for any remaining data issues\n",
    "    issues_found = []\n",
    "    \n",
    "    # Check for infinite values\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    infinite_count = np.isinf(df[numeric_cols]).sum().sum()\n",
    "    if infinite_count > 0:\n",
    "        issues_found.append(f\"Infinite values: {infinite_count:,}\")\n",
    "    \n",
    "    # Check for negative values where they shouldn't exist\n",
    "    non_negative_cols = ['loan_amnt', 'annual_inc', 'int_rate', 'dti', 'revol_bal', 'fico_avg']\n",
    "    for col in non_negative_cols:\n",
    "        if col in df.columns:\n",
    "            negative_count = (df[col] < 0).sum()\n",
    "            if negative_count > 0:\n",
    "                issues_found.append(f\"Negative {col}: {negative_count:,}\")\n",
    "    \n",
    "    if issues_found:\n",
    "        print(f\"   âš ï¸  Issues found: {len(issues_found)}\")\n",
    "        for issue in issues_found:\n",
    "            print(f\"     - {issue}\")\n",
    "    else:\n",
    "        print(f\"   ğŸ‰ No data quality issues detected!\")\n",
    "    \n",
    "    # Save cleaned dataset\n",
    "    print(f\"\\nğŸ’¾ Saving cleaned dataset...\")\n",
    "    \n",
    "    try:\n",
    "        # Create data/processed directory if it doesn't exist\n",
    "        os.makedirs('../data/processed', exist_ok=True)\n",
    "        \n",
    "        # Save cleaned data\n",
    "        cleaned_file = '../data/processed/loan_data_cleaned.csv'\n",
    "        df.to_csv(cleaned_file, index=False)\n",
    "        print(f\"   âœ… Cleaned dataset saved to: {cleaned_file}\")\n",
    "        print(f\"   File size: {os.path.getsize(cleaned_file) / 1024**2:.1f} MB\")\n",
    "        \n",
    "        # Save cleaning log\n",
    "        cleaning_log = {\n",
    "            'cleaning_date': datetime.now().isoformat(),\n",
    "            'original_shape': df_original.shape,\n",
    "            'cleaned_shape': df.shape,\n",
    "            'records_removed': df_original.shape[0] - df.shape[0],\n",
    "            'columns_added': df.shape[1] - df_original.shape[1],\n",
    "            'target_variable': 'loan_status_binary',\n",
    "            'default_rate': default_rate if 'loan_status_binary' in df.columns else None,\n",
    "            'missing_values_handled': total_missing,\n",
    "            'outliers_handled': outliers_handled if 'outliers_handled' in locals() else 0,\n",
    "            'engineered_features': engineered_features if 'engineered_features' in locals() else 0\n",
    "        }\n",
    "        \n",
    "        with open('../data/processed/cleaning_log.json', 'w') as f:\n",
    "            json.dump(cleaning_log, f, indent=2, default=str)\n",
    "        print(f\"   âœ… Cleaning log saved to: ../data/processed/cleaning_log.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error saving cleaned dataset: {e}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ DATA CLEANING COMPLETED SUCCESSFULLY!\")\n",
    "    print(f\"=\"*50)\n",
    "    print(f\"âœ… Dataset is now ready for feature engineering and modeling\")\n",
    "    print(f\"ğŸ“ Next step: Proceed to '03_feature_engineering.ipynb'\")\n",
    "    print(f\"ğŸ“Š Or use the cleaned dataset directly for modeling\")\n",
    "    \n",
    "    # Show sample of cleaned data\n",
    "    print(f\"\\nğŸ“‹ Sample of Cleaned Data (First 3 rows, Selected Columns):\")\n",
    "    display_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_avg', \n",
    "                   'loan_status_binary', 'loan_to_income_ratio', 'effective_dti']\n",
    "    display_cols = [col for col in display_cols if col in df.columns]\n",
    "    \n",
    "    if display_cols:\n",
    "        display(df[display_cols].head(3).round(2))\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Dataset not available for final summary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (credit-risk-env)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}