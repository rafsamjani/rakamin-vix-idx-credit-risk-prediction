{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model Credit Risk Training\n",
    "\n",
    "This notebook creates 3 specialized models for different risk categories:\n",
    "1. **Low Risk Model** - For applicants with DTI < 20% and FICO > 720\n",
    "2. **Medium Risk Model** - For applicants with DTI 20-35% and FICO 680-720\n",
    "3. **High Risk Model** - For applicants with DTI > 35% or FICO < 680"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned data\n",
    "print('Loading cleaned dataset...')\n",
    "df = pd.read_csv('../data/processed/loan_data_cleaned.csv')\n",
    "print(f'Dataset shape: {df.shape}')\n",
    "print(f'Loan status distribution:')\n",
    "print(df['loan_status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline\n",
    "\n",
    "### Step 1: Feature Engineering\n",
    "We create meaningful features for risk assessment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"Create engineered features for risk assessment\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # 1. Credit grade encoding (A=1, B=2, ..., G=7)\n",
    "    grade_mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n",
    "    df_features['grade_encoded'] = df_features['grade'].map(grade_mapping)\n",
    "    \n",
    "    # 2. Employment length in years\n",
    "    emp_mapping = {\n",
    "        '< 1 year': 0.5, '1 year': 1, '2 years': 2, '3 years': 3,\n",
    "        '4 years': 4, '5 years': 5, '6 years': 6, '7 years': 7,\n",
    "        '8 years': 8, '9 years': 9, '10+ years': 10\n",
    "    }\n",
    "    df_features['emp_length_numeric'] = df_features['emp_length'].map(emp_mapping)\n",
    "    \n",
    "    # 3. Home ownership encoding\n",
    "    home_mapping = {'RENT': 1, 'MORTGAGE': 2, 'OWN': 3, 'OTHER': 0, 'NONE': 0, 'ANY': 0}\n",
    "    df_features['home_ownership_encoded'] = df_features['home_ownership'].map(home_mapping)\n",
    "    \n",
    "    # 4. Verification status encoding\n",
    "    verification_mapping = {'Not Verified': 0, 'Source Verified': 1, 'Verified': 2}\n",
    "    df_features['verification_status_encoded'] = df_features['verification_status'].map(verification_mapping)\n",
    "    \n",
    "    # 5. Loan purpose encoding (simplified)\n",
    "    purpose_mapping = {\n",
    "        'debt_consolidation': 1, 'credit_card': 2, 'home_improvement': 3,\n",
    "        'major_purchase': 4, 'car': 5, 'medical': 6, 'moving': 7,\n",
    "        'vacation': 8, 'house': 9, 'renewable_energy': 10, 'other': 11\n",
    "    }\n",
    "    df_features['purpose_encoded'] = df_features['purpose'].map(purpose_mapping)\n",
    "    \n",
    "    # 6. Risk ratio features\n",
    "    df_features['dti_grade_interaction'] = df_features['dti'] * df_features['grade_encoded']\n",
    "    df_features['loan_inc_ratio'] = df_features['loan_amnt'] / df_features['annual_inc']\n",
    "    df_features['installment_inc_ratio'] = df_features['installment'] / (df_features['annual_inc'] / 12)\n",
    "    \n",
    "    # 7. Log transformations for skewed features\n",
    "    df_features['log_loan_amnt'] = np.log1p(df_features['loan_amnt'])\n",
    "    df_features['log_annual_inc'] = np.log1p(df_features['annual_inc'])\n",
    "    df_features['log_int_rate'] = np.log1p(df_features['int_rate'])\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "print('Creating engineered features...')\n",
    "df_features = create_features(df)\n",
    "print(f'Features created. New shape: {df_features.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define core features for modeling\n",
    "core_features = [\n",
    "    'int_rate', 'dti', 'annual_inc', 'installment', 'loan_amnt',\n",
    "    'grade_encoded', 'emp_length_numeric', 'home_ownership_encoded',\n",
    "    'verification_status_encoded', 'purpose_encoded',\n",
    "    'dti_grade_interaction', 'loan_inc_ratio', 'installment_inc_ratio',\n",
    "    'log_loan_amnt', 'log_annual_inc', 'log_int_rate'\n",
    "]\n",
    "\n",
    "# Check for missing values in core features\n",
    "missing_values = df_features[core_features].isnull().sum()\n",
    "print('Missing values in core features:')\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Risk-Based Data Segmentation\n",
    "\n",
    "We segment data based on DTI ratio and estimated FICO score (derived from grade):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_risk_data(df):\n",
    "    \"\"\"Segment data into risk categories based on DTI and credit grade\"\"\"\n",
    "    \n",
    "    # Estimate FICO score from grade (approximate mapping)\n",
    "    def estimate_fico_from_grade(grade):\n",
    "        fico_ranges = {\n",
    "            'A': (760, 850),\n",
    "            'B': (720, 759),\n",
    "            'C': (680, 719),\n",
    "            'D': (640, 679),\n",
    "            'E': (600, 639),\n",
    "            'F': (580, 599),\n",
    "            'G': (550, 579)\n",
    "        }\n",
    "        return np.mean(fico_ranges.get(grade, (650, 700)))\n",
    "    \n",
    "    df['estimated_fico'] = df['grade'].apply(estimate_fico_from_grade)\n",
    "    \n",
    "    # Define risk segments\n",
    "    low_risk_mask = (df['dti'] < 20) & (df['estimated_fico'] > 720)\n",
    "    medium_risk_mask = (df['dti'] >= 20) & (df['dti'] <= 35) & (df['estimated_fico'] >= 680) & (df['estimated_fico'] <= 720)\n",
    "    high_risk_mask = (df['dti'] > 35) | (df['estimated_fico'] < 680)\n",
    "    \n",
    "    low_risk_df = df[low_risk_mask].copy()\n",
    "    medium_risk_df = df[medium_risk_mask].copy()\n",
    "    high_risk_df = df[high_risk_mask].copy()\n",
    "    \n",
    "    print(f'Low Risk (DTI < 20% & FICO > 720): {len(low_risk_df)} applicants ({len(low_risk_df)/len(df)*100:.1f}%)')\n",
    "    print(f'Medium Risk (DTI 20-35% & FICO 680-720): {len(medium_risk_df)} applicants ({len(medium_risk_df)/len(df)*100:.1f}%)')\n",
    "    print(f'High Risk (DTI > 35% or FICO < 680): {len(high_risk_df)} applicants ({len(high_risk_df)/len(df)*100:.1f}%)')\n",
    "    \n",
    "    return low_risk_df, medium_risk_df, high_risk_df\n",
    "\n",
    "low_risk_df, medium_risk_df, high_risk_df = segment_risk_data(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check default rates in each segment\n",
    "def analyze_default_rates(df, segment_name):\n",
    "    default_rate = df['loan_status'].mean() * 100\n",
    "    avg_dti = df['dti'].mean()\n",
    "    avg_int_rate = df['int_rate'].mean()\n",
    "    avg_grade = df['grade_encoded'].mean()\n",
    "    \n",
    "    print(f'{segment_name}:')\n",
    "    print(f'  - Default Rate: {default_rate:.2f}%')\n",
    "    print(f'  - Average DTI: {avg_dti:.1f}%')\n",
    "    print(f'  - Average Interest Rate: {avg_int_rate:.1f}%')\n",
    "    print(f'  - Average Grade: {avg_grade:.1f}')\n",
    "    print(f'  - Total Applicants: {len(df)}')\n",
    "    print()\n",
    "\n",
    "print('Risk Segment Analysis:')\n",
    "print('='*50)\n",
    "analyze_default_rates(low_risk_df, 'Low Risk')\n",
    "analyze_default_rates(medium_risk_df, 'Medium Risk')\n",
    "analyze_default_rates(high_risk_df, 'High Risk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Training Specialized Models\n",
    "\n",
    "Each model is trained on its specific risk segment with features relevant to that segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_specialized_model(df, model_name, risk_category):\n",
    "    \"\"\"Train a specialized model for a specific risk category\"\"\"\n",
    "    \n",
    "    print(f'Training {model_name}...')\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df[core_features].fillna(0)  # Simple missing value imputation\n",
    "    y = df['loan_status']\n",
    "    \n",
    "    # Handle missing values\n",
    "    X.fillna(X.mean(), inplace=True)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Train Random Forest with parameters tuned for each risk category\n",
    "    if risk_category == 'low':\n",
    "        # Conservative model for low risk\n",
    "        rf_params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 6,\n",
    "            'min_samples_split': 20,\n",
    "            'min_samples_leaf': 10,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    elif risk_category == 'medium':\n",
    "        # Balanced model for medium risk\n",
    "        rf_params = {\n",
    "            'n_estimators': 150,\n",
    "            'max_depth': 8,\n",
    "            'min_samples_split': 15,\n",
    "            'min_samples_leaf': 8,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    else:  # high risk\n",
    "        # Sensitive model for high risk\n",
    "        rf_params = {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 10,\n",
    "            'min_samples_split': 10,\n",
    "            'min_samples_leaf': 5,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    \n",
    "    model = RandomForestClassifier(**rf_params)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f'{model_name} Performance:')\n",
    "    print(f'  - Accuracy: {accuracy:.3f}')\n",
    "    print(f'  - ROC AUC: {roc_auc:.3f}')\n",
    "    print(f'  - Training samples: {len(X_train)}')\n",
    "    print(f'  - Test samples: {len(X_test)}')\n",
    "    print()\n",
    "    \n",
    "    # Save model components\n",
    "    model_data = {\n",
    "        'model': model,\n",
    "        'scaler': scaler,\n",
    "        'features': core_features,\n",
    "        'risk_category': risk_category,\n",
    "        'performance': {\n",
    "            'accuracy': accuracy,\n",
    "            'roc_auc': roc_auc,\n",
    "            'training_samples': len(X_train)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save to models directory\n",
    "    joblib.dump(model_data, f'../models/{model_name}.pkl')\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': core_features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance.to_csv(f'../models/{model_name}_feature_importance.csv', index=False)\n",
    "    \n",
    "    print(f'{model_name} saved successfully!')\n",
    "    print(f'Top 5 important features:')\n",
    "    for i, row in feature_importance.head(5).iterrows():\n",
    "        print(f'  {i+1}. {row[\"feature\"]}: {row[\"importance\"]:.4f}')\n",
    "    print()\n",
    "    \n",
    "    return model_data, feature_importance"
   ",
    "models = {}\n",
    "feature_importances = {}\n",
    "\n",
    "# Train low risk model\n",
    "if len(low_risk_df) > 1000:  # Ensure sufficient data\n",
    "    models['low_risk'], feature_importances['low_risk'] = train_specialized_model(\n",
    "        low_risk_df, 'credit_risk_model_low', 'low'\n",
    "    )\n",
    "\n",
    "# Train medium risk model\n",
    "if len(medium_risk_df) > 1000:  # Ensure sufficient data\n",
    "    models['medium_risk'], feature_importances['medium_risk'] = train_specialized_model(\n",
    "        medium_risk_df, 'credit_risk_model_medium', 'medium'\n",
    "    )\n",
    "\n",
    "# Train high risk model\n",
    "if len(high_risk_df) > 1000:  # Ensure sufficient data\n",
    "    models['high_risk'], feature_importances['high_risk'] = train_specialized_model(\n",
    "        high_risk_df, 'credit_risk_model_high', 'high'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Risk Score Calculation Formula\n",
    "\n",
    "Our risk score calculation uses the following formula:\n",
    "\n",
    "```\n",
    "Risk Score = Base Score + (Probability × Weight) + Risk Adjustments\n",
    "\n",
    "Where:\n",
    "- Base Score = 20 (minimum score)\n",
    "- Probability = Model's predicted probability of default\n",
    "- Weight = 60 (scales probability to 0-60 range)\n",
    "- Risk Adjustments = Additional points for specific risk factors\n",
    "\n",
    "Risk Adjustments:\n",
    "- DTI > 30%: +10 points\n",
    "- DTI > 40%: +20 points\n",
    "- Grade E-F: +15 points\n",
    "- Grade G: +25 points\n",
    "- Employment < 1 year: +10 points\n",
    "- Employment < 5 years: +5 points\n",
    "- Loan amount > $30,000: +10 points\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_risk_score_with_formula(features, model_data):\n",
    "    \"\"\"Calculate risk score using transparent formula\"\"\"\n",
    "    \n",
    "    model = model_data['model']\n",
    "    scaler = model_data['scaler']\n",
    "    model_features = model_data['features']\n",
    "    risk_category = model_data['risk_category']\n",
    "    \n",
    "    # Prepare feature vector\n",
    "    feature_vector = []\n",
    "    for feature in model_features:\n",
    "        if feature in features:\n",
    "            feature_vector.append(features[feature])\n",
    "        else:\n",
    "            feature_vector.append(0)  # Default value\n",
    "    \n",
    "    # Scale features\n",
    "    feature_vector = np.array(feature_vector).reshape(1, -1)\n",
    "    feature_vector_scaled = scaler.transform(feature_vector)\n",
    "    \n",
    "    # Get prediction probability\n",
    "    default_probability = model.predict_proba(feature_vector_scaled)[0, 1]\n",
    "    \n",
    "    # Base score calculation\n",
    "    base_score = 20\n",
    "    probability_score = default_probability * 60\n",
    "    \n",
    "    # Risk adjustments\n",
    "    risk_adjustments = 0\n",
    "    \n",
    "    # DTI adjustments\n",
    "    dti = features.get('dti', 0)\n",
    "    if dti > 40:\n",
    "        risk_adjustments += 20\n",
    "    elif dti > 30:\n",
    "        risk_adjustments += 10\n",
    "    \n",
    "    # Grade adjustments\n",
    "    grade_encoded = features.get('grade_encoded', 3)  # Default to C\n",
    "    if grade_encoded >= 6:  # F or G\n",
    "        risk_adjustments += 25 if grade_encoded == 7 else 15\n",
    "    elif grade_encoded >= 5:  # E\n",
    "        risk_adjustments += 15\n",
    "    \n",
    "    # Employment adjustments\n",
    "    emp_length = features.get('emp_length_numeric', 2)\n",
    "    if emp_length < 1:\n",
    "        risk_adjustments += 10\n",
    "    elif emp_length < 5:\n",
    "        risk_adjustments += 5\n",
    "    \n",
    "    # Loan amount adjustments\n",
    "    loan_amnt = features.get('loan_amnt', 10000)\n",
    "    if loan_amnt > 30000:\n",
    "        risk_adjustments += 10\n",
    "    \n",
    "    # Category-specific adjustments\n",
    "    if risk_category == 'low':\n",
    "        # Low risk applicants get some reduction\n",
    "        risk_adjustments = max(0, risk_adjustments - 5)\n",
    "    elif risk_category == 'high':\n",
    "        # High risk applicants get extra scrutiny\n",
    "        risk_adjustments += 10\n",
    "    \n",
    "    # Final risk score\n",
    "    risk_score = base_score + probability_score + risk_adjustments\n",
    "    risk_score = min(100, max(0, risk_score))  # Clamp between 0-100\n",
    "    \n",
    "    return risk_score, default_probability, risk_adjustments\n",
    "\n",
    "# Test the formula with sample data\n",
    "print('Testing risk score calculation formula:')\n",
    "print('='*50)\n",
    "\n",
    "# Low risk example\n",
    "low_risk_example = {\n",
    "    'int_rate': 8.5, 'dti': 15, 'annual_inc': 85000, 'installment': 350,\n",
    "    'loan_amnt': 15000, 'grade_encoded': 1, 'emp_length_numeric': 8,\n",
    "    'home_ownership_encoded': 2, 'verification_status_encoded': 2,\n",
    "    'purpose_encoded': 1, 'dti_grade_interaction': 15, 'loan_inc_ratio': 0.18,\n",
    "    'installment_inc_ratio': 0.05, 'log_loan_amnt': 9.6, 'log_annual_inc': 11.35,\n",
    "    'log_int_rate': 2.28\n",
    "}\n",
    "\n",
    "if 'low_risk' in models:\n",
    "    score, prob, adjustments = calculate_risk_score_with_formula(low_risk_example, models['low_risk'])\n",
    "    print(f'Low Risk Example:')\n",
    "    print(f'  - Base Score: 20')\n",
    "    print(f'  - Probability Component: {prob*60:.1f}')\n",
    "    print(f'  - Risk Adjustments: {adjustments}')\n",
    "    print(f'  - Final Risk Score: {score:.1f}%')\n",
    "    print(f'  - Default Probability: {prob*100:.1f}%')\n",
    "    print()\n",
    "\n",
    "# High risk example\n",
    "high_risk_example = {\n",
    "    'int_rate': 15.5, 'dti': 38, 'annual_inc': 45000, 'installment': 650,\n",
    "    'loan_amnt': 35000, 'grade_encoded': 6, 'emp_length_numeric': 0.5,\n",
    "    'home_ownership_encoded': 1, 'verification_status_encoded': 0,\n",
    "    'purpose_encoded': 11, 'dti_grade_interaction': 228, 'loan_inc_ratio': 0.78,\n",
    "    'installment_inc_ratio': 0.17, 'log_loan_amnt': 10.46, 'log_annual_inc': 10.71,\n",
    "    'log_int_rate': 2.79\n",
    "}\n",
    "\n",
    "if 'high_risk' in models:\n",
    "    score, prob, adjustments = calculate_risk_score_with_formula(high_risk_example, models['high_risk'])\n",
    "    print(f'High Risk Example:')\n",
    "    print(f'  - Base Score: 20')\n",
    "    print(f'  - Probability Component: {prob*60:.1f}')\n",
    "    print(f'  - Risk Adjustments: {adjustments}')\n",
    "    print(f'  - Final Risk Score: {score:.1f}%')\n",
    "    print(f'  - Default Probability: {prob*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Save Model Summary\n",
    "\n",
    "Create a summary file with all model information for the dashboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model summary\n",
    "model_summary = {\n",
    "    'models_created': {\n",
    "        'low_risk': 'low_risk' in models,\n",
    "        'medium_risk': 'medium_risk' in models,\n",
    "        'high_risk': 'high_risk' in models\n",
    "    },\n",
    "    'preprocessing_steps': [\n",
    "        '1. Grade encoding (A=1, B=2, ..., G=7)',\n",
    "        '2. Employment length mapping to years',\n",
    "        '3. Home ownership encoding',\n",
    "        '4. Verification status encoding',\n",
    "        '5. Loan purpose encoding',\n",
    "        '6. Risk interaction features',\n",
    "        '7. Log transformations for skewed variables',\n",
    "        '8. Standard scaling for all features'\n",
    "    ],\n",
    "    'risk_formula': {\n",
    "        'base_score': 20,\n",
    "        'probability_weight': 60,\n",
    "        'dti_adjustments': {\n",
    "            'dti_30_40': '+10 points',\n",
    "            'dti_40_plus': '+20 points'\n",
    "        },\n",
    "        'grade_adjustments': {\n",
    "            'grade_E': '+15 points',\n",
    "            'grade_F': '+15 points',\n",
    "            'grade_G': '+25 points'\n",
    "        },\n",
    "        'employment_adjustments': {\n",
    "            'emp_lt_1_year': '+10 points',\n",
    "            'emp_1_5_years': '+5 points'\n",
    "        },\n",
    "        'loan_amount_adjustments': {\n",
    "            'loan_gt_30k': '+10 points'\n",
    "        }\n",
    "    },\n",
    "    'segment_performance': {\n",
    "        'low_risk': {\n",
    "            'default_rate': low_risk_df['loan_status'].mean(),\n",
    "            'sample_count': len(low_risk_df),\n",
    "            'avg_dti': low_risk_df['dti'].mean()\n",
    "        },\n",
    "        'medium_risk': {\n",
    "            'default_rate': medium_risk_df['loan_status'].mean(),\n",
    "            'sample_count': len(medium_risk_df),\n",
    "            'avg_dti': medium_risk_df['dti'].mean()\n",
    "        },\n",
    "        'high_risk': {\n",
    "            'default_rate': high_risk_df['loan_status'].mean(),\n",
    "            'sample_count': len(high_risk_df),\n",
    "            'avg_dti': high_risk_df['dti'].mean()\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add model performance metrics\n",
    "for category, model_info in models.items():\n",
    "    if category in model_summary['segment_performance']:\n",
    "        model_summary['segment_performance'][category]['model_accuracy'] = model_info['performance']['accuracy']\n",
    "        model_summary['segment_performance'][category]['model_roc_auc'] = model_info['performance']['roc_auc']\n",
    "\n",
    "# Save summary\n",
    "import json\n",
    "with open('../models/multi_model_summary.json', 'w') as f:\n",
    "    json.dump(model_summary, f, indent=2)\n",
    "\n",
    "print('Multi-model training completed successfully!')\n",
    "print(f'Summary saved to: ../models/multi_model_summary.json')\n",
    "print()\n",
    "print('Models created:')\n",
    "for category, created in model_summary['models_created'].items():\n",
    "    status = '✓' if created else '✗'\n",
    "    print(f'  {status} {category.replace(\"_\", \" \").title()} Model')\n",
    "print()\n",
    "print('Files created:')\n",
    "print('  - ../models/credit_risk_model_low.pkl')\n",
    "print('  - ../models/credit_risk_model_medium.pkl')\n",
    "print('  - ../models/credit_risk_model_high.pkl')\n",
    "print('  - ../models/multi_model_summary.json')\n",
    "print('  - Feature importance files for each model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}