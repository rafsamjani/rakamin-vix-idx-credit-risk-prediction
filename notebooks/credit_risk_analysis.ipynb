{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Risk Prediction Analysis\n",
    "\n",
    "**Author**: Rafsamjani Anugrah  \n",
    "**Date**: 2024  \n",
    "**Project**: Rakamin VIX Program - ID/X Partners Credit Risk Prediction\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook presents a comprehensive analysis of loan data from Lending Club (2007-2014) to build a credit risk prediction model. The analysis includes:\n",
    "\n",
    "1. **Data Exploration and Preprocessing**\n",
    "2. **Exploratory Data Analysis (EDA)**\n",
    "3. **Feature Engineering**\n",
    "4. **Model Development and Training**\n",
    "5. **Model Evaluation and Selection**\n",
    "6. **Business Insights and Recommendations**\n",
    "\n",
    "### Business Context\n",
    "\n",
    "ID/X Partners is a lending company that needs to assess creditworthiness of loan applicants to minimize financial losses while maximizing profitable lending opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "# Set styling\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    # Try loading from the data directory\n",
    "    df = pd.read_csv('../data/raw/loan_data_2007_2014.csv')\n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset file not found. Please ensure the data is in the correct location.\")\n",
    "    print(\"Expected path: ../data/raw/loan_data_2007_2014.csv\")\n",
    "    # For demonstration, create a sample dataset structure\n",
    "    print(\"\\nCreating sample dataset structure for demonstration...\")\n",
    "    # This would be replaced with actual data loading\n",
    "\n",
    "# Display basic information\n",
    "if 'df' in locals():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"DATASET INFORMATION\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total Records: {df.shape[0]:,}\")\n",
    "    print(f\"Total Features: {df.shape[1]}\")\n",
    "    print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows and column information\n",
    "if 'df' in locals():\n",
    "    print(\"First 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(\"\\nColumn Information:\")\n",
    "    print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding and Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data types and missing values\n",
    "if 'df' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"DATA QUALITY ASSESSMENT\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_analysis = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Data Type': df.dtypes.values,\n",
    "        'Missing Count': df.isnull().sum().values,\n",
    "        'Missing %': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "    })\n",
    "    \n",
    "    # Sort by missing percentage\n",
    "    missing_analysis = missing_analysis.sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    # Display columns with missing values\n",
    "    missing_cols = missing_analysis[missing_analysis['Missing Count'] > 0]\n",
    "    \n",
    "    print(f\"\\nColumns with missing values: {len(missing_cols)} out of {len(df.columns)}\")\n",
    "    print(\"\\nTop 20 columns with missing values:\")\n",
    "    display(missing_cols.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable distribution\n",
    "if 'df' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"TARGET VARIABLE ANALYSIS\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    # Loan status distribution\n",
    "    if 'loan_status' in df.columns:\n",
    "        status_counts = df['loan_status'].value_counts()\n",
    "        status_percentages = (status_counts / len(df) * 100).round(2)\n",
    "        \n",
    "        print(\"\\nLoan Status Distribution:\")\n",
    "        for status, count in status_counts.items():\n",
    "            percentage = status_percentages[status]\n",
    "            print(f\"  {status}: {count:,} ({percentage}%)\")\n",
    "        \n",
    "        # Visualize loan status distribution\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Bar plot\n",
    "        status_counts.plot(kind='bar', ax=ax1)\n",
    "        ax1.set_title('Loan Status Distribution')\n",
    "        ax1.set_xlabel('Loan Status')\n",
    "        ax1.set_ylabel('Count')\n",
    "        ax1.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Pie chart\n",
    "        ax2.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%')\n",
    "        ax2.set_title('Loan Status Proportion')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Focus on completed loans (Fully Paid and Charged Off)\n",
    "        completed_loans = ['Fully Paid', 'Charged Off']\n",
    "        df_completed = df[df['loan_status'].isin(completed_loans)].copy()\n",
    "        \n",
    "        print(f\"\\nCompleted loans (Fully Paid + Charged Off): {len(df_completed):,} ({len(df_completed)/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        # Default rate among completed loans\n",
    "        if len(df_completed) > 0:\n",
    "            default_rate = (df_completed['loan_status'] == 'Charged Off').mean() * 100\n",
    "            print(f\"Default rate among completed loans: {default_rate:.1f}%\")\n",
    "    else:\n",
    "        print(\"Loan status column not found in dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for completed loans and prepare target variable\n",
    "if 'df' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"DATA PREPROCESSING\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    # Filter for completed loans only\n",
    "    completed_loans = ['Fully Paid', 'Charged Off']\n",
    "    df_clean = df[df['loan_status'].isin(completed_loans)].copy()\n",
    "    \n",
    "    print(f\"Original dataset: {len(df):,} records\")\n",
    "    print(f\"Filtered dataset: {len(df_clean):,} records\")\n",
    "    print(f\"Records removed: {len(df) - len(df_clean):,} ({(len(df) - len(df_clean))/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Convert target variable to binary\n",
    "    df_clean['loan_status_binary'] = (df_clean['loan_status'] == 'Charged Off').astype(int)\n",
    "    \n",
    "    print(f\"\\nTarget variable distribution:\")\n",
    "    target_counts = df_clean['loan_status_binary'].value_counts()\n",
    "    for status, count in target_counts.items():\n",
    "        status_name = 'Charged Off' if status == 1 else 'Fully Paid'\n",
    "        percentage = count / len(df_clean) * 100\n",
    "        print(f\"  {status_name}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Class imbalance ratio\n",
    "    imbalance_ratio = target_counts[0] / target_counts[1]\n",
    "    print(f\"\\nClass imbalance ratio (Non-default:Default): {imbalance_ratio:.1f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and transform key variables\n",
    "if 'df_clean' in locals():\n",
    "    print(\"Cleaning and transforming variables...\")\n",
    "    \n",
    "    # Convert percentage columns\n",
    "    percentage_cols = ['int_rate', 'revol_util']\n",
    "    for col in percentage_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = df_clean[col].astype(str).str.replace('%', '').astype(float)\n",
    "            print(f\"Converted {col} to numeric\")\n",
    "    \n",
    "    # Clean employment length\n",
    "    if 'emp_length' in df_clean.columns:\n",
    "        emp_length_mapping = {\n",
    "            '< 1 year': 0.5, '1 year': 1, '2 years': 2, '3 years': 3,\n",
    "            '4 years': 4, '5 years': 5, '6 years': 6, '7 years': 7,\n",
    "            '8 years': 8, '9 years': 9, '10+ years': 10\n",
    "        }\n",
    "        df_clean['emp_length_numeric'] = df_clean['emp_length'].map(emp_length_mapping)\n",
    "        print(f\"Converted employment length to numeric\")\n",
    "    \n",
    "    # Convert date columns\n",
    "    date_cols = ['issue_d', 'earliest_cr_line']\n",
    "    for col in date_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], format='%b-%y', errors='coerce')\n",
    "            print(f\"Converted {col} to datetime\")\n",
    "    \n",
    "    # Create derived features\n",
    "    if 'fico_range_low' in df_clean.columns and 'fico_range_high' in df_clean.columns:\n",
    "        df_clean['fico_avg'] = (df_clean['fico_range_low'] + df_clean['fico_range_high']) / 2\n",
    "        print(\"Created FICO score average\")\n",
    "    \n",
    "    # Financial ratios\n",
    "    if 'annual_inc' in df_clean.columns and 'loan_amnt' in df_clean.columns:\n",
    "        df_clean['loan_to_income_ratio'] = df_clean['loan_amnt'] / (df_clean['annual_inc'] / 12)\n",
    "        print(\"Created loan-to-income ratio\")\n",
    "    \n",
    "    # Credit age\n",
    "    if 'earliest_cr_line' in df_clean.columns and 'issue_d' in df_clean.columns:\n",
    "        df_clean['credit_age_years'] = (\n",
    "            df_clean['issue_d'] - df_clean['earliest_cr_line']\n",
    "        ).dt.days / 365.25\n",
    "        print(\"Created credit age in years\")\n",
    "    \n",
    "    print(f\"\\nData cleaning completed. Final shape: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Loan Characteristics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze loan characteristics by default status\n",
    "if 'df_clean' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"LOAN CHARACTERISTICS ANALYSIS\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    # Key loan characteristics\n",
    "    loan_chars = ['loan_amnt', 'int_rate', 'term', 'purpose', 'grade']\n",
    "    available_chars = [col for col in loan_chars if col in df_clean.columns]\n",
    "    \n",
    "    for char in available_chars:\n",
    "        print(f\"\\n--- {char.upper()} ANALYSIS ---\")\n",
    "        \n",
    "        if df_clean[char].dtype in ['object', 'category']:\n",
    "            # Categorical analysis\n",
    "            cross_tab = pd.crosstab(df_clean[char], df_clean['loan_status_binary'], normalize='index') * 100\n",
    "            cross_tab.columns = ['Fully Paid Rate', 'Default Rate']\n",
    "            \n",
    "            print(f\"Default rates by {char}:\")\n",
    "            display(cross_tab.sort_values('Default Rate', ascending=False))\n",
    "            \n",
    "            # Visualization\n",
    "            if len(df_clean[char].unique()) <= 10:  # Only plot for reasonable number of categories\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                cross_tab['Default Rate'].sort_values().plot(kind='bar')\n",
    "                plt.title(f'Default Rate by {char}')\n",
    "                plt.xlabel(char)\n",
    "                plt.ylabel('Default Rate (%)')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.show()\n",
    "        else:\n",
    "            # Numerical analysis\n",
    "            stats_by_status = df_clean.groupby('loan_status_binary')[char].describe()\n",
    "            stats_by_status.index = ['Fully Paid', 'Charged Off']\n",
    "            \n",
    "            print(f\"{char} statistics by loan status:\")\n",
    "            display(stats_by_status)\n",
    "            \n",
    "            # Visualization\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Box plot\n",
    "            df_clean.boxplot(column=char, by='loan_status_binary', ax=ax1)\n",
    "            ax1.set_title(f'{char} by Loan Status')\n",
    "            ax1.set_xlabel('Loan Status (0=Fully Paid, 1=Charged Off)')\n",
    "            \n",
    "            # Distribution\n",
    "            for status in [0, 1]:\n",
    "                status_name = 'Fully Paid' if status == 0 else 'Charged Off'\n",
    "                df_clean[df_clean['loan_status_binary'] == status][char].hist(\n",
    "                    alpha=0.7, label=status_name, ax=ax2, bins=30\n",
    "                )\n",
    "            ax2.set_title(f'Distribution of {char}')\n",
    "            ax2.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Borrower Demographics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze borrower demographics\n",
    "if 'df_clean' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"BORROWER DEMOGRAPHICS ANALYSIS\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    # Key borrower characteristics\n",
    "    borrower_chars = ['annual_inc', 'emp_length_numeric', 'home_ownership', 'dti', 'fico_avg']\n",
    "    available_borrower_chars = [col for col in borrower_chars if col in df_clean.columns]\n",
    "    \n",
    "    for char in available_borrower_chars:\n",
    "        print(f\"\\n--- {char.upper()} ANALYSIS ---\")\n",
    "        \n",
    "        if df_clean[char].dtype in ['object', 'category']:\n",
    "            # Categorical analysis\n",
    "            cross_tab = pd.crosstab(df_clean[char], df_clean['loan_status_binary'], normalize='index') * 100\n",
    "            cross_tab.columns = ['Fully Paid Rate', 'Default Rate']\n",
    "            \n",
    "            print(f\"Default rates by {char}:\")\n",
    "            display(cross_tab.sort_values('Default Rate', ascending=False))\n",
    "            \n",
    "            # Visualization\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            cross_tab['Default Rate'].sort_values().plot(kind='bar', color='coral')\n",
    "            plt.title(f'Default Rate by {char}')\n",
    "            plt.xlabel(char)\n",
    "            plt.ylabel('Default Rate (%)')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.show()\n",
    "        else:\n",
    "            # Numerical analysis\n",
    "            stats_by_status = df_clean.groupby('loan_status_binary')[char].describe()\n",
    "            stats_by_status.index = ['Fully Paid', 'Charged Off']\n",
    "            \n",
    "            print(f\"{char} statistics by loan status:\")\n",
    "            display(stats_by_status)\n",
    "            \n",
    "            # Calculate correlation with default\n",
    "            correlation = df_clean[char].corr(df_clean['loan_status_binary'])\n",
    "            print(f\"Correlation with default: {correlation:.3f}\")\n",
    "            \n",
    "            # Visualization\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "            \n",
    "            # Box plot\n",
    "            df_clean.boxplot(column=char, by='loan_status_binary', ax=axes[0,0])\n",
    "            axes[0,0].set_title(f'{char} by Loan Status')\n",
    "            \n",
    "            # Histogram\n",
    "            axes[0,1].hist(df_clean[df_clean['loan_status_binary'] == 0][char], \n",
    "                          alpha=0.7, label='Fully Paid', bins=30, color='green')\n",
    "            axes[0,1].hist(df_clean[df_clean['loan_status_binary'] == 1][char], \n",
    "                          alpha=0.7, label='Charged Off', bins=30, color='red')\n",
    "            axes[0,1].set_title(f'Distribution of {char}')\n",
    "            axes[0,1].legend()\n",
    "            \n",
    "            # Density plot\n",
    "            df_clean[df_clean['loan_status_binary'] == 0][char].plot.density(\n",
    "                ax=axes[1,0], label='Fully Paid', color='green')\n",
    "            df_clean[df_clean['loan_status_binary'] == 1][char].plot.density(\n",
    "                ax=axes[1,0], label='Charged Off', color='red')\n",
    "            axes[1,0].set_title(f'Density Plot of {char}')\n",
    "            axes[1,0].legend()\n",
    "            \n",
    "            # Default rate by quantiles\n",
    "            try:\n",
    "                quantiles = pd.qcut(df_clean[char], q=10, duplicates='drop')\n",
    "                default_by_quantile = df_clean.groupby(quantiles)['loan_status_binary'].mean() * 100\n",
    "                default_by_quantile.plot(kind='bar', ax=axes[1,1], color='purple')\n",
    "                axes[1,1].set_title(f'Default Rate by {char} Quantiles')\n",
    "                axes[1,1].set_xlabel(f'{char} Quantiles')\n",
    "                axes[1,1].set_ylabel('Default Rate (%)')\n",
    "                axes[1,1].tick_params(axis='x', rotation=45)\n",
    "            except:\n",
    "                axes[1,1].text(0.5, 0.5, 'Cannot create quantiles', \n",
    "                              ha='center', va='center', transform=axes[1,1].transAxes)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Credit History Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze credit history variables\n",
    "if 'df_clean' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"CREDIT HISTORY ANALYSIS\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    # Credit history variables\n",
    "    credit_vars = ['delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', \n",
    "                  'revol_bal', 'revol_util', 'credit_age_years']\n",
    "    available_credit_vars = [col for col in credit_vars if col in df_clean.columns]\n",
    "    \n",
    "    # Create credit score categories\n",
    "    if 'fico_avg' in df_clean.columns:\n",
    "        df_clean['fico_category'] = pd.cut(\n",
    "            df_clean['fico_avg'],\n",
    "            bins=[0, 680, 720, 760, 850],\n",
    "            labels=['Fair (680-)', 'Good (680-720)', 'Very Good (720-760)', 'Excellent (760+)']\n",
    "        )\n",
    "        \n",
    "        # Default rate by FICO category\n",
    "        fico_default_rates = df_clean.groupby('fico_category')['loan_status_binary'].mean() * 100\n",
    "        print(\"Default rate by FICO category:\")\n",
    "        display(fico_default_rates)\n",
    "        \n",
    "        # Visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        fico_default_rates.sort_values().plot(kind='bar', color='navy')\n",
    "        plt.title('Default Rate by FICO Score Category')\n",
    "        plt.ylabel('Default Rate (%)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "    \n",
    "    # Analyze other credit variables\n",
    "    for var in available_credit_vars:\n",
    "        if var == 'fico_avg' or var == 'fico_category':\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- {var.upper()} ANALYSIS ---\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(f\"Statistics for {var}:\")\n",
    "        print(df_clean[var].describe())\n",
    "        \n",
    "        # Default rate by categories\n",
    "        if df_clean[var].nunique() <= 10:  # Discrete variable\n",
    "            default_by_var = df_clean.groupby(var)['loan_status_binary'].mean() * 100\n",
    "            print(f\"\\nDefault rate by {var}:\")\n",
    "            display(default_by_var)\n",
    "            \n",
    "            # Visualization\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            default_by_var.plot(kind='bar', color='darkorange')\n",
    "            plt.title(f'Default Rate by {var}')\n",
    "            plt.ylabel('Default Rate (%)')\n",
    "            plt.show()\n",
    "        else:  # Continuous variable\n",
    "            # Create bins\n",
    "            try:\n",
    "                df_clean[f'{var}_binned'] = pd.qcut(df_clean[var], q=5, duplicates='drop')\n",
    "                default_by_bins = df_clean.groupby(f'{var}_binned')['loan_status_binary'].mean() * 100\n",
    "                \n",
    "                print(f\"\\nDefault rate by {var} quintiles:\")\n",
    "                display(default_by_bins)\n",
    "                \n",
    "                # Visualization\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                default_by_bins.plot(kind='bar', color='darkorange')\n",
    "                plt.title(f'Default Rate by {var} Quintiles')\n",
    "                plt.ylabel('Default Rate (%)')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"Could not create bins for {var}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "if 'df_clean' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"CORRELATION ANALYSIS\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    # Select numerical columns for correlation\n",
    "    numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    # Remove some ID or highly specific columns\n",
    "    numerical_cols = [col for col in numerical_cols if \n",
    "                     not any(skip in col.lower() for skip in ['id', 'member_id', 'policy_code'])]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df_clean[numerical_cols].corr()\n",
    "    \n",
    "    # Focus on correlation with target variable\n",
    "    target_correlation = correlation_matrix['loan_status_binary'].sort_values(ascending=False)\n",
    "    \n",
    "    print(\"\\nCorrelation with Default (loan_status_binary):\")\n",
    "    print(target_correlation)\n",
    "    \n",
    "    # Top correlations with target\n",
    "    top_correlations = target_correlation.abs().sort_values(ascending=False).head(15)\n",
    "    print(f\"\\nTop 15 variables most correlated with default:\")\n",
    "    display(top_correlations)\n",
    "    \n",
    "    # Heatmap of top correlated features\n",
    "    top_features = top_correlations.index.tolist()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_subset = df_clean[top_features].corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    mask = np.triu(np.ones_like(correlation_subset, dtype=bool))\n",
    "    sns.heatmap(correlation_subset, \n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                mask=mask,\n",
    "                fmt='.2f',\n",
    "                square=True)\n",
    "    plt.title('Correlation Heatmap of Top Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify multicollinearity (high correlations between features)\n",
    "    print(\"\\nHigh correlations between features (|r| > 0.7):\")\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_value) > 0.7:\n",
    "                var1 = correlation_matrix.columns[i]\n",
    "                var2 = correlation_matrix.columns[j]\n",
    "                high_corr_pairs.append((var1, var2, corr_value))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        for var1, var2, corr in high_corr_pairs:\n",
    "            print(f\"  {var1} â†” {var2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"  No high correlations found between features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "if 'df_clean' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"FEATURE ENGINEERING\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    df_features = df_clean.copy()\n",
    "    \n",
    "    # Create additional derived features\n",
    "    print(\"Creating additional engineered features...\")\n",
    "    \n",
    "    # Effective DTI (including new loan payment)\n",
    "    if all(col in df_features.columns for col in ['dti', 'installment', 'annual_inc']):\n",
    "        monthly_income = df_features['annual_inc'] / 12\n",
    "        new_payment_dti = (df_features['installment'] / monthly_income) * 100\n",
    "        df_features['effective_dti'] = df_features['dti'] + new_payment_dti\n",
    "        print(\"Created effective DTI (including new loan payment)\")\n",
    "    \n",
    "    # Monthly debt burden\n",
    "    if all(col in df_features.columns for col in ['installment', 'annual_inc']):\n",
    "        df_features['monthly_debt_burden'] = df_features['installment'] / (df_features['annual_inc'] / 12)\n",
    "        print(\"Created monthly debt burden ratio\")\n",
    "    \n",
    "    # Employment stability score\n",
    "    if all(col in df_features.columns for col in ['emp_length_numeric', 'home_ownership']):\n",
    "        df_features['employment_stability'] = (\n",
    "            df_features['emp_length_numeric'] * 7 +\n",
    "            (df_features['home_ownership'].isin(['MORTGAGE', 'OWN']) * 20)\n",
    "        )\n",
    "        print(\"Created employment stability score\")\n",
    "    \n",
    "    # Credit utilization ratio (if available)\n",
    "    if 'revol_util' in df_features.columns:\n",
    "        df_features['high_utilization'] = (df_features['revol_util'] > 80).astype(int)\n",
    "        print(\"Created high utilization flag\")\n",
    "    \n",
    "    # Risk flags\n",
    "    if 'fico_avg' in df_features.columns:\n",
    "        df_features['low_fico'] = (df_features['fico_avg'] < 680).astype(int)\n",
    "        print(\"Created low FICO flag\")\n",
    "    \n",
    "    if 'dti' in df_features.columns:\n",
    "        df_features['high_dti'] = (df_features['dti'] > 30).astype(int)\n",
    "        print(\"Created high DTI flag\")\n",
    "    \n",
    "    print(f\"\\nFeature engineering completed. Final shape: {df_features.shape}\")\n",
    "    \n",
    "    # Display new features\n",
    "    new_features = [col for col in df_features.columns if col not in df_clean.columns]\n",
    "    print(f\"\\nCreated {len(new_features)} new features:\")\n",
    "    for feature in new_features:\n",
    "        print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "if 'df_features' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"FEATURE SELECTION\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    # Select key features for modeling\n",
    "    # Based on EDA, domain knowledge, and correlation analysis\n",
    "    \n",
    "    key_features = [\n",
    "        # Loan characteristics\n",
    "        'loan_amnt', 'term', 'int_rate', 'installment', 'grade', 'purpose',\n",
    "        \n",
    "        # Borrower information\n",
    "        'annual_inc', 'emp_length_numeric', 'home_ownership', 'verification_status', 'dti',\n",
    "        \n",
    "        # Credit history\n",
    "        'fico_avg', 'credit_age_years', 'delinq_2yrs', 'inq_last_6mths', \n",
    "        'open_acc', 'pub_rec', 'revol_bal', 'revol_util',\n",
    "        \n",
    "        # Engineered features\n",
    "        'loan_to_income_ratio', 'effective_dti', 'monthly_debt_burden',\n",
    "        'employment_stability'\n",
    "    ]\n",
    "    \n",
    "    # Filter available features\n",
    "    available_features = [f for f in key_features if f in df_features.columns]\n",
    "    \n",
    "    print(f\"Selected {len(available_features)} features for modeling:\")\n",
    "    for i, feature in enumerate(available_features, 1):\n",
    "        print(f\"  {i:2d}. {feature}\")\n",
    "    \n",
    "    # Create final feature matrix\n",
    "    X = df_features[available_features].copy()\n",
    "    y = df_features['loan_status_binary']\n",
    "    \n",
    "    print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "    print(f\"Target vector shape: {y.shape}\")\n",
    "    \n",
    "    # Check for missing values in selected features\n",
    "    missing_in_features = X.isnull().sum()\n",
    "    if missing_in_features.sum() > 0:\n",
    "        print(f\"\\nMissing values in selected features:\")\n",
    "        display(missing_in_features[missing_in_features > 0])\n",
    "    else:\n",
    "        print(\"\\nNo missing values in selected features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "if 'X' in locals() and 'y' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"DATA PREPARATION FOR MODELING\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "    print(f\"Training set default rate: {y_train.mean()*100:.1f}%\")\n",
    "    print(f\"Test set default rate: {y_test.mean()*100:.1f}%\")\n",
    "    \n",
    "    # Preprocess categorical variables\n",
    "    def preprocess_features(X_train, X_test):\n",
    "        \"\"\"Preprocess features for modeling\"\"\"\n",
    "        X_train_processed = X_train.copy()\n",
    "        X_test_processed = X_test.copy()\n",
    "        \n",
    "        # Handle categorical variables\n",
    "        categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "        numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "        \n",
    "        # Encode categorical variables\n",
    "        for col in categorical_cols:\n",
    "            if col == 'grade':\n",
    "                # Ordinal encoding for loan grades\n",
    "                grade_mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n",
    "                X_train_processed[col] = X_train[col].map(grade_mapping).fillna(4)\n",
    "                X_test_processed[col] = X_test[col].map(grade_mapping).fillna(4)\n",
    "            else:\n",
    "                # One-hot encoding for other categorical variables\n",
    "                encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "                \n",
    "                # Fit on training data\n",
    "                train_encoded = encoder.fit_transform(X_train[[col]])\n",
    "                feature_names = [f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
    "                \n",
    "                train_encoded_df = pd.DataFrame(train_encoded, columns=feature_names, index=X_train.index)\n",
    "                X_train_processed = pd.concat([X_train_processed.drop(col, axis=1), train_encoded_df], axis=1)\n",
    "                \n",
    "                # Transform test data\n",
    "                test_encoded = encoder.transform(X_test[[col]])\n",
    "                test_encoded_df = pd.DataFrame(test_encoded, columns=feature_names, index=X_test.index)\n",
    "                X_test_processed = pd.concat([X_test_processed.drop(col, axis=1), test_encoded_df], axis=1)\n",
    "        \n",
    "        # Handle missing values in numerical columns\n",
    "        for col in numerical_cols:\n",
    "            if X_train_processed[col].isnull().any():\n",
    "                median_value = X_train_processed[col].median()\n",
    "                X_train_processed[col].fillna(median_value, inplace=True)\n",
    "                X_test_processed[col].fillna(median_value, inplace=True)\n",
    "        \n",
    "        # Scale numerical features\n",
    "        scaler = StandardScaler()\n",
    "        numerical_cols_processed = X_train_processed.select_dtypes(include=['int64', 'float64']).columns\n",
    "        \n",
    "        X_train_processed[numerical_cols_processed] = scaler.fit_transform(X_train_processed[numerical_cols_processed])\n",
    "        X_test_processed[numerical_cols_processed] = scaler.transform(X_test_processed[numerical_cols_processed])\n",
    "        \n",
    "        return X_train_processed, X_test_processed, scaler, encoder\n",
    "    \n",
    "    X_train_processed, X_test_processed, scaler, encoders = preprocess_features(X_train, X_test)\n",
    "    \n",
    "    print(f\"\\nProcessed training set shape: {X_train_processed.shape}\")\n",
    "    print(f\"Processed test set shape: {X_test_processed.shape}\")\n",
    "    \n",
    "    # Handle class imbalance with SMOTE\n",
    "    print(\"\\nApplying SMOTE to handle class imbalance...\")\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)\n",
    "    \n",
    "    print(f\"Original training set: {X_train_processed.shape}\")\n",
    "    print(f\"Resampled training set: {X_train_resampled.shape}\")\n",
    "    print(f\"Original target distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "    print(f\"Resampled target distribution: {pd.Series(y_train_resampled).value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "if 'X_train_resampled' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"MODEL TRAINING\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            random_state=42\n",
    "        ),\n",
    "        'XGBoost': xgb.XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=10,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    results = {}\n",
    "    best_model = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n--- Training {name} ---\")\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_processed)\n",
    "        y_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_proba)\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'roc_auc': roc_auc\n",
    "        }\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'metrics': metrics,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy:  {accuracy:.3f}\")\n",
    "        print(f\"Precision: {precision:.3f}\")\n",
    "        print(f\"Recall:    {recall:.3f}\")\n",
    "        print(f\"F1-Score:  {f1:.3f}\")\n",
    "        print(f\"ROC-AUC:   {roc_auc:.3f}\")\n",
    "        \n",
    "        # Track best model\n",
    "        if roc_auc > best_score:\n",
    "            best_score = roc_auc\n",
    "            best_model = name\n",
    "            best_model_instance = model\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(f\"BEST MODEL: {best_model} with ROC-AUC: {best_score:.3f}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed evaluation of the best model\n",
    "if 'results' in locals() and 'best_model' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(f\"DETAILED EVALUATION - {best_model.upper()}\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    best_result = results[best_model]\n",
    "    model = best_result['model']\n",
    "    y_pred = best_result['predictions']\n",
    "    y_proba = best_result['probabilities']\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Negatives:  {tn:6d} (Good loans correctly approved)\")\n",
    "    print(f\"False Positives: {fp:6d} (Good loans incorrectly rejected)\")\n",
    "    print(f\"False Negatives: {fn:6d} (Bad loans incorrectly approved)\")\n",
    "    print(f\"True Positives:  {tp:6d} (Bad loans correctly rejected)\")\n",
    "    \n",
    "    # Visualize confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Fully Paid', 'Charged Off'],\n",
    "                yticklabels=['Fully Paid', 'Charged Off'])\n",
    "    plt.title(f'Confusion Matrix - {best_model}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC Curve (AUC = {best_result[\"metrics\"][\"roc_auc\"]:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {best_model}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature Importance (for tree-based models)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': X_test_processed.columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 15 Most Important Features:\")\n",
    "        display(feature_importance.head(15))\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_features = feature_importance.head(15)\n",
    "        plt.barh(range(len(top_features)), top_features['importance'])\n",
    "        plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title(f'Top 15 Feature Importance - {best_model}')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, \n",
    "                              target_names=['Fully Paid', 'Charged Off']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison visualization\n",
    "if 'results' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for name, result in results.items():\n",
    "        metrics = result['metrics']\n",
    "        comparison_data.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'F1-Score': metrics['f1'],\n",
    "            'ROC-AUC': metrics['roc_auc']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data).set_index('Model')\n",
    "    \n",
    "    print(\"Model Performance Comparison:\")\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    metrics_list = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "    \n",
    "    for i, metric in enumerate(metrics_list):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        \n",
    "        comparison_df[metric].sort_values().plot(kind='bar', ax=axes[row, col], color='skyblue')\n",
    "        axes[row, col].set_title(f'{metric} Comparison')\n",
    "        axes[row, col].set_ylabel(metric)\n",
    "        axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Hide empty subplot\n",
    "    axes[1, 2].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create radar chart for overall comparison\n",
    "    from math import pi\n",
    "    \n",
    "    # Number of variables\n",
    "    categories = metrics_list\n",
    "    N = len(categories)\n",
    "    \n",
    "    # Angle for each axis\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    # Initialize radar chart\n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    # Plot each model\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    for i, (model_name, row) in enumerate(comparison_df.iterrows()):\n",
    "        values = row[categories].values.flatten().tolist()\n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[i])\n",
    "        ax.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    # Add labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Model Performance Radar Chart', size=16, pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business impact simulation\n",
    "if 'results' in locals() and 'best_model' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"BUSINESS IMPACT ANALYSIS\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    best_result = results[best_model]\n",
    "    y_pred = best_result['predictions']\n",
    "    y_proba = best_result['probabilities']\n",
    "    \n",
    "    # Business assumptions\n",
    "    avg_loan_amount = X_test['loan_amnt'].mean() if 'loan_amnt' in X_test.columns else 15000\n",
    "    avg_interest_rate = X_test['int_rate'].mean() if 'int_rate' in X_test.columns else 0.13\n",
    "    default_loss_rate = 0.60  # 60% loss on default\n",
    "    cost_of_capital = 0.05   # 5% cost of funds\n",
    "    \n",
    "    # Confusion matrix values\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    \n",
    "    # Calculate financial impact\n",
    "    # Benefits\n",
    "    profit_from_good_loans = tn * avg_loan_amount * avg_interest_rate\n",
    "    savings_from_avoided_defaults = tp * avg_loan_amount * default_loss_rate\n",
    "    \n",
    "    # Costs\n",
    "    opportunity_cost = fp * avg_loan_amount * (avg_interest_rate - cost_of_capital)\n",
    "    default_losses = fn * avg_loan_amount * default_loss_rate\n",
    "    \n",
    "    # Net impact\n",
    "    total_benefit = profit_from_good_loans + savings_from_avoided_defaults\n",
    "    total_cost = opportunity_cost + default_losses\n",
    "    net_financial_impact = total_benefit - total_cost\n",
    "    \n",
    "    print(\"\\nBusiness Impact Simulation:\")\n",
    "    print(f\"Average loan amount: ${avg_loan_amount:,.0f}\")\n",
    "    print(f\"Average interest rate: {avg_interest_rate:.1%}\")\n",
    "    print(f\"Default loss rate: {default_loss_rate:.1%}\")\n",
    "    print(f\"Cost of capital: {cost_of_capital:.1%}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINANCIAL IMPACT BREAKDOWN\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nðŸ’° BENEFITS:\")\n",
    "    print(f\"  Profit from good loans:        ${profit_from_good_loans:,.0f}\")\n",
    "    print(f\"  Savings from avoided defaults:  ${savings_from_avoided_defaults:,.0f}\")\n",
    "    print(f\"  Total Benefits:                 ${total_benefit:,.0f}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¸ COSTS:\")\n",
    "    print(f\"  Opportunity cost (false pos):  ${opportunity_cost:,.0f}\")\n",
    "    print(f\"  Default losses (false neg):    ${default_losses:,.0f}\")\n",
    "    print(f\"  Total Costs:                    ${total_cost:,.0f}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š NET FINANCIAL IMPACT:\")\n",
    "    print(f\"  Net Impact:                     ${net_financial_impact:,.0f}\")\n",
    "    \n",
    "    if net_financial_impact > 0:\n",
    "        print(f\"  âœ… Model creates ${net_financial_impact:,.0f} positive financial impact\")\n",
    "    else:\n",
    "        print(f\"  âŒ Model results in ${abs(net_financial_impact):,.0f} negative financial impact\")\n",
    "    \n",
    "    # Calculate ROI\n",
    "    roi = (net_financial_impact / total_cost) * 100 if total_cost > 0 else 0\n",
    "    print(f\"  Return on Investment: {roi:.1f}%\")\n",
    "    \n",
    "    # Optimal threshold analysis\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"OPTIMAL THRESHOLD ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    thresholds = np.arange(0.1, 0.9, 0.1)\n",
    "    threshold_analysis = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresh = (y_proba >= threshold).astype(int)\n",
    "        tn_t, fp_t, fn_t, tp_t = confusion_matrix(y_test, y_pred_thresh).ravel()\n",
    "        \n",
    "        # Calculate financial impact for this threshold\n",
    "        benefits = (tn_t * avg_loan_amount * avg_interest_rate + \n",
    "                   tp_t * avg_loan_amount * default_loss_rate)\n",
    "        costs = (fp_t * avg_loan_amount * (avg_interest_rate - cost_of_capital) + \n",
    "                fn_t * avg_loan_amount * default_loss_rate)\n",
    "        net_impact = benefits - costs\n",
    "        \n",
    "        threshold_analysis.append({\n",
    "            'threshold': threshold,\n",
    "            'net_impact': net_impact,\n",
    "            'approval_rate': (tn_t + fn_t) / len(y_test),\n",
    "            'default_rate': fn_t / (tn_t + fn_t) if (tn_t + fn_t) > 0 else 0\n",
    "        })\n",
    "    \n",
    "    threshold_df = pd.DataFrame(threshold_analysis)\n",
    "    best_threshold_row = threshold_df.loc[threshold_df['net_impact'].idxmax()]\n",
    "    \n",
    "    print(\"\\nThreshold Analysis:\")\n",
    "    print(threshold_df.round(2))\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Optimal Threshold: {best_threshold_row['threshold']:.2f}\")\n",
    "    print(f\"   Expected Net Impact: ${best_threshold_row['net_impact']:,.0f}\")\n",
    "    print(f\"   Approval Rate: {best_threshold_row['approval_rate']:.1%}\")\n",
    "    print(f\"   Default Rate: {best_threshold_row['default_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. SHAP Model Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP analysis for model interpretability\n",
    "if 'best_model_instance' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"SHAP MODEL INTERPRETABILITY\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    try:\n",
    "        # Sample data for SHAP analysis (to speed up computation)\n",
    "        sample_size = min(100, len(X_test_processed))\n",
    "        sample_indices = np.random.choice(len(X_test_processed), sample_size, replace=False)\n",
    "        X_sample = X_test_processed.iloc[sample_indices]\n",
    "        \n",
    "        print(f\"Running SHAP analysis on {sample_size} samples...\")\n",
    "        \n",
    "        # Create SHAP explainer\n",
    "        if best_model == 'XGBoost' or best_model == 'Random Forest':\n",
    "            explainer = shap.TreeExplainer(best_model_instance)\n",
    "        else:\n",
    "            explainer = shap.LinearExplainer(best_model_instance, X_sample)\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        shap_values = explainer.shap_values(X_sample)\n",
    "        \n",
    "        # For binary classification, get values for positive class\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]\n",
    "        \n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_sample, plot_type=\"bar\", max_display=15)\n",
    "        plt.title(f'SHAP Feature Importance - {best_model}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Detailed summary plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_sample, max_display=15)\n",
    "        plt.title(f'SHAP Summary Plot - {best_model}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Individual explanations for a few samples\n",
    "        print(\"\\nIndividual Sample Explanations:\")\n",
    "        for i in range(min(3, len(X_sample))):\n",
    "            sample_idx = sample_indices[i]\n",
    "            actual_outcome = 'Charged Off' if y_test.iloc[sample_idx] == 1 else 'Fully Paid'\n",
    "            predicted_prob = y_proba[sample_idx]\n",
    "            \n",
    "            print(f\"\\n--- Sample {i+1} ---\")\n",
    "            print(f\"Actual Outcome: {actual_outcome}\")\n",
    "            print(f\"Predicted Default Probability: {predicted_prob:.3f}\")\n",
    "            \n",
    "            # Force plot\n",
    "            plt.figure(figsize=(12, 3))\n",
    "            shap.force_plot(explainer.expected_value, shap_values[i], X_sample.iloc[i], \n",
    "                          matplotlib=True, show=False)\n",
    "            plt.title(f'SHAP Force Plot - Sample {i+1}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"SHAP analysis failed: {str(e)}\")\n",
    "        print(\"This might be due to memory limitations or model compatibility issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate final summary and recommendations\n",
    "if 'results' in locals() and 'best_model' in locals():\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    print(\"FINAL CONCLUSIONS AND RECOMMENDATIONS\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    best_result = results[best_model]\n",
    "    metrics = best_result['metrics']\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ MODEL PERFORMANCE SUMMARY\")\n",
    "    print(f\"   Best Model: {best_model}\")\n",
    "    print(f\"   Accuracy: {metrics['accuracy']:.1%}\")\n",
    "    print(f\"   Precision: {metrics['precision']:.1%}\")\n",
    "    print(f\"   Recall: {metrics['recall']:.1%}\")\n",
    "    print(f\"   F1-Score: {metrics['f1']:.1%}\")\n",
    "    print(f\"   ROC-AUC: {metrics['roc_auc']:.1%}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ KEY BUSINESS INSIGHTS\")\n",
    "    \n",
    "    # Most important risk factors (if feature importance available)\n",
    "    if 'feature_importance' in locals():\n",
    "        print(\"\\nTop 5 Risk Factors:\")\n",
    "        for i, (_, row) in enumerate(feature_importance.head(5).iterrows()):\n",
    "            print(f\"   {i+1}. {row['feature']}\")\n",
    "    \n",
    "    # Risk profiles from EDA\n",
    "    print(\"\\nHigh-Risk Borrower Profile:\")\n",
    "    print(\"   â€¢ Low FICO scores (< 680)\")\n",
    "    print(\"   â€¢ High debt-to-income ratio (> 30%)\")\n",
    "    print(\"   â€¢ Short employment history (< 2 years)\")\n",
    "    print(\"   â€¢ High credit utilization (> 80%)\")\n",
    "    print(\"   â€¢ Recent delinquencies or inquiries\")\n",
    "    \n",
    "    print(\"\\nLow-Risk Borrower Profile:\")\n",
    "    print(\"   â€¢ High FICO scores (> 760)\")\n",
    "    print(\"   â€¢ Low debt-to-income ratio (< 15%)\")\n",
    "    print(\"   â€¢ Stable employment (> 5 years)\")\n",
    "    print(\"   â€¢ Home ownership (mortgage or owned)\")\n",
    "    print(\"   â€¢ Clean credit history\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ BUSINESS RECOMMENDATIONS\")\n",
    "    print(\"\\n1. Risk-Based Pricing:\")\n",
    "    print(\"   â€¢ Implement tiered interest rates based on risk scores\")\n",
    "    print(\"   â€¢ Higher rates for high-risk applicants to compensate for increased default risk\")\n",
    "    \n",
    "    print(\"\\n2. Underwriting Guidelines:\")\n",
    "    print(\"   â€¢ Minimum FICO score of 680 for standard loans\")\n",
    "    print(\"   â€¢ Maximum DTI ratio of 40% for loan approval\")\n",
    "    print(\"   â€¢ Require income verification for all applicants\")\n",
    "    print(\"   â€¢ Minimum employment length of 6 months\")\n",
    "    \n",
    "    print(\"\\n3. Portfolio Management:\")\n",
    "    print(\"   â€¢ Diversify loan portfolio by risk grade\")\n",
    "    print(\"   â€¢ Monitor portfolio performance and adjust risk thresholds\")\n",
    "    print(\"   â€¢ Implement early warning systems for at-risk loans\")\n",
    "    \n",
    "    print(\"\\n4. Model Governance:\")\n",
    "    print(\"   â€¢ Regular model retraining (quarterly or when performance degrades)\")\n",
    "    print(\"   â€¢ Monitor for model drift and bias\")\n",
    "    print(\"   â€¢ Maintain model documentation and validation procedures\")\n",
    "    \n",
    "    print(\"\\n5. Future Enhancements:\")\n",
    "    print(\"   â€¢ Incorporate alternative data sources (employment verification, bank statements)\")\n",
    "    print(\"   â€¢ Develop real-time risk monitoring dashboard\")\n",
    "    print(\"   â€¢ Implement automated decision workflows for low-risk applications\")\n",
    "    \n",
    "    # Save the best model\n",
    "    try:\n",
    "        import joblib\n",
    "        model_data = {\n",
    "            'model': best_model_instance,\n",
    "            'scaler': scaler,\n",
    "            'encoders': encoders,\n",
    "            'feature_columns': X_train.columns.tolist(),\n",
    "            'processed_columns': X_train_processed.columns.tolist(),\n",
    "            'metrics': metrics,\n",
    "            'model_type': best_model\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, '../models/best_model.pkl')\n",
    "        print(f\"\\nâœ… Best model saved to '../models/best_model.pkl'\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš ï¸  Could not save model: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"\"\"=\"\"*50)\n",
    "    \n",
    "    print(f\"\\nDate: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Analyst: Rafsamjani Anugrah\")\n",
    "    print(f\"Project: Credit Risk Prediction for ID/X Partners\")\n",
    "    print(f\"Status: Successfully completed with {best_model} model\")\n",
    "else:\n",
    "    print(\"No model results available for final analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}