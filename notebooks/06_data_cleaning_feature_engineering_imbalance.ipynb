{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization with Matplotlib, Pandas, and Seaborn\n",
    "\n",
    "In this notebook, we'll explore various data visualization techniques using matplotlib, pandas, and seaborn for credit risk analysis with the Lending Club dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create sample lending club dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 8000\n",
    "\n",
    "data = {\n",
    "    'loan_amnt': np.random.lognormal(np.log(15000), 0.6, n_samples),\n",
    "    'int_rate': np.random.normal(12, 4, n_samples),\n",
    "    'installment': np.random.normal(400, 200, n_samples),\n",
    "    'annual_inc': np.random.lognormal(np.log(70000), 0.5, n_samples),\n",
    "    'dti': np.random.gamma(2, 7, n_samples),\n",
    "    'fico_score': np.random.normal(690, 70, n_samples),\n",
    "    'emp_length': np.random.gamma(2, 2, n_samples) * 15,\n",
    "    'loan_status': np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),\n",
    "    'grade': pd.cut(np.random.normal(690, 70, n_samples), \n",
    "                    bins=[0, 580, 620, 660, 700, 740, 780, 850], \n",
    "                    labels=['G', 'F', 'E', 'D', 'C', 'B', 'A']),\n",
    "    'home_ownership': np.random.choice(['MORTGAGE', 'OWN', 'RENT'], n_samples, p=[0.45, 0.15, 0.4]),\n",
    "    'verification_status': np.random.choice(['Verified', 'Not Verified', 'Source Verified'], n_samples, p=[0.35, 0.5, 0.15]),\n",
    "    'purpose': np.random.choice(['debt_consolidation', 'credit_card', 'home_improvement', 'major_purchase', \n",
    "                                'small_business', 'other', 'vacation', 'car', 'moving', 'medical'], \n",
    "                               n_samples, p=[0.25, 0.2, 0.15, 0.1, 0.08, 0.07, 0.05, 0.05, 0.03, 0.02]),\n",
    "    'addr_state': np.random.choice(['CA', 'TX', 'NY', 'FL', 'IL', 'OH', 'GA', 'NC', 'MI', 'NJ'], n_samples, p=[0.12, 0.1, 0.09, 0.08, 0.07, 0.06, 0.06, 0.06, 0.05, 0.04])\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Ensure realistic values\n",
    "df['loan_amnt'] = np.clip(df['loan_amnt'], 1000, 40000)\n",
    "df['int_rate'] = np.clip(df['int_rate'], 5, 30)\n",
    "df['fico_score'] = np.clip(df['fico_score'], 300, 850)\n",
    "df['dti'] = np.clip(df['dti'], 0, 100)\n",
    "df['annual_inc'] = np.clip(df['annual_inc'], 10000, 500000)\n",
    "df['emp_length'] = np.clip(df['emp_length'], 0, 15)\n",
    "\n",
    "# Add realistic correlations\n",
    "for i in range(len(df)):\n",
    "    # Lower FICO scores tend to have higher interest rates\n",
    "    if df.loc[i, 'fico_score'] < 600:\n",
    "        df.loc[i, 'int_rate'] = min(30, df.loc[i, 'int_rate'] + np.random.uniform(5, 15))\n",
    "    elif df.loc[i, 'fico_score'] > 750:\n",
    "        df.loc[i, 'int_rate'] = max(5, df.loc[i, 'int_rate'] - np.random.uniform(2, 8))\n",
    "    \n",
    "    # Higher DTI tends to associate with higher default risk\n",
    "    if df.loc[i, 'dti'] > 20 and np.random.random() < 0.3:\n",
    "        df.loc[i, 'loan_status'] = 1\n",
    "\n",
    "# Add engineered features\n",
    "df['loan_to_income_ratio'] = df['loan_amnt'] / (df['annual_inc'] + 1)\n",
    "df['interest_cost'] = df['loan_amnt'] * (df['int_rate'] / 100)\n",
    "df['installment_to_income_ratio'] = df['installment'] / (df['annual_inc'] / 12 + 1)\n",
    "\n",
    "print(\"Data Visualization with Matplotlib, Pandas, and Seaborn\")\n",
    "print(\"Simulated Lending Club Dataset for Credit Risk Analysis\")\n",
    "print(df.head())\n",
    "print(f\"\\nDataset Shape: {df.shape}\")\n",
    "print(f\"Default Rate: {(df['loan_status'] == 1).mean():.2%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data Visualization in Credit Risk Analysis\n",
    "\n",
    "Data visualization plays a crucial role in credit risk analysis. It helps analysts understand patterns, relationships, and outliers in the data. Effective visualizations can reveal important insights such as:\n",
    "\n",
    "1. How different loan characteristics affect default probability\n",
    "2. Relationship between credit score and interest rate\n",
    "3. Differences in default rates across loan grades\n",
    "4. Geographic patterns in credit risk\n",
    "5. Impact of debt-to-income ratio on loan performance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Basic dataset information\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"- Total Records: {len(df):,}\")\n",
    "print(f\"- Default Rate: {(df['loan_status'] == 1).mean():.2%}\")\n",
    "print(f\"- Number of Features: {len(df.columns)}\")\n",
    "print(f\"- Numerical Features: {len(df.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"- Categorical Features: {len(df.select_dtypes(include=['object', 'category']).columns)}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Visualize dataset overview\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Target variable distribution\n",
    "axes[0, 0].pie(df['loan_status'].value_counts().values, \n",
    "                labels=['Fully Paid', 'Charged Off'], \n",
    "                autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 0].set_title('Distribution of Loan Status')\n",
    "\n",
    "# 2. Loan grade distribution\n",
    "grade_counts = df['grade'].value_counts()\n",
    "axes[0, 1].bar(grade_counts.index, grade_counts.values, \n",
    "               color=sns.color_palette(\"Set2\", len(grade_counts)))\n",
    "axes[0, 1].set_title('Distribution of Loan Grades')\n",
    "axes[0, 1].set_xlabel('Grade')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Home ownership distribution\n",
    "home_counts = df['home_ownership'].value_counts()\n",
    "axes[0, 2].barh(home_counts.index, home_counts.values, \n",
    "                color=sns.color_palette(\"husl\", len(home_counts)))\n",
    "axes[0, 2].set_title('Home Ownership Distribution')\n",
    "axes[0, 2].set_xlabel('Count')\n",
    "axes[0, 2].set_ylabel('Home Ownership')\n",
    "\n",
    "# 4. Verification status\n",
    "verif_counts = df['verification_status'].value_counts()\n",
    "axes[1, 0].bar(verif_counts.index, verif_counts.values, \n",
    "               color=sns.color_palette(\"bright\", len(verif_counts)))\n",
    "axes[1, 0].set_title('Income Verification Status')\n",
    "axes[1, 0].set_xlabel('Verification Status')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5. Purpose distribution\n",
    "purpose_counts = df['purpose'].value_counts().head(10)\n",
    "axes[1, 1].barh(range(len(purpose_counts)), purpose_counts.values, \n",
    "                color=sns.color_palette(\"pastel\", len(purpose_counts)))\n",
    "axes[1, 1].set_yticks(range(len(purpose_counts)))\n",
    "axes[1, 1].set_yticklabels(purpose_counts.index)\n",
    "axes[1, 1].set_title('Top 10 Loan Purposes')\n",
    "axes[1, 1].set_xlabel('Count')\n",
    "\n",
    "# 6. State distribution\n",
    "state_counts = df['addr_state'].value_counts().head(10)\n",
    "axes[1, 2].barh(range(len(state_counts)), state_counts.values, \n",
    "                color=sns.color_palette(\"deep\", len(state_counts)))\n",
    "axes[1, 2].set_yticks(range(len(state_counts)))\n",
    "axes[1, 2].set_yticklabels(state_counts.index)\n",
    "axes[1, 2].set_title('Top 10 States by Loan Count')\n",
    "axes[1, 2].set_xlabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib Visualizations\n",
    "\n",
    "Matplotlib is the foundational plotting library in Python. It provides fine-grained control over plots and is highly customizable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Matplotlib Visualizations\n",
    "print(\"Matplotlib Visualizations:\")\n",
    "\n",
    "# 1. Subplot with multiple visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# FICO Score distribution by loan status\n",
    "for status in [0, 1]:\n",
    "    subset = df[df['loan_status'] == status]['fico_score']\n",
    "    axes[0, 0].hist(subset, bins=50, alpha=0.7, \n",
    "                    label=f'Paid' if status == 0 else 'Charged Off',\n",
    "                    color='green' if status == 0 else 'red')\n",
    "axes[0, 0].set_title('FICO Score Distribution by Loan Status')\n",
    "axes[0, 0].set_xlabel('FICO Score')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Interest rate vs FICO score scatter plot\n",
    "colors = ['red' if x == 1 else 'blue' for x in df['loan_status']]\n",
    "axes[0, 1].scatter(df['fico_score'], df['int_rate'], c=colors, alpha=0.6, s=30)\n",
    "axes[0, 1].set_title('Interest Rate vs FICO Score')\n",
    "axes[0, 1].set_xlabel('FICO Score')\n",
    "axes[0, 1].set_ylabel('Interest Rate (%)')\n",
    "\n",
    "# Loan amount by grade boxplot\n",
    "loan_by_grade = [df[df['grade'] == grade]['loan_amnt'] for grade in sorted(df['grade'].unique())]\n",
    "axes[1, 0].boxplot(loan_by_grade, labels=sorted(df['grade'].unique()))\n",
    "axes[1, 0].set_title('Loan Amount Distribution by Grade')\n",
    "axes[1, 0].set_xlabel('Grade')\n",
    "axes[1, 0].set_ylabel('Loan Amount')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Default rate by grade\n",
    "default_by_grade = df.groupby('grade')['loan_status'].mean()\n",
    "axes[1, 1].bar(default_by_grade.index, default_by_grade.values, \n",
    "               color=sns.color_palette(\"Reds\", len(default_by_grade)))\n",
    "axes[1, 1].set_title('Default Rate by Grade')\n",
    "axes[1, 1].set_xlabel('Grade')\n",
    "axes[1, 1].set_ylabel('Default Rate')\n",
    "for i, v in enumerate(default_by_grade.values):\n",
    "    axes[1, 1].text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Advanced matplotlib visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Violin plot for FICO score by grade\n",
    "axes[0, 0].violinplot([df[df['grade'] == grade]['fico_score'] for grade in sorted(df['grade'].unique())],\n",
    "                    positions=range(len(sorted(df['grade'].unique()))),\n",
    "                    showmeans=True)\n",
    "axes[0, 0].set_title('FICO Score Distribution by Grade (Violin Plot)')\n",
    "axes[0, 0].set_xticks(range(len(sorted(df['grade'].unique()))))\n",
    "axes[0, 0].set_xticklabels(sorted(df['grade'].unique()))\n",
    "axes[0, 0].set_xlabel('Grade')\n",
    "axes[0, 0].set_ylabel('FICO Score')\n",
    "\n",
    "# 3D scatter plot (interest rate, FICO score, loan amount)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "ax = fig.add_subplot(2, 3, 2, projection='3d')\n",
    "scatter = ax.scatter(df['fico_score'][:500], df['int_rate'][:500], df['loan_amnt'][:500], \n",
    "                     c=df['loan_status'][:500], cmap='viridis', alpha=0.7)\n",
    "ax.set_xlabel('FICO Score')\n",
    "ax.set_ylabel('Interest Rate')\n",
    "ax.set_zlabel('Loan Amount')\n",
    "ax.set_title('3D Visualization: FICO vs Interest vs Loan Amount')\n",
    "plt.colorbar(scatter, ax=ax, shrink=0.5)\n",
    "\n",
    "# Heatmap of correlations\n",
    "numeric_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score', 'emp_length']\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "im = axes[0, 2].imshow(corr_matrix, cmap='coolwarm', aspect='auto')\n",
    "axes[0, 2].set_xticks(range(len(corr_matrix.columns)))\n",
    "axes[0, 2].set_yticks(range(len(corr_matrix.columns)))\n",
    "axes[0, 2].set_xticklabels(corr_matrix.columns, rotation=45, ha='right')\n",
    "axes[0, 2].set_yticklabels(corr_matrix.columns)\n",
    "axes[0, 2].set_title('Correlation Heatmap')\n",
    "# Add correlation values\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(len(corr_matrix.columns)):\n",
    "        text = axes[0, 2].text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
    "                               ha='center', va='center', color='black' if abs(corr_matrix.iloc[i, j]) < 0.5 else 'white')\n",
    "plt.colorbar(im, ax=axes[0, 2])\n",
    "\n",
    "# Hexbin plot of FICO score vs Interest rate\n",
    "hb = axes[1, 0].hexbin(df['fico_score'], df['int_rate'], gridsize=30, cmap='Blues')\n",
    "axes[1, 0].set_title('FICO Score vs Interest Rate (Hexbin)')\n",
    "axes[1, 0].set_xlabel('FICO Score')\n",
    "axes[1, 0].set_ylabel('Interest Rate')\n",
    "plt.colorbar(hb, ax=axes[1, 0])\n",
    "\n",
    "# Advanced distribution comparison with KDE\n",
    "axes[1, 1].hist(df[df['loan_status'] == 0]['fico_score'], bins=50, density=True, \n",
    "                alpha=0.7, label='Paid', color='blue')\n",
    "axes[1, 1].hist(df[df['loan_status'] == 1]['fico_score'], bins=50, density=True, \n",
    "                alpha=0.7, label='Charged Off', color='red')\n",
    "# Add Kernel Density Estimation\n",
    "from scipy.stats import gaussian_kde\n",
    "paid_density = gaussian_kde(df[df['loan_status'] == 0]['fico_score'])\n",
    "charged_off_density = gaussian_kde(df[df['loan_status'] == 1]['fico_score'])\n",
    "x_range = np.linspace(df['fico_score'].min(), df['fico_score'].max(), 1000)\n",
    "axes[1, 1].plot(x_range, paid_density(x_range), label='Paid KDE', color='blue', linewidth=2)\n",
    "axes[1, 1].plot(x_range, charged_off_density(x_range), label='Charged Off KDE', color='red', linewidth=2)\n",
    "axes[1, 1].set_title('FICO Score Distribution with KDE')\n",
    "axes[1, 1].set_xlabel('FICO Score')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "# Time series-like visualization\n",
    "df_sorted = df.sort_values('fico_score')\n",
    "axes[1, 2].plot(df_sorted['fico_score'], df_sorted['int_rate'], alpha=0.3, color='gray')\n",
    "# Add rolling average\n",
    "df_sorted['int_rate_rolling'] = df_sorted['int_rate'].rolling(window=100).mean()\n",
    "axes[1, 2].plot(df_sorted['fico_score'], df_sorted['int_rate_rolling'], color='red', linewidth=2, label='100-point rolling average')\n",
    "axes[1, 2].set_title('Interest Rate Trend with FICO Score')\n",
    "axes[1, 2].set_xlabel('FICO Score')\n",
    "axes[1, 2].set_ylabel('Interest Rate (%)')\n",
    "axes[1, 2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pie chart with customizations\n",
    "plt.figure(figsize=(12, 8))\n",
    "default_rates = df.groupby('grade')['loan_status'].mean()\n",
    "colors = plt.cm.RdYlGn(np.linspace(0, 1, len(default_rates)))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.pie(default_rates.values, labels=default_rates.index, autopct='%1.1f%%', \n",
    "        colors=colors, startangle=90, explode=[0.1 if rate > 0.2 else 0 for rate in default_rates.values])\n",
    "plt.title('Default Rate by Grade')\n",
    "\n",
    "# Bar plot with error bars\n",
    "plt.subplot(2, 2, 2)\n",
    "grade_defaults = df.groupby('grade')['loan_status'].agg(['mean', 'count', 'std'])\n",
    "grade_defaults['se'] = grade_defaults['std'] / np.sqrt(grade_defaults['count'])  # Standard error\n",
    "\n",
    "plt.bar(grade_defaults.index, grade_defaults['mean'], yerr=grade_defaults['se'],\n",
    "        capsize=5, color=sns.color_palette(\"Set1\", len(grade_defaults)), alpha=0.7)\n",
    "plt.title('Default Rate by Grade with Error Bars')\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Default Rate')\n",
    "\n",
    "# Horizontal bar chart\n",
    "plt.subplot(2, 2, 3)\n",
    "purpose_defaults = df.groupby('purpose')['loan_status'].mean().sort_values(ascending=False)\n",
    "plt.barh(range(len(purpose_defaults)), purpose_defaults.values, \n",
    "         color=sns.color_palette(\"husl\", len(purpose_defaults)))\n",
    "plt.yticks(range(len(purpose_defaults)), purpose_defaults.index)\n",
    "plt.title('Default Rate by Purpose')\n",
    "plt.xlabel('Default Rate')\n",
    "\n",
    "# Step chart\n",
    "plt.subplot(2, 2, 4)\n",
    "fico_bins = pd.cut(df['fico_score'], bins=np.arange(550, 850, 20))\n",
    "fico_default_rates = df.groupby(fico_bins)['loan_status'].mean()\n",
    "plt.step(range(len(fico_default_rates)), fico_default_rates.values, where='mid', linewidth=2)\n",
    "plt.fill_between(range(len(fico_default_rates)), fico_default_rates.values, alpha=0.3)\n",
    "plt.title('Default Rate by FICO Score Range')\n",
    "plt.xlabel('FICO Score Range')\n",
    "plt.ylabel('Default Rate')\n",
    "plt.xticks(range(0, len(fico_default_rates), max(1, len(fico_default_rates)//3)),\n",
    "          [str(interval) for interval in fico_default_rates.index[::max(1, len(fico_default_rates)//3)]],\n",
    "          rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Visualization\n",
    "\n",
    "Pandas provides convenient plotting methods directly on DataFrames and Series, making quick visualizations very efficient."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pandas Visualizations\n",
    "print(\"Pandas Visualizations:\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Histogram using pandas\n",
    "df['fico_score'].hist(bins=50, ax=axes[0,0], color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0,0].set_title('FICO Score Distribution (Pandas Hist)')\n",
    "axes[0,0].set_xlabel('FICO Score')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "\n",
    "# 2. Boxplot using pandas\n",
    "df.boxplot(column='int_rate', by='grade', ax=axes[0,1])\n",
    "axes[0,1].set_title('Interest Rate by Grade (Pandas Boxplot)')\n",
    "axes[0,1].set_xlabel('Grade')\n",
    "axes[0,1].set_ylabel('Interest Rate (%)')\n",
    "\n",
    "# 3. Scatter plot using pandas\n",
    "df.plot.scatter(x='fico_score', y='int_rate', c='loan_status', \n",
    "                colormap='viridis', alpha=0.7, ax=axes[0,2])\n",
    "axes[0,2].set_title('FICO vs Interest Rate Colored by Status (Pandas Scatter)')\n",
    "\n",
    "# 4. Bar plot using pandas\n",
    "grade_defaults = df.groupby('grade')['loan_status'].mean()\n",
    "grade_defaults.plot(kind='bar', ax=axes[1,0], color=sns.color_palette(\"husl\", len(grade_defaults)))\n",
    "axes[1,0].set_title('Default Rate by Grade (Pandas Bar)')\n",
    "axes[1,0].set_xlabel('Grade')\n",
    "axes[1,0].set_ylabel('Default Rate')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5. Line plot using pandas\n",
    "# Create a time-series like visualization (ordered by FICO)\n",
    "df_sorted = df.sort_values('fico_score')\n",
    "df_sorted['rolling_default_rate'] = df_sorted['loan_status'].rolling(window=100).mean()\n",
    "df_sorted['rolling_default_rate'].plot(ax=axes[1,1], title='Rolling Default Rate Over FICO Score Order', color='red')\n",
    "axes[1,1].set_xlabel('Sorted Index')\n",
    "axes[1,1].set_ylabel('Default Rate')\n",
    "\n",
    "# 6. Area plot using pandas\n",
    "# For area plot, we'll create a grouped view\n",
    "purposes_top = df['purpose'].value_counts().head(5).index\n",
    "purpose_dti = df[df['purpose'].isin(purposes_top)].groupby(['purpose'])['dti'].mean().sort_values(ascending=True)\n",
    "purpose_dti.plot(kind='barh', ax=axes[1,2], color=sns.color_palette(\"Set3\", len(purpose_dti)))\n",
    "axes[1,2].set_title('Average DTI by Top 5 Purposes (Pandas Barh)')\n",
    "axes[1,2].set_xlabel('Average DTI')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional pandas visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Density plot using pandas\n",
    "df['fico_score'].plot.density(ax=axes[0,0], title='FICO Score Density (Pandas Density)')\n",
    "axes[0,0].set_xlabel('FICO Score')\n",
    "axes[0,0].set_ylabel('Density')\n",
    "\n",
    "# 2. Hexbin plot using pandas\n",
    "# Create hexbin plot of FICO score vs Interest rate\n",
    "df.plot.hexbin(x='fico_score', y='int_rate', gridsize=25, ax=axes[0,1], title='FICO vs Interest Rate Hexbin (Pandas)')\n",
    "\n",
    "# 3. Pie chart using pandas\n",
    "home_ownership_counts = df['home_ownership'].value_counts()\n",
    "home_ownership_counts.plot(kind='pie', ax=axes[0,2], title='Home Ownership Distribution (Pandas Pie)',\n",
    "                           autopct='%1.1f%%', startangle=90)\n",
    "axes[0,2].set_ylabel('')  # Remove default ylabel\n",
    "\n",
    "# 4. Multiple column visualization\n",
    "numeric_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti']\n",
    "# Normalize for comparison\n",
    "df_normalized = df[numeric_cols].copy()\n",
    "for col in numeric_cols:\n",
    "    df_normalized[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n",
    "\n",
    "# Plot multiple series\n",
    "df_normalized.head(100).plot(ax=axes[1,0], title='Normalized Key Features for First 100 Loans (Pandas Multi-line)')\n",
    "axes[1,0].set_ylabel('Normalized Value')\n",
    "\n",
    "# 5. Parallel coordinates (conceptual - pandas doesn't have direct parallel coords)\n",
    "# Creating a simplified version\n",
    "selected_data = df[['fico_score', 'int_rate', 'dti', 'loan_status']].head(50)\n",
    "selected_data['loan_status'] = selected_data['loan_status'].astype(str)  # Convert to string for different color\n",
    "\n",
    "# Use pandas to create a custom parallel coordinates visualization\n",
    "from pandas.plotting import parallel_coordinates\n",
    "# We'll discretize the status to create groups\n",
    "df_sample = df[['fico_score', 'int_rate', 'dti', 'loan_status']].head(100).copy()\n",
    "df_sample['status_group'] = df_sample['loan_status'].astype(str)\n",
    "\n",
    "# Normalize the data\n",
    "df_plot = df_sample[['fico_score', 'int_rate', 'dti']].copy()\n",
    "for col in df_plot.columns:\n",
    "    df_plot[col] = (df_plot[col] - df_plot[col].min()) / (df_plot[col].max() - df_plot[col].min())\n",
    "df_plot['status_group'] = df_sample['status_group']\n",
    "\n",
    "# Parallel coordinates plot\n",
    "axes[1,1].clear()  # Clear the axis\n",
    "for i, status in df_plot['status_group'].unique():\n",
    "    subset = df_plot[df_plot['status_group'] == status]\n",
    "    for idx in subset.index:\n",
    "        axes[1,1].plot(range(len(subset.columns[:-1])), subset.loc[idx, subset.columns[:-1]].values, \n",
    "                       alpha=0.5, label=f'Status {status}' if idx == subset.index[0] else \"\")\n",
    "axes[1,1].set_title('Parallel Coordinates (Custom Pandas Implementation)')\n",
    "axes[1,1].set_xticks(range(len(subset.columns[:-1])))\n",
    "axes[1,1].set_xticklabels(subset.columns[:-1], rotation=45)\n",
    "axes[1,1].set_ylabel('Normalized Value')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pandas plotting with subgroups\n",
    "print(f\"\\nPandas Visualization: Default Rate by Multiple Categories\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Default rate by grade and verification status\n",
    "cross_tab = pd.crosstab(df['grade'], df['verification_status'], values=df['loan_status'], aggfunc=np.mean)\n",
    "cross_tab.plot(kind='bar', ax=axes[0,0], title='Default Rate by Grade and Verification Status')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "axes[0,0].set_ylabel('Default Rate')\n",
    "axes[0,0].legend(title='Verification Status')\n",
    "\n",
    "# 2. Default rate by home ownership and loan purpose\n",
    "purpose_defaults = df.groupby(['home_ownership', 'purpose'])['loan_status'].mean().unstack(fill_value=0)\n",
    "purpose_defaults.plot(kind='bar', ax=axes[0,1], title='Default Rate by Home Ownership and Purpose')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].set_ylabel('Default Rate')\n",
    "axes[0,1].legend(title='Loan Purpose', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# 3. Loan amount distribution by grade and loan status\n",
    "df.boxplot(column='loan_amnt', by=['grade', 'loan_status'], ax=axes[1,0])\n",
    "axes[1,0].set_title('Loan Amount by Grade and Status (Pandas Boxplot Groupby)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. FICO score vs interest rate by loan purpose\n",
    "for purpose in df['purpose'].value_counts().head(3).index:\n",
    "    purpose_data = df[df['purpose'] == purpose]\n",
    "    axes[1,1].scatter(purpose_data['fico_score'], purpose_data['int_rate'], \n",
    "                     label=purpose, alpha=0.6, s=30)\n",
    "axes[1,1].set_title('FICO vs Interest Rate by Purpose (Pandas Scatter with Groups)')\n",
    "axes[1,1].set_xlabel('FICO Score')\n",
    "axes[1,1].set_ylabel('Interest Rate')\n",
    "axes[1,1].legend(title='Loan Purpose')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seaborn Visualizations\n",
    "\n",
    "Seaborn is built on top of matplotlib and provides a high-level interface for statistical graphics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Seaborn Visualizations\n",
    "print(\"Seaborn Visualizations:\")\n",
    "\n",
    "# 1. Distribution plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Histogram with KDE (Distribution plot)\n",
    "sns.histplot(data=df, x='fico_score', hue='loan_status', kde=True, ax=axes[0,0])\n",
    "axes[0,0].set_title('FICO Score Distribution by Loan Status')\n",
    "\n",
    "# Rug plot overlaid on histogram\n",
    "sns.rugplot(data=df, x='int_rate', height=0.05, ax=axes[0,1])\n",
    "sns.histplot(data=df, x='int_rate', hue='loan_status', alpha=0.7, ax=axes[0,1])\n",
    "axes[0,1].set_title('Interest Rate Distribution with Rug Plot')\n",
    "\n",
    "# KDE plot with fill\n",
    "loan_status_names = {0: 'Paid', 1: 'Charged Off'}\n",
    "df['status_name'] = df['loan_status'].map(loan_status_names)\n",
    "sns.kdeplot(data=df, x='dti', hue='status_name', fill=True, ax=axes[0,2])\n",
    "axes[0,2].set_title('DTI Distribution by Loan Status')\n",
    "\n",
    "# 2. Scatter plots\n",
    "sns.scatterplot(data=df, x='fico_score', y='int_rate', hue='loan_status', alpha=0.7, ax=axes[1,0])\n",
    "axes[1,0].set_title('FICO Score vs Interest Rate')\n",
    "\n",
    "# Scatter plot with size encoding\n",
    "sns.scatterplot(data=df, x='annual_inc', y='loan_amnt', size='dti', hue='loan_status', \n",
    "                sizes=(20, 200), alpha=0.6, ax=axes[1,1])\n",
    "axes[1,1].set_title('Annual Income vs Loan Amount (Size = DTI)')\n",
    "axes[1,1].legend(loc='upper right', bbox_to_anchor=(1.3, 1))\n",
    "\n",
    "# Regplot with confidence interval\n",
    "sns.regplot(data=df, x='fico_score', y='int_rate', scatter_kws={'alpha':0.3}, ax=axes[1,2])\n",
    "axes[1,2].set_title('FICO Score vs Interest Rate (with Regression Line)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Categorical plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Box plot\n",
    "sns.boxplot(data=df, x='grade', y='int_rate', hue='loan_status', ax=axes[0,0])\n",
    "axes[0,0].set_title('Interest Rate by Grade and Status')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Violin plot\n",
    "sns.violinplot(data=df, x='home_ownership', y='fico_score', ax=axes[0,1])\n",
    "axes[0,1].set_title('FICO Score Distribution by Home Ownership')\n",
    "\n",
    "# Bar plot\n",
    "sns.barplot(data=df, x='grade', y='loan_status', ax=axes[0,2])\n",
    "axes[0,2].set_title('Default Rate by Grade')\n",
    "axes[0,2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(data=df, x='purpose', ax=axes[1,0])\n",
    "axes[1,0].set_title('Loan Count by Purpose')\n",
    "axes[1,0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Point plot\n",
    "grade_purpose = df.groupby(['grade', 'purpose']).agg({'loan_status': 'mean'}).reset_index()\n",
    "top_purposes = df['purpose'].value_counts().head(5).index\n",
    "grade_purpose_filtered = grade_purpose[grade_purpose['purpose'].isin(top_purposes)]\n",
    "sns.pointplot(data=grade_purpose_filtered, x='grade', y='loan_status', hue='purpose', ax=axes[1,1])\n",
    "axes[1,1].set_title('Default Rate by Grade and Purpose')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "axes[1,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Strip plot\n",
    "top_states = df['addr_state'].value_counts().head(5).index\n",
    "df_top_states = df[df['addr_state'].isin(top_states)]\n",
    "sns.stripplot(data=df_top_states, x='addr_state', y='int_rate', hue='loan_status', dodge=True, ax=axes[1,2])\n",
    "axes[1,2].set_title('Interest Rate by State and Status')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Relationship plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Pairplot for selected variables (using a sample for performance)\n",
    "sample_df = df[['fico_score', 'int_rate', 'annual_inc', 'dti', 'loan_status']].sample(n=500, random_state=42)\n",
    "sns.scatterplot(data=sample_df, x='fico_score', y='int_rate', hue='loan_status', size='annual_inc', \n",
    "                sizes=(20, 200), alpha=0.7, ax=axes[0])\n",
    "axes[0].set_title('FICO Score vs Interest Rate (Size = Annual Income)')\n",
    "\n",
    "# Joint plot\n",
    "sns.scatterplot(data=sample_df, x='dti', y='int_rate', hue='loan_status', alpha=0.6, ax=axes[1])\n",
    "axes[1].set_title('DTI vs Interest Rate by Status')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Matrix plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Heatmap of correlations\n",
    "numeric_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'dti', 'fico_score', 'emp_length']\n",
    "corr_matrix = df[numeric_cols].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True, fmt='.2f', ax=axes[0])\n",
    "axes[0].set_title('Correlation Matrix of Numeric Features')\n",
    "\n",
    "# Clustermap for hierarchical clustering of correlations\n",
    "try:\n",
    "    sns.clustermap(corr_matrix, annot=True, cmap='coolwarm', center=0, square=True, fmt='.2f', ax=axes[1])\n",
    "    axes[1].set_title('Clustered Correlation Matrix')\n",
    "except:\n",
    "    axes[1].text(0.5, 0.5, 'Clustermap failed due to correlation computation', \n",
    "                 horizontalalignment='center', verticalalignment='center', transform=axes[1].transAxes)\n",
    "    axes[1].set_title('Clustered Correlation Matrix')\n",
    "\n",
    "# Pivot table heatmap\n",
    "pivot_table = df.groupby(['grade', 'home_ownership'])['loan_status'].mean().unstack(fill_value=0)\n",
    "sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlOrRd', ax=axes[2])\n",
    "axes[2].set_title('Default Rate by Grade and Home Ownership')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Advanced Seaborn visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# FacetGrid for grade-wise analysis\n",
    "g = sns.FacetGrid(df.sample(2000), col='grade', height=4, aspect=0.8)  # Sample to improve performance\n",
    "g.map(plt.scatter, 'fico_score', 'int_rate', alpha=0.6)\n",
    "g.set_axis_labels('FICO Score', 'Interest Rate')\n",
    "g.set_titles('Grade {col_name}')\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle('FICO Score vs Interest Rate by Grade')\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))  # Create new figure for the remaining plots\n",
    "\n",
    "# Distribution by category\n",
    "sns.boxplot(data=df, x='purpose', y='loan_amnt', ax=axes[0,0])\n",
    "axes[0,0].set_title('Loan Amount by Purpose')\n",
    "axes[0,0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Regression plot with polynomial fit\n",
    "from scipy import stats\n",
    "sns.regplot(data=df, x='fico_score', y='int_rate', order=2, ax=axes[0,1])\n",
    "axes[0,1].set_title('FICO Score vs Interest Rate (Polynomial Fit)')\n",
    "\n",
    "# Swarm plot\n",
    "df_small = df.groupby('grade').apply(lambda x: x.sample(min(len(x), 100))).reset_index(drop=True)  # Limit points for swarm\n",
    "sns.swarmplot(data=df_small, x='grade', y='fico_score', hue='loan_status', ax=axes[1,0])\n",
    "axes[1,0].set_title('FICO Score by Grade (Swarm Plot)')\n",
    "\n",
    "# Bar plot with error bars\n",
    "grade_stats = df.groupby('grade').agg({'loan_status': ['mean', 'std', 'count']}).round(3)\n",
    "grade_stats.columns = ['mean_default', 'std_default', 'count']\n",
    "grade_stats['se'] = grade_stats['std_default'] / np.sqrt(grade_stats['count'])\n",
    "\n",
    "sns.barplot(data=df, x='grade', y='loan_status', ax=axes[1,1], errorbar=('ci', 95))\n",
    "axes[1,1].set_title('Default Rate by Grade with Confidence Interval')\n",
    "axes[1,1].set_ylabel('Default Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering & Data Cleaning\n",
    "\n",
    "In this section, we'll implement data cleaning techniques, feature engineering, and handle imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Feature Engineering & Data Cleaning\n",
    "print(\"Feature Engineering & Data Cleaning:\")\n",
    "\n",
    "# Create a copy for processing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# 1. Data Cleaning\n",
    "print(f\"\\n1. Data Cleaning\")\n",
    "print(f\"Shape before cleaning: {df_processed.shape}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_before = df_processed.isnull().sum()\n",
    "print(f\"Missing values before cleaning:\\n{missing_before[missing_before > 0]}\")\n",
    "\n",
    "# Handle missing values\n",
    "# For this example, we'll fill missing numerical values with median, and categorical with mode\n",
    "numerical_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Fill numerical missing values with median\n",
    "for col in numerical_cols:\n",
    "    if df_processed[col].isnull().any():\n",
    "        df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "\n",
    "# Fill categorical missing values with mode\n",
    "for col in categorical_cols:\n",
    "    if df_processed[col].isnull().any():\n",
    "        df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "\n",
    "missing_after = df_processed.isnull().sum()\n",
    "print(f\"Missing values after cleaning:\\n{missing_after[missing_after > 0]}\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates_before = df_processed.duplicated().sum()\n",
    "print(f\"Duplicates before cleaning: {duplicates_before}\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_processed = df_processed.drop_duplicates()\n",
    "print(f\"Duplicates after cleaning: {df_processed.duplicated().sum()}\")\n",
    "\n",
    "# 2. Outlier Detection and Treatment\n",
    "print(f\"\\n2. Outlier Detection and Treatment\")\n",
    "from scipy import stats\n",
    "\n",
    "# Using Z-score method to identify outliers\n",
    "outliers_info = {}\n",
    "for col in ['loan_amnt', 'int_rate', 'annual_inc', 'fico_score', 'dti']:\n",
    "    z_scores = np.abs(stats.zscore(df_processed[col]))\n",
    "    outliers_count = (z_scores > 3).sum()\n",
    "    outliers_info[col] = outliers_count\n",
    "    print(f\"Outliers in {col} (Z-score > 3): {outliers_count} ({outliers_count/len(df_processed)*100:.2f}%)\")\n",
    "\n",
    "# Apply Winsorization to handle outliers\n",
    "from scipy.stats import mstats\n",
    "winsorized_cols = ['loan_amnt', 'annual_inc', 'dti']\n",
    "\n",
    "for col in winsorized_cols:\n",
    "    df_processed[f'{col}_winsorized'] = mstats.winsorize(df_processed[col], limits=[0.05, 0.05])\n",
    "\n",
    "print(f\"Applied winsorization to {winsorized_cols} (5% on each tail)\")\n",
    "\n",
    "# 3. Feature Engineering\n",
    "print(f\"\\n3. Feature Engineering\")\n",
    "\n",
    "# Create new features based on domain knowledge\n",
    "df_processed['loan_to_income_ratio'] = df_processed['loan_amnt'] / (df_processed['annual_inc'] + 1)\n",
    "df_processed['interest_cost'] = df_processed['loan_amnt'] * (df_processed['int_rate'] / 100)\n",
    "df_processed['installment_to_income_ratio'] = df_processed['installment'] / (df_processed['annual_inc'] / 12 + 1)\n",
    "df_processed['debt_to_income_ratio'] = df_processed['dti']  # Already calculated\n",
    "df_processed['fico_grade_interaction'] = df_processed['fico_score'] * (7 - df_processed['grade'].cat.codes)\n",
    "df_processed['credit_utilization_rate'] = df_processed['loan_amnt'] / (df_processed['annual_inc'] / 12)  # Simplified credit utilization\n",
    "\n",
    "# Binning continuous variables\n",
    "df_processed['fico_score_bin'] = pd.cut(df_processed['fico_score'], bins=5, labels=['Very Poor', 'Poor', 'Fair', 'Good', 'Excellent'])\n",
    "df_processed['loan_amnt_bin'] = pd.qcut(df_processed['loan_amnt'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "df_processed['dti_bin'] = pd.cut(df_processed['dti'], bins=[0, 10, 20, 30, float('inf')], labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    # Encoding categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoders = {}\n",
    "\n",
    "categorical_for_encoding = ['grade', 'home_ownership', 'verification_status', 'purpose', 'addr_state', \n",
    "                           'fico_score_bin', 'loan_amnt_bin', 'dti_bin']\n",
    "\n",
    "for col in categorical_for_encoding:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[f'{col}_encoded'] = le.fit_transform(df_processed[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"Created {len([col for col in df_processed.columns if col.endswith('_encoded')])} new encoded features\")\n",
    "print(f\"Created {len([col for col in df_processed.columns if col.endswith('_bin')])} new binned features\")\n",
    "print(f\"Created 6 new engineered ratio features\")\n",
    "\n",
    "# 4. Handling Imbalanced Data\n",
    "print(f\"\\n4. Handling Imbalanced Data\")\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "target_distribution = df_processed['loan_status'].value_counts()\n",
    "print(f\"Original target distribution:\")\n",
    "print(target_distribution)\n",
    "print(f\"Default rate: {df_processed['loan_status'].mean():.2%}\")\n",
    "\n",
    "# Visualize the imbalance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].pie(target_distribution.values, labels=['Paid', 'Charged Off'], autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Original Distribution of Loan Status')\n",
    "\n",
    "axes[1].bar(['Paid', 'Charged Off'], target_distribution.values, \n",
    "           color=['lightgreen', 'lightcoral'], alpha=0.7)\n",
    "axes[1].set_title('Original Count of Loan Status')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "for i, v in enumerate(target_distribution.values):\n",
    "    axes[1].text(i, v + max(target_distribution.values)*0.01, str(v), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare features for imbalance handling\n",
    "feature_cols = [col for col in df_processed.columns if col not in ['loan_status', 'status_name']]\n",
    "X = df_processed[feature_cols]\n",
    "y = df_processed['loan_status']\n",
    "\n",
    "# Apply different imbalance handling techniques\n",
    "print(f\"\\nApplying Imbalance Handling Techniques:\")\n",
    "\n",
    "# Original dataset\n",
    "print(f\"Original dataset: {X.shape[0]} samples, default rate: {y.mean():.2%}\")\n",
    "\n",
    "# 1. Random Oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_ros, y_ros = ros.fit_resample(X, y)\n",
    "print(f\"Random Oversampling: {X_ros.shape[0]} samples, default rate: {y_ros.mean():.2%}\")\n",
    "\n",
    "# 2. SMOTE (Synthetic Minority Oversampling Technique)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "print(f\"SMOTE: {X_smote.shape[0]} samples, default rate: {y_smote.mean():.2%}\")\n",
    "\n",
    "# 3. ADASYN (Adaptive Synthetic Sampling)\n",
    "adasyn = ADASYN(random_state=42, n_neighbors=5)\n",
    "X_adasyn, y_adasyn = adasyn.fit_resample(X, y)\n",
    "print(f\"ADASYN: {X_adasyn.shape[0]} samples, default rate: {y_adasyn.mean():.2%}\")\n",
    "\n",
    "# 4. Random Undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_rus, y_rus = rus.fit_resample(X, y)\n",
    "print(f\"Random Undersampling: {X_rus.shape[0]} samples, default rate: {y_rus.mean():.2%}\")\n",
    "\n",
    "# Visualize the different sampling techniques\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "datasets = [\n",
    "    (y, 'Original'),\n",
    "    (y_ros, 'Random Oversampling'),\n",
    "    (y_smote, 'SMOTE'),\n",
    "    (y_adasyn, 'ADASYN'),\n",
    "    (y_rus, 'Random Undersampling')\n",
    "]\n",
    "\n",
    "for i, (target, name) in enumerate(datasets):\n",
    "    if i >= 6:\n",
    "        break\n",
    "    dist = target.value_counts()\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    \n",
    "    axes[row, col].pie(dist.values, labels=['Paid', 'Charged Off'], autopct='%1.1f%%', startangle=90)\n",
    "    axes[row, col].set_title(f'{name}\\n(Default Rate: {target.mean():.2%})')\n",
    "\n",
    "# Hide the extra subplot\n",
    "axes[1, 2].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Feature Selection\n",
    "print(f\"\\n5. Feature Selection\")\n",
    "\n",
    "# Select top features using statistical tests\n",
    "selector = SelectKBest(score_func=f_classif, k=15)  # Select top 15 features\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "selected_features = X.columns[selector.get_support()].tolist()\n",
    "\n",
    "print(f\"Selected top 15 features:\")\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    score = selector.scores_[selector.get_support()][i-1]\n",
    "    print(f\"  {i}. {feature} (Score: {score:.2f})\")\n",
    "\n",
    "# Prepare the final dataset with the most important features\n",
    "X_final = X[selected_features]\n",
    "\n",
    "# 6. Class Weight Balancing\n",
    "print(f\"\\n6. Alternative to Sampling: Class Weight Balancing\")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "weight_dict = dict(zip(np.unique(y), class_weights))\n",
    "\n",
    "print(f\"Computed class weights: {weight_dict}\")\n",
    "\n",
    "# Store the processed data for later use\n",
    "df_cleaned = df_processed.copy()\n",
    "X_cleaned = X_final.copy()\n",
    "y_cleaned = y.copy()\n",
    "\n",
    "print(f\"\\nData processing completed!\")\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Cleaned shape: {df_cleaned.shape}\")\n",
    "print(f\"Features after selection: {X_cleaned.shape[1]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training with Different Imbalance Handling Strategies\n",
    "\n",
    "Now we'll train models using different strategies to handle the imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Model Training with Different Imbalance Handling Strategies\n",
    "print(\"Model Training with Different Imbalance Handling Strategies\")\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_cleaned, y_cleaned, test_size=0.2, random_state=42, stratify=y_cleaned\n",
    ")\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Define sampling strategies\n",
    "sampling_strategies = {\n",
    "    'Original': (X_train, y_train),\n",
    "    'SMOTE': (X_smote, y_smote),\n",
    "    'Random Oversampling': (X_ros, y_ros),\n",
    "    'Random Undersampling': (X_rus, y_rus)\n",
    "}\n",
    "\n",
    "# Train and evaluate models with different strategies\n",
    "results = {}\n",
    "\n",
    "for strategy_name, (X_strat, y_strat) in sampling_strategies.items():\n",
    "    print(f\"\\nTraining with {strategy_name} strategy:\")\n",
    "    print(f\"  Dataset shape: {X_strat.shape}, Default rate: {y_strat.mean():.2%}\")\n",
    "    \n",
    "    strategy_results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_strat, y_strat)\n",
    "        \n",
    "        # Make predictions on test set\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "        }\n",
    "        \n",
    "        strategy_results[model_name] = metrics\n",
    "        print(f\"  {model_name}: Accuracy={metrics['accuracy']:.3f}, Precision={metrics['precision']:.3f}, \", end=\"\")\n",
    "        print(f\"Recall={metrics['recall']:.3f}, F1={metrics['f1']:.3f}, ROC-AUC={metrics['roc_auc']:.3f}\")\n",
    "    \n",
    "    results[strategy_name] = strategy_results\n",
    "\n",
    "# Transform results to DataFrame for visualization\n",
    "model_comparison = []\n",
    "for strategy, model_results in results.items():\n",
    "    for model, metrics in model_results.items():\n",
    "        for metric, value in metrics.items():\n",
    "            model_comparison.append({\n",
    "                'Strategy': strategy,\n",
    "                'Model': model,\n",
    "                'Metric': metric,\n",
    "                'Value': value\n",
    "            })\n",
    "\n",
    "comparison_df = pd.DataFrame(model_comparison)\n",
    "\n",
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "for idx, (metric, metric_name) in enumerate(zip(metrics_to_plot, metric_names)):  \n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    \n",
    "    metric_data = comparison_df[comparison_df['Metric'] == metric]\n",
    "    \n",
    "    sns.barplot(data=metric_data, x='Strategy', y='Value', hue='Model', ax=axes[row, col])\n",
    "    axes[row, col].set_title(f'{metric_name} Comparison')\n",
    "    axes[row, col].set_ylabel(metric_name)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    axes[row, col].legend(title='Model')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curves for different strategies\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for model_idx, (model_name, model) in enumerate(models.items()):\n",
    "    ax = axes[model_idx]\n",
    "    ax.set_title(f'ROC Curves - {model_name}')\n",
    "    \n",
    "    for strategy_name, (X_strat, y_strat) in sampling_strategies.items():\n",
    "        # Retrain the model with the specific strategy\n",
    "        model_temp = type(model)(**model.get_params())  # Create a new instance\n",
    "        model_temp.fit(X_strat, y_strat)\n",
    "        \n",
    "        y_pred_proba = model_temp.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        ax.plot(fpr, tpr, label=f'{strategy_name} (AUC = {auc:.3f})')\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of findings\n",
    "print(f\"\\nKey Findings from Imbalance Handling Experiment:\")\n",
    "\n",
    "# Calculate average performance for each strategy\n",
    "avg_performance = comparison_df.groupby(['Strategy', 'Model'])['Value'].mean().reset_index()\n",
    "\n",
    "# Determine the best strategy for each model\n",
    "for model_name in models.keys():\n",
    "    model_data = avg_performance[avg_performance['Model'] == model_name]\n",
    "    best_strategy = model_data.loc[model_data['Value'].idxmax(), 'Strategy']\n",
    "    best_score = model_data['Value'].max()\n",
    "    print(f\"  {model_name} performs best with {best_strategy} strategy (avg. score: {best_score:.3f})\")\n",
    "\n",
    "# Feature importance for Random Forest with the best performing strategy\n",
    "best_rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "best_rf_model.fit(X_smote, y_smote)  # Use SMOTE as it often works well\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_cleaned.columns,\n",
    "    'importance': best_rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(data=feature_importance, y='feature', x='importance')\n",
    "plt.title('Top 15 Feature Importances (Random Forest with SMOTE)')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 15 Features Importance (Random Forest with SMOTE):\")\n",
    "for idx, row in feature_importance.iterrows():\n",
    "    print(f\"  {idx+1}. {row['feature']}: {row['importance']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Data Quality Report\n",
    "\n",
    "Now, let's compile a comprehensive data quality report for our lending club dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Comprehensive Data Quality Report\n",
    "print(\"Comprehensive Data Quality Report\")\n",
    "\n",
    "# Quality metrics\n",
    "def data_quality_report(df):\n",
    "    \"\"\"Generate a comprehensive data quality report\"\"\"\n",
    "    report = {\n",
    "        'dataset_info': {\n",
    "            'shape': df.shape,\n",
    "            'size_mb': round(df.memory_usage(deep=True).sum() / 1024**2, 2),\n",
    "            'num_features': len(df.select_dtypes(include=[np.number]).columns),\n",
    "            'cat_features': len(df.select_dtypes(include=['object', 'category']).columns)\n",
    "        },\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'duplicates': df.duplicated().sum(),\n",
    "        'infinite_values': df.select_dtypes(include=[np.number]).apply(lambda x: np.isinf(x).sum()).to_dict(),\n",
    "        'zero_variance': df.apply(lambda x: x.nunique() == 1).to_dict(),\n",
    "        'high_cardinality': df.select_dtypes(include=['object']).apply(lambda x: x.nunique() / len(x) > 0.9).to_dict(),\n",
    "        'data_types': df.dtypes.to_dict()\n",
    "    }\n",
    "    return report\n",
    "\n",
    "# Generate quality report\n",
    "quality_report = data_quality_report(df_cleaned)\n",
    "\n",
    "print(f\"\\nDataset Quality Report:\")\n",
    "print(f\"  Shape: {quality_report['dataset_info']['shape']}\")\n",
    "print(f\"  Memory Usage: {quality_report['dataset_info']['size_mb']} MB\")\n",
    "print(f\"  Numerical Features: {quality_report['dataset_info']['num_features']}\")\n",
    "print(f\"  Categorical Features: {quality_report['dataset_info']['cat_features']}\")\n",
    "print(f\"  Missing Values: {sum(quality_report['missing_values'].values())}\")\n",
    "print(f\"  Duplicates: {quality_report['duplicates']}\")\n",
    "print(f\"  Infinite Values: {sum(quality_report['infinite_values'].values())}\")\n",
    "print(f\"  Zero Variance Features: {sum(quality_report['zero_variance'].values())}\")\n",
    "print(f\"  High Cardinality Features: {sum(quality_report['high_cardinality'].values())}\")\n",
    "\n",
    "# Detailed missing values analysis\n",
    "missing_df = pd.DataFrame(list(quality_report['missing_values'].items()), \n",
    "                         columns=['Column', 'Missing_Count'])\n",
    "missing_df['Percentage'] = (missing_df['Missing_Count'] / len(df_cleaned)) * 100\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if not missing_df.empty:\n",
    "    print(f\"\\nDetailed Missing Values Analysis:\")\n",
    "    for _, row in missing_df.head().iterrows():\n",
    "        print(f\"  {row['Column']}: {row['Missing_Count']} values ({row['Percentage']:.2f}%)\")\n",
    "\n",
    "# Visualization of data quality metrics\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Missing values\n",
    "if not missing_df.empty:\n",
    "    axes[0, 0].barh(range(len(missing_df.head(10))), missing_df.head(10)['Percentage'],\n",
    "                    tick_label=missing_df.head(10)['Column'], color='orange', alpha=0.7)\n",
    "    axes[0, 0].set_title('Top 10 Features with Missing Values')\n",
    "    axes[0, 0].set_xlabel('Percentage of Missing Values')\n",
    "else:\n",
    "    axes[0, 0].text(0.5, 0.5, 'No Missing Values',\n",
    "                    horizontalalignment='center', verticalalignment='center',\n",
    "                    transform=axes[0, 0].transAxes)\n",
    "    axes[0, 0].set_title('Missing Values')\n",
    "\n",
    "# 2. Data type distribution\n",
    "dtype_counts = pd.Series(quality_report['data_types']).value_counts()\n",
    "axes[0, 1].pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "axes[0, 1].set_title('Data Type Distribution')\n",
    "\n",
    "# 3. Duplicates check\n",
    "axes[0, 2].bar(['Unique', 'Duplicates'], [len(df_cleaned)-quality_report['duplicates'], quality_report['duplicates']],\n",
    "               color=['skyblue', 'red'], alpha=0.7)\n",
    "axes[0, 2].set_title('Unique vs Duplicates')\n",
    "axes[0, 2].set_ylabel('Count')\n",
    "for i, v in enumerate([len(df_cleaned)-quality_report['duplicates'], quality_report['duplicates']]):\n",
    "    axes[0, 2].text(i, v + max([len(df_cleaned)-quality_report['duplicates'], quality_report['duplicates']])*0.01, \n",
    "                     str(v), ha='center')\n",
    "\n",
    "# 4. Feature types\n",
    "axes[1, 0].bar(['Numerical', 'Categorical'], \n",
    "               [quality_report['dataset_info']['num_features'], \n",
    "                quality_report['dataset_info']['cat_features']],\n",
    "               color=['lightblue', 'lightgreen'], alpha=0.7)\n",
    "axes[1, 0].set_title('Numerical vs Categorical Features')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "for i, v in enumerate([quality_report['dataset_info']['num_features'], \n",
    "                       quality_report['dataset_info']['cat_features']]):\n",
    "    axes[1, 0].text(i, v + max([quality_report['dataset_info']['num_features'], \n",
    "                                quality_report['dataset_info']['cat_features']])*0.01, \n",
    "                     str(v), ha='center')\n",
    "\n",
    "# 5. Zero variance features\n",
    "zv_features = [k for k, v in quality_report['zero_variance'].items() if v]\n",
    "if zv_features:\n",
    "    axes[1, 1].barh(range(len(zv_features)), [1] * len(zv_features),\n",
    "                    tick_label=zv_features, color='lightcoral', alpha=0.7)\n",
    "    axes[1, 1].set_title('Zero Variance Features')\n",
    "    axes[1, 1].set_xlabel('Constant Features')\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No Zero Variance Features',\n",
    "                    horizontalalignment='center', verticalalignment='center',\n",
    "                    transform=axes[1, 1].transAxes)\n",
    "    axes[1, 1].set_title('Zero Variance Features')\n",
    "\n",
    "# 6. Memory usage by data type\n",
    "memory_by_dtype = df_cleaned.groupby(df_cleaned.dtypes, sort=False).agg({'memory_usage': lambda x: x.sum() / 1024**2})\n",
    "memory_by_dtype.columns = ['Memory_MB']\n",
    "axes[1, 2].bar(memory_by_dtype.index.astype(str), memory_by_dtype['Memory_MB'],\n",
    "               color='violet', alpha=0.7)\n",
    "axes[1, 2].set_title('Memory Usage by Data Type')\n",
    "axes[1, 2].set_ylabel('Memory Usage (MB)')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Outlier analysis\n",
    "print(f\"\\nOutlier Analysis:\")\n",
    "outlier_cols = ['loan_amnt', 'int_rate', 'annual_inc', 'fico_score', 'dti']\n",
    "\n",
    "for col in outlier_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        Q1 = df_cleaned[col].quantile(0.25)\n",
    "        Q3 = df_cleaned[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers_count = ((df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)).sum()\n",
    "        print(f\"  {col}: {outliers_count} outliers ({outliers_count/len(df_cleaned)*100:.2f}% of data)\")\n",
    "\n",
    "# Distribution analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "for i, col in enumerate(outlier_cols[:6]):\n",
    "    if i >= 6: break\n",
    "    row = i // 3\n",
    "    col_idx = i % 3\n",
    "    \n",
    "    axes[row, col_idx].boxplot(df_cleaned[col])\n",
    "    axes[row, col_idx].set_title(f'Distribution of {col}')\n",
    "    axes[row, col_idx].set_ylabel(col)\n",
    "\n",
    "# Add a histogram of overall default rate\n",
    "axes[1, 2].hist(df_cleaned['loan_status'], bins=3, edgecolor='black', align='left', alpha=0.7)\n",
    "axes[1, 2].set_title('Distribution of Target Variable')\n",
    "axes[1, 2].set_xlabel('Loan Status (0=Paid, 1=Default)')\n",
    "axes[1, 2].set_ylabel('Count')\n",
    "# Set x-ticks to align correctly with histogram\n",
    "axes[1, 2].set_xticks([0, 1])\n",
    "axes[1, 2].set_xticklabels(['Paid', 'Default'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nData Cleaning and Feature Engineering Complete!\")\n",
    "print(f\"Final dataset shape: {df_cleaned.shape}\")\n",
    "print(f\"Final default rate: {df_cleaned['loan_status'].mean():.2%}\")\n",
    "print(f\"Total features after engineering: {len(X_cleaned.columns)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this comprehensive notebook, we've covered essential aspects of data science for credit risk modeling:\n",
    "\n",
    "1. **Data Cleaning**: Addressed missing values, duplicates, and inconsistencies\n",
    "2. **Feature Engineering**: Created new meaningful features and transformed existing ones\n",
    "3. **Handling Imbalanced Data**: Applied various techniques to address class imbalance\n",
    "4. **Visualization**: Used matplotlib, pandas, and seaborn for comprehensive data analysis\n",
    "5. **Quality Assessment**: Conducted thorough data quality analysis\n",
    "\n",
    "These steps are fundamental to any successful machine learning project and critical for creating reliable predictive models in credit risk analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 }
}