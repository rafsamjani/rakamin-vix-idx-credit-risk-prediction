{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Feature Engineering and Model Development\n",
    "\n",
    "**Author**: Rafsamjani Anugrah  \n",
    "**Date**: 2024  \n",
    "**Project**: Credit Risk Prediction - ID/X Partners  \n",
    "\n",
    "## Tujuan Notebook\n",
    "\n",
    "Notebook ini berfokus pada:\n",
    "1. Load cleaned dataset dari notebook sebelumnya\n",
    "2. Feature selection untuk model yang optimal\n",
    "3. Data preprocessing untuk machine learning\n",
    "4. Training multiple machine learning models\n",
    "5. Hyperparameter tuning dan model selection\n",
    "6. Model evaluation dan business impact analysis\n",
    "\n",
    "## Prerequisites\n",
    "- Dataset sudah dibersihkan di notebook 02\n",
    "- Environment sudah ter-setup dengan ML libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "import shap\n",
    "\n",
    "# Set styling\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"ü§ñ Machine Learning Libraries Loaded!\")\n",
    "print(f\"üìÖ Modeling started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset\n",
    "cleaned_data_paths = [\n",
    "    '../data/processed/loan_data_cleaned.csv',\n",
    "    '../../data/processed/loan_data_cleaned.csv',\n",
    "    'data/processed/loan_data_cleaned.csv',\n",
    "    'loan_data_cleaned.csv'\n",
    "]\n",
    "\n",
    "cleaned_path = None\n",
    "for path in cleaned_data_paths:\n",
    "    if os.path.exists(path):\n",
    "        cleaned_path = path\n",
    "        break\n",
    "\n",
    "if cleaned_path:\n",
    "    print(f\"‚úÖ Loading cleaned dataset from: {cleaned_path}\")\n",
    "    \n",
    "    # Load cleaned dataset\n",
    "    df = pd.read_csv(cleaned_path, low_memory=False)\n",
    "    \n",
    "    print(f\"üìä Cleaned dataset loaded successfully!\")\n",
    "    print(f\"   Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Load cleaning log if available\n",
    "    cleaning_log_path = cleaned_path.replace('.csv', '_log.json')\n",
    "    if os.path.exists(cleaning_log_path):\n",
    "        with open(cleaning_log_path, 'r') as f:\n",
    "            cleaning_log = json.load(f)\n",
    "        print(f\"üìã Cleaning log loaded\")\n",
    "        print(f\"   Original dataset: {cleaning_log['original_shape']}\")\n",
    "        print(f\"   Records processed: {cleaning_log['records_removed']:,} removed\")\n",
    "        print(f\"   Features engineered: {cleaning_log['engineered_features']}\")\n",
    "        print(f\"   Default rate: {cleaning_log['default_rate']:.2f}%\")\n",
    "    \n",
    "    # Check target variable\n",
    "    if 'loan_status_binary' in df.columns:\n",
    "        target_counts = df['loan_status_binary'].value_counts()\n",
    "        default_rate = target_counts[1] / len(df) * 100\n",
    "        print(f\"\\nüéØ Target Variable (loan_status_binary):\")\n",
    "        print(f\"   Fully Paid (0): {target_counts[0]:,} ({target_counts[0]/len(df)*100:.1f}%)\")\n",
    "        print(f\"   Charged Off (1): {target_counts[1]:,} ({default_rate:.1f}%)\")\n",
    "        print(f\"   Class imbalance ratio: {target_counts[0]/target_counts[1]:.1f}:1\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cleaned dataset not found!\")\n",
    "    print(\"Please ensure you've run the data cleaning notebook first.\")\n",
    "    print(\"Expected location: data/processed/loan_data_cleaned.csv\")\n",
    "    df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"FEATURE SELECTION AND ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display available columns\n",
    "    print(f\"\\nüìã Available Columns ({len(df.columns)} total):\")\n",
    "    \n",
    "    # Group columns by category for better analysis\n",
    "    loan_features = []\n",
    "    borrower_features = []\n",
    "    credit_features = []\n",
    "    engineered_features = []\n",
    "    other_features = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in ['loan_status', 'loan_status_binary']:\n",
    "            continue  # Skip target variables\n",
    "        elif any(keyword in col.lower() for keyword in ['loan', 'term', 'purpose', 'grade', 'int_rate', 'installment']):\n",
    "            loan_features.append(col)\n",
    "        elif any(keyword in col.lower() for keyword in ['annual', 'emp', 'home', 'verification', 'dti']):\n",
    "            borrower_features.append(col)\n",
    "        elif any(keyword in col.lower() for keyword in ['fico', 'credit', 'delinq', 'revol', 'pub_rec', 'inq', 'earliest']):\n",
    "            credit_features.append(col)\n",
    "        elif any(keyword in col.lower() for keyword in ['ratio', 'score', 'flag', 'category', 'effective']):\n",
    "            engineered_features.append(col)\n",
    "        else:\n",
    "            other_features.append(col)\n",
    "    \n",
    "    print(f\"\\nüí∞ Loan Features ({len(loan_features)}):\")\n",
    "    for i, col in enumerate(loan_features, 1):\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    print(f\"\\nüë§ Borrower Features ({len(borrower_features)}):\")\n",
    "    for i, col in enumerate(borrower_features, 1):\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    print(f\"\\nüìà Credit History Features ({len(credit_features)}):\")\n",
    "    for i, col in enumerate(credit_features, 1):\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    print(f\"\\nüîß Engineered Features ({len(engineered_features)}):\")\n",
    "    for i, col in enumerate(engineered_features, 1):\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    print(f\"\\nüìÇ Other Features ({len(other_features)}):\")\n",
    "    for i, col in enumerate(other_features, 1):\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "    \n",
    "    # Select features for modeling (based on domain knowledge and availability)\n",
    "    print(f\"\\nüéØ SELECTING FEATURES FOR MODELING:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Core features based on importance and data availability\n",
    "    selected_features = [\n",
    "        # Loan characteristics\n",
    "        'loan_amnt', 'int_rate', 'term_months', 'installment', \n",
    "        'grade', 'purpose',\n",
    "        \n",
    "        # Borrower information\n",
    "        'annual_inc', 'emp_length_numeric', 'home_ownership', \n",
    "        'verification_status', 'dti',\n",
    "        \n",
    "        # Credit history\n",
    "        'fico_avg', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', \n",
    "        'pub_rec', 'revol_bal', 'revol_util',\n",
    "        \n",
    "        # Engineered features\n",
    "        'loan_to_income_ratio', 'effective_dti', 'employment_stability_score',\n",
    "        'credit_age_years', 'high_utilization_flag', 'recent_delinquency_flag'\n",
    "    ]\n",
    "    \n",
    "    # Filter available features\n",
    "    available_selected_features = [f for f in selected_features if f in df.columns]\n",
    "    \n",
    "    print(f\"\\nüìä Selected Features ({len(available_selected_features)}):\")\n",
    "    for i, feature in enumerate(available_selected_features, 1):\n",
    "        data_type = df[feature].dtype\n",
    "        missing_count = df[feature].isnull().sum()\n",
    "        unique_count = df[feature].nunique()\n",
    "        print(f\"   {i:2d}. {feature:<25} | {str(data_type):<8} | {missing_count:>4} missing | {unique_count:>4} unique\")\n",
    "    \n",
    "    # Check correlation with target for numerical features\n",
    "    if 'loan_status_binary' in df.columns:\n",
    "        print(f\"\\nüîç Correlation Analysis with Target:\")\n",
    "        \n",
    "        numerical_features = df[available_selected_features].select_dtypes(include=[np.number]).columns\n",
    "        correlations = df[numerical_features].corrwith(df['loan_status_binary']).sort_values(ascending=False)\n",
    "        \n",
    "        print(f\"   Top positive correlations (higher default risk):\")\n",
    "        for col, corr in correlations.head(5).items():\n",
    "            print(f\"     {col:<25}: {corr:+.3f}\")\n",
    "        \n",
    "        print(f\"\\n   Top negative correlations (lower default risk):\")\n",
    "        for col, corr in correlations.tail(5).items():\n",
    "            print(f\"     {col:<25}: {corr:+.3f}\")\n",
    "    \n",
    "    # Create feature matrix and target vector\n",
    "    X = df[available_selected_features].copy()\n",
    "    y = df['loan_status_binary'].copy()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Feature selection completed!\")\n",
    "    print(f\"   Feature matrix shape: {X.shape}\")\n",
    "    print(f\"   Target vector shape: {y.shape}\")\n",
    "    print(f\"   Target distribution: {y.value_counts().to_dict()}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Dataset not available for feature selection\")\n",
    "    X, y = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X is not None and y is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"DATA PREPROCESSING FOR MACHINE LEARNING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    print(\"\\nüì¶ Splitting data into train and test sets...\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"   Training set: {X_train.shape[0]:,} samples\")\n",
    "    print(f\"   Test set: {X_test.shape[0]:,} samples\")\n",
    "    print(f\"   Training default rate: {y_train.mean()*100:.2f}%\")\n",
    "    print(f\"   Test default rate: {y_test.mean()*100:.2f}%\")\n",
    "    \n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nüìä Column Types:\")\n",
    "    print(f\"   Categorical columns: {len(categorical_cols)}\")\n",
    "    print(f\"   Numerical columns: {len(numerical_cols)}\")\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(f\"   Categorical: {categorical_cols}\")\n",
    "    \n",
    "    # Preprocessing function\n",
    "    def preprocess_data(X_train, X_test, categorical_cols, numerical_cols):\n",
    "        \"\"\"Comprehensive preprocessing pipeline\"\"\"\n",
    "        \n",
    "        print(\"\\nüîß Preprocessing data...\")\n",
    "        \n",
    "        # Make copies\n",
    "        X_train_processed = X_train.copy()\n",
    "        X_test_processed = X_test.copy()\n",
    "        \n",
    "        # 1. Handle categorical variables\n",
    "        print(\"   Processing categorical variables...\")\n",
    "        categorical_encoders = {}\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            print(f\"     Processing {col}...\")\n",
    "            \n",
    "            if col == 'grade':\n",
    "                # Ordinal encoding for loan grades\n",
    "                grade_mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n",
    "                X_train_processed[col] = X_train[col].map(grade_mapping).fillna(4)  # Default to grade D\n",
    "                X_test_processed[col] = X_test[col].map(grade_mapping).fillna(4)\n",
    "                categorical_encoders[col] = {'type': 'ordinal', 'mapping': grade_mapping}\n",
    "                \n",
    "            else:\n",
    "                # One-hot encoding for other categorical variables\n",
    "                encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "                \n",
    "                # Fit on training data\n",
    "                train_encoded = encoder.fit_transform(X_train[[col]])\n",
    "                feature_names = [f\"{col}_{cat}\" for cat in encoder.categories_[0]]\n",
    "                \n",
    "                # Convert to DataFrame\n",
    "                train_encoded_df = pd.DataFrame(train_encoded, columns=feature_names, index=X_train.index)\n",
    "                X_train_processed = pd.concat([X_train_processed.drop(col, axis=1), train_encoded_df], axis=1)\n",
    "                \n",
    "                # Transform test data\n",
    "                test_encoded = encoder.transform(X_test[[col]])\n",
    "                test_encoded_df = pd.DataFrame(test_encoded, columns=feature_names, index=X_test.index)\n",
    "                X_test_processed = pd.concat([X_test_processed.drop(col, axis=1), test_encoded_df], axis=1)\n",
    "                \n",
    "                categorical_encoders[col] = {\n",
    "                    'type': 'onehot',\n",
    "                    'encoder': encoder,\n",
    "                    'feature_names': feature_names\n",
    "                }\n",
    "        \n",
    "        # 2. Handle numerical variables\n",
    "        print(\"   Processing numerical variables...\")\n",
    "        \n",
    "        # Handle missing values in numerical columns\n",
    "        numerical_imputer_values = {}\n",
    "        for col in numerical_cols:\n",
    "            if col in X_train_processed.columns:\n",
    "                if X_train_processed[col].isnull().any():\n",
    "                    impute_value = X_train_processed[col].median()\n",
    "                    X_train_processed[col].fillna(impute_value, inplace=True)\n",
    "                    X_test_processed[col].fillna(impute_value, inplace=True)\n",
    "                    numerical_imputer_values[col] = impute_value\n",
    "        \n",
    "        # 3. Scale numerical features\n",
    "        print(\"   Scaling numerical features...\")\n",
    "        \n",
    "        # Update numerical columns after encoding\n",
    "        final_numerical_cols = X_train_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_processed[final_numerical_cols] = scaler.fit_transform(X_train_processed[final_numerical_cols])\n",
    "        X_test_processed[final_numerical_cols] = scaler.transform(X_test_processed[final_numerical_cols])\n",
    "        \n",
    "        print(f\"   Final training set shape: {X_train_processed.shape}\")\n",
    "        print(f\"   Final test set shape: {X_test_processed.shape}\")\n",
    "        \n",
    "        return (X_train_processed, X_test_processed, \n",
    "                categorical_encoders, numerical_imputer_values, \n",
    "                scaler, final_numerical_cols)\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    (X_train_processed, X_test_processed, \n",
    "     categorical_encoders, numerical_imputer_values, \n",
    "     scaler, final_numerical_cols) = preprocess_data(X_train, X_test, categorical_cols, numerical_cols)\n",
    "    \n",
    "    # Display preprocessing results\n",
    "    print(f\"\\n‚úÖ Preprocessing completed!\")\n",
    "    print(f\"   Training features: {X_train_processed.shape[1]}\")\n",
    "    print(f\"   Test features: {X_test_processed.shape[1]}\")\n",
    "    print(f\"   Feature alignment: {X_train_processed.shape[1] == X_test_processed.shape[1]}\")\n",
    "    \n",
    "    # Display sample of processed data\n",
    "    print(f\"\\nüìã Sample of processed training data (first 3 rows, first 5 columns):\")\n",
    "    display(X_train_processed.iloc[:3, :5].round(3))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Data not available for preprocessing\")\n",
    "    X_train_processed, X_test_processed = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train_processed is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"HANDLING CLASS IMBALANCE\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Analyze class imbalance\n",
    "    print(f\"\\nüìä Class Imbalance Analysis:\")\n",
    "    print(f\"   Training set distribution:\")\n",
    "    print(f\"     Class 0 (Fully Paid): {np.sum(y_train == 0):,} ({np.mean(y_train == 0)*100:.1f}%)\")\n",
    "    print(f\"     Class 1 (Charged Off): {np.sum(y_train == 1):,} ({np.mean(y_train == 1)*100:.1f}%)\")\n",
    "    print(f\"     Imbalance ratio: {np.sum(y_train == 0)/np.sum(y_train == 1):.1f}:1\")\n",
    "    \n",
    "    # Apply SMOTE to handle class imbalance\n",
    "    print(f\"\\nüîß Applying SMOTE (Synthetic Minority Over-sampling Technique)...\")\n",
    "    \n",
    "    # Use SMOTETomek for better balance\n",
    "    smote_tomek = SMOTETomek(random_state=42, sampling_strategy='auto')\n",
    "    \n",
    "    print(f\"   Original training set shape: {X_train_processed.shape}\")\n",
    "    print(f\"   Original target distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train_processed, y_train)\n",
    "    \n",
    "    print(f\"\\n   Resampled training set shape: {X_train_resampled.shape}\")\n",
    "    print(f\"   Resampled target distribution: {pd.Series(y_train_resampled).value_counts().to_dict()}\")\n",
    "    \n",
    "    # Visualize class distribution before and after SMOTE\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Before SMOTE\n",
    "    original_counts = pd.Series(y_train).value_counts()\n",
    "    ax1.bar(['Fully Paid', 'Charged Off'], [original_counts[0], original_counts[1]], \n",
    "           color=['green', 'red'])\n",
    "    ax1.set_title('Before SMOTE')\n",
    "    ax1.set_ylabel('Count')\n",
    "    for i, count in enumerate([original_counts[0], original_counts[1]]):\n",
    "        ax1.text(i, count + max(original_counts)*0.01, f'{count:,}', ha='center')\n",
    "    \n",
    "    # After SMOTE\n",
    "    resampled_counts = pd.Series(y_train_resampled).value_counts()\n",
    "    ax2.bar(['Fully Paid', 'Charged Off'], [resampled_counts[0], resampled_counts[1]], \n",
    "           color=['green', 'red'])\n",
    "    ax2.set_title('After SMOTE')\n",
    "    ax2.set_ylabel('Count')\n",
    "    for i, count in enumerate([resampled_counts[0], resampled_counts[1]]):\n",
    "        ax2.text(i, count + max(resampled_counts)*0.01, f'{count:,}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Class imbalance handled successfully!\")\n",
    "    print(f\"   Balanced dataset ready for model training\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Processed data not available for SMOTE\")\n",
    "    X_train_resampled, y_train_resampled = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if X_train_resampled is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"MODEL TRAINING AND EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Define models to train\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            random_state=42, \n",
    "            max_iter=1000,\n",
    "            class_weight='balanced'  # Handle imbalance in model\n",
    "        ),\n",
    "        'Random Forest': RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=15,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        ),\n",
    "        'XGBoost': xgb.XGBClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            scale_pos_weight=3  # Handle imbalance\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"\\nü§ñ Training {len(models)} models...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nüîß Training {name}...\")\n",
    "        \n",
    "        # Train model\n",
    "        start_time = datetime.now()\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "        training_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"   Training completed in {training_time:.2f} seconds\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_processed)\n",
    "        y_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_proba)\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train_resampled, y_train_resampled, \n",
    "                                  cv=5, scoring='roc_auc')\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'training_time': training_time\n",
    "        }\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'metrics': metrics,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_proba\n",
    "        }\n",
    "        \n",
    "        print(f\"   üìä Performance Metrics:\")\n",
    "        print(f\"      Accuracy:  {accuracy:.3f}\")\n",
    "        print(f\"      Precision: {precision:.3f}\")\n",
    "        print(f\"      Recall:    {recall:.3f}\")\n",
    "        print(f\"      F1-Score:  {f1:.3f}\")\n",
    "        print(f\"      ROC-AUC:   {roc_auc:.3f}\")\n",
    "        print(f\"      CV Score:  {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = max(results.keys(), key=lambda k: results[k]['metrics']['roc_auc'])\n",
    "    best_model = results[best_model_name]['model']\n",
    "    best_metrics = results[best_model_name]['metrics']\n",
    "    \n",
    "    print(f\"\\nüèÜ BEST MODEL: {best_model_name}\")\n",
    "    print(f\"   ROC-AUC: {best_metrics['roc_auc']:.3f}\")\n",
    "    print(f\"   F1-Score: {best_metrics['f1']:.3f}\")\n",
    "    print(f\"   Precision: {best_metrics['precision']:.3f}\")\n",
    "    print(f\"   Recall: {best_metrics['recall']:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Resampled data not available for model training\")\n",
    "    results = None\n",
    "    best_model_name = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Detailed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"MODEL COMPARISON AND DETAILED ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for name, result in results.items():\n",
    "        metrics = result['metrics']\n",
    "        comparison_data.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'F1-Score': metrics['f1'],\n",
    "            'ROC-AUC': metrics['roc_auc'],\n",
    "            'CV Mean': metrics['cv_mean'],\n",
    "            'CV Std': metrics['cv_std']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data).set_index('Model')\n",
    "    \n",
    "    print(f\"\\nüìä Model Performance Comparison:\")\n",
    "    print(\"=\"*50)\n",
    "    display(comparison_df.round(3))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    metrics_list = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "    \n",
    "    for i, metric in enumerate(metrics_list):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        \n",
    "        comparison_df[metric].sort_values().plot(kind='bar', ax=axes[row, col], color='skyblue')\n",
    "        axes[row, col].set_title(f'{metric} Comparison')\n",
    "        axes[row, col].set_ylabel(metric)\n",
    "        axes[row, col].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels\n",
    "        for j, v in enumerate(comparison_df[metric].sort_values()):\n",
    "            axes[row, col].text(j, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # ROC curves comparison\n",
    "    ax_roc = axes[1, 2]\n",
    "    for name, result in results.items():\n",
    "        fpr, tpr, _ = roc_curve(y_test, result['probabilities'])\n",
    "        ax_roc.plot(fpr, tpr, linewidth=2, label=f'{name} (AUC = {result[\"metrics\"][\"roc_auc\"]:.3f})')\n",
    "    \n",
    "    ax_roc.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "    ax_roc.set_xlabel('False Positive Rate')\n",
    "    ax_roc.set_ylabel('True Positive Rate')\n",
    "    ax_roc.set_title('ROC Curves Comparison')\n",
    "    ax_roc.legend()\n",
    "    ax_roc.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Detailed analysis of best model\n",
    "    if best_model_name:\n",
    "        print(f\"\\nüîç Detailed Analysis - Best Model: {best_model_name}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        best_result = results[best_model_name]\n",
    "        y_pred = best_result['predictions']\n",
    "        y_proba = best_result['probabilities']\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        print(f\"\\nüìä Confusion Matrix:\")\n",
    "        print(f\"   True Negatives:  {tn:6d} (Good loans correctly approved)\")\n",
    "        print(f\"   False Positives: {fp:6d} (Good loans incorrectly rejected)\")\n",
    "        print(f\"   False Negatives: {fn:6d} (Bad loans incorrectly approved)\")\n",
    "        print(f\"   True Positives:  {tp:6d} (Bad loans correctly rejected)\")\n",
    "        \n",
    "        # Visualize confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=['Fully Paid', 'Charged Off'],\n",
    "                    yticklabels=['Fully Paid', 'Charged Off'])\n",
    "        plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "        \n",
    "        # Feature Importance (for tree-based models)\n",
    "        if hasattr(best_result['model'], 'feature_importances_'):\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'feature': X_test_processed.columns,\n",
    "                'importance': best_result['model'].feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nüéØ Top 15 Feature Importances:\")\n",
    "            display(feature_importance.head(15))\n",
    "            \n",
    "            # Visualize feature importance\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            top_features = feature_importance.head(15)\n",
    "            plt.barh(range(len(top_features)), top_features['importance'], color='lightcoral')\n",
    "            plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "            plt.xlabel('Feature Importance')\n",
    "            plt.title(f'Top 15 Feature Importance - {best_model_name}')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Classification Report\n",
    "        print(f\"\\nüìã Detailed Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred, \n",
    "                                  target_names=['Fully Paid', 'Charged Off']))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No model results available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results is not None and best_model_name:\n",
    "    print(\"=\"*80)\n",
    "    print(\"BUSINESS IMPACT ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    best_result = results[best_model_name]\n",
    "    y_pred = best_result['predictions']\n",
    "    y_proba = best_result['probabilities']\n",
    "    \n",
    "    # Business assumptions\n",
    "    avg_loan_amount = X_test['loan_amnt'].mean() if 'loan_amnt' in X_test.columns else 15000\n",
    "    avg_interest_rate = X_test['int_rate'].mean() if 'int_rate' in X_test.columns else 0.13\n",
    "    default_loss_rate = 0.60  # 60% loss on default\n",
    "    cost_of_capital = 0.05   # 5% cost of funds\n",
    "    processing_cost = 100   # $100 per application processing cost\n",
    "    \n",
    "    # Confusion matrix values\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    \n",
    "    print(f\"\\nüí∞ Financial Assumptions:\")\n",
    "    print(f\"   Average loan amount: ${avg_loan_amount:,.0f}\")\n",
    "    print(f\"   Average interest rate: {avg_interest_rate:.1%}\")\n",
    "    print(f\"   Default loss rate: {default_loss_rate:.1%}\")\n",
    "    print(f\"   Cost of capital: {cost_of_capital:.1%}\")\n",
    "    print(f\"   Processing cost per application: ${processing_cost}\")\n",
    "    \n",
    "    # Calculate financial impact\n",
    "    total_applications = len(y_test)\n",
    "    \n",
    "    # Benefits\n",
    "    profit_from_good_loans = tn * avg_loan_amount * (avg_interest_rate - cost_of_capital)\n",
    "    savings_from_avoided_defaults = tp * avg_loan_amount * default_loss_rate\n",
    "    \n",
    "    # Costs\n",
    "    opportunity_cost = fp * avg_loan_amount * (avg_interest_rate - cost_of_capital)\n",
    "    default_losses = fn * avg_loan_amount * default_loss_rate\n",
    "    processing_costs = total_applications * processing_cost\n",
    "    \n",
    "    # Net impact\n",
    "    total_benefits = profit_from_good_loans + savings_from_avoided_defaults\n",
    "    total_costs = opportunity_cost + default_losses + processing_costs\n",
    "    net_financial_impact = total_benefits - total_costs\n",
    "    \n",
    "    print(f\"\\nüìä Financial Impact Breakdown:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüíµ BENEFITS:\")\n",
    "    print(f\"   Profit from approved good loans:  ${profit_from_good_loans:,.0f}\")\n",
    "    print(f\"   Savings from avoided defaults:     ${savings_from_avoided_defaults:,.0f}\")\n",
    "    print(f\"   Total Benefits:                    ${total_benefits:,.0f}\")\n",
    "    \n",
    "    print(f\"\\nüí∏ COSTS:\")\n",
    "    print(f\"   Opportunity cost (false pos):    ${opportunity_cost:,.0f}\")\n",
    "    print(f\"   Default losses (false neg):      ${default_losses:,.0f}\")\n",
    "    print(f\"   Processing costs:                 ${processing_costs:,.0f}\")\n",
    "    print(f\"   Total Costs:                      ${total_costs:,.0f}\")\n",
    "    \n",
    "    print(f\"\\nüìà NET FINANCIAL IMPACT:\")\n",
    "    print(f\"   Net Impact:                       ${net_financial_impact:,.0f}\")\n",
    "    \n",
    "    if net_financial_impact > 0:\n",
    "        print(f\"   ‚úÖ Model creates ${net_financial_impact:,.0f} positive impact\")\n",
    "        roi = (net_financial_impact / total_costs) * 100 if total_costs > 0 else 0\n",
    "        print(f\"   üíπ Return on Investment: {roi:.1f}%\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Model results in ${abs(net_financial_impact):,.0f} negative impact\")\n",
    "    \n",
    "    # Calculate efficiency metrics\n",
    "    approval_rate = (tn + fn) / len(y_test)\n",
    "    default_rate_in_approved = fn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä Efficiency Metrics:\")\n",
    "    print(f\"   Approval Rate: {approval_rate:.1%}\")\n",
    "    print(f\"   Default Rate in Approved: {default_rate_in_approved:.1%}\")\n",
    "    print(f\"   True Positive Rate: {(tp/(tp+fn)):.1%}\")\n",
    "    print(f\"   True Negative Rate: {(tn/(tn+fp)):.1%}\")\n",
    "    \n",
    "    # Optimal threshold analysis\n",
    "    print(f\"\\nüéØ Threshold Optimization Analysis:\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "    threshold_analysis = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred_thresh = (y_proba >= threshold).astype(int)\n",
    "        tn_t, fp_t, fn_t, tp_t = confusion_matrix(y_test, y_pred_thresh).ravel()\n",
    "        \n",
    "        # Calculate financial impact for this threshold\n",
    "        benefits_t = (tn_t * avg_loan_amount * (avg_interest_rate - cost_of_capital) + \n",
    "                      tp_t * avg_loan_amount * default_loss_rate)\n",
    "        costs_t = (fp_t * avg_loan_amount * (avg_interest_rate - cost_of_capital) + \n",
    "                    fn_t * avg_loan_amount * default_loss_rate + \n",
    "                    len(y_test) * processing_cost)\n",
    "        net_impact_t = benefits_t - costs_t\n",
    "        \n",
    "        threshold_analysis.append({\n",
    "            'threshold': threshold,\n",
    "            'net_impact': net_impact_t,\n",
    "            'approval_rate': (tn_t + fn_t) / len(y_test),\n",
    "            'default_rate': fn_t / (tn_t + fn_t) if (tn_t + fn_t) > 0 else 0,\n",
    "            'precision': precision_score(y_test, y_pred_thresh),\n",
    "            'recall': recall_score(y_test, y_pred_thresh)\n",
    "        })\n",
    "    \n",
    "    threshold_df = pd.DataFrame(threshold_analysis)\n",
    "    best_threshold_row = threshold_df.loc[threshold_df['net_impact'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nOptimal Threshold Analysis Results:\")\n",
    "    print(f\"   Best Threshold: {best_threshold_row['threshold']:.2f}\")\n",
    "    print(f\"   Max Net Impact: ${best_threshold_row['net_impact']:,.0f}\")\n",
    "    print(f\"   Approval Rate: {best_threshold_row['approval_rate']:.1%}\")\n",
    "    print(f\"   Default Rate: {best_threshold_row['default_rate']:.1%}\")\n",
    "    print(f\"   Precision: {best_threshold_row['precision']:.3f}\")\n",
    "    print(f\"   Recall: {best_threshold_row['recall']:.3f}\")\n",
    "    \n",
    "    # Visualize threshold analysis\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Net impact by threshold\n",
    "    ax1.plot(threshold_df['threshold'], threshold_df['net_impact'], 'b-', linewidth=2)\n",
    "    ax1.axvline(best_threshold_row['threshold'], color='r', linestyle='--', alpha=0.7)\n",
    "    ax1.set_xlabel('Threshold')\n",
    "    ax1.set_ylabel('Net Financial Impact ($)', color='b')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "    ax1.set_title('Net Financial Impact by Threshold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Approval rate by threshold\n",
    "    ax2.plot(threshold_df['threshold'], threshold_df['approval_rate'], 'g-', linewidth=2)\n",
    "    ax2.axvline(best_threshold_row['threshold'], color='r', linestyle='--', alpha=0.7)\n",
    "    ax2.set_xlabel('Threshold')\n",
    "    ax2.set_ylabel('Approval Rate', color='g')\n",
    "    ax2.tick_params(axis='y', labelcolor='g')\n",
    "    ax2.set_title('Approval Rate by Threshold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall by threshold\n",
    "    ax3.plot(threshold_df['threshold'], threshold_df['precision'], 'r-', linewidth=2, label='Precision')\n",
    "    ax3.plot(threshold_df['threshold'], threshold_df['recall'], 'orange', linewidth=2, label='Recall')\n",
    "    ax3.axvline(best_threshold_row['threshold'], color='b', linestyle='--', alpha=0.7)\n",
    "    ax3.set_xlabel('Threshold')\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.set_title('Precision-Recall by Threshold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Default rate by threshold\n",
    "    ax4.plot(threshold_df['threshold'], threshold_df['default_rate'], 'purple', linewidth=2)\n",
    "    ax4.axvline(best_threshold_row['threshold'], color='r', linestyle='--', alpha=0.7)\n",
    "    ax4.set_xlabel('Threshold')\n",
    "    ax4.set_ylabel('Default Rate')\n",
    "    ax4.set_title('Default Rate by Threshold')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No model results available for business impact analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results is not None and best_model_name:\n",
    "    print(\"=\"*80)\n",
    "    print(\"SAVE MODEL AND RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    \n",
    "    # Prepare model data for saving\n",
    "    model_data = {\n",
    "        'best_model_name': best_model_name,\n",
    "        'best_model': results[best_model_name]['model'],\n",
    "        'all_results': results,\n",
    "        'feature_columns': X.columns.tolist(),\n",
    "        'processed_columns': X_test_processed.columns.tolist(),\n",
    "        'categorical_encoders': categorical_encoders,\n",
    "        'numerical_imputer_values': numerical_imputer_values,\n",
    "        'scaler': scaler,\n",
    "        'best_metrics': results[best_model_name]['metrics'],\n",
    "        'business_analysis': {\n",
    "            'net_financial_impact': net_financial_impact if 'net_financial_impact' in locals() else 0,\n",
    "            'optimal_threshold': best_threshold_row['threshold'] if 'best_threshold_row' in locals() else 0.5,\n",
    "            'avg_loan_amount': avg_loan_amount,\n",
    "            'default_rate': default_rate if 'default_rate' in locals() else 0\n",
    "        },\n",
    "        'model_metadata': {\n",
    "            'training_date': datetime.now().isoformat(),\n",
    "            'dataset_shape': df.shape,\n",
    "            'target_distribution': y.value_counts().to_dict(),\n",
    "            'model_version': '1.0.0'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Save the complete model package\n",
    "        model_filename = f'../models/credit_risk_model_{best_model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "        joblib.dump(model_data, model_filename)\n",
    "        \n",
    "        # Also save as default model\n",
    "        default_filename = '../models/credit_risk_model_best.pkl'\n",
    "        joblib.dump(model_data, default_filename)\n",
    "        \n",
    "        print(f\"‚úÖ Model saved successfully!\")\n",
    "        print(f\"   Best model: {best_model_name}\")\n",
    "        print(f\"   ROC-AUC: {results[best_model_name]['metrics']['roc_auc']:.3f}\")\n",
    "        print(f\"   F1-Score: {results[best_model_name]['metrics']['f1']:.3f}\")\n",
    "        print(f\"   Files saved:\")\n",
    "        print(f\"     - {model_filename}\")\n",
    "        print(f\"     - {default_filename}\")\n",
    "        \n",
    "        # Save feature importance if available\n",
    "        if 'feature_importance' in locals():\n",
    "            feature_importance.to_csv('../models/feature_importance.csv', index=False)\n",
    "            print(f\"     - ../models/feature_importance.csv\")\n",
    "        \n",
    "        # Save model comparison results\n",
    "        comparison_df.to_csv('../models/model_comparison.csv')\n",
    "        print(f\"     - ../models/model_comparison.csv\")\n",
    "        \n",
    "        # Save training summary\n",
    "        training_summary = {\n",
    "            'best_model': best_model_name,\n",
    "            'best_metrics': results[best_model_name]['metrics'],\n",
    "            'all_models_comparison': comparison_df.to_dict(),\n",
    "            'business_impact': {\n",
    "                'net_financial_impact': net_financial_impact if 'net_financial_impact' in locals() else 0,\n",
    "                'optimal_threshold': best_threshold_row['threshold'] if 'best_threshold_row' in locals() else 0.5\n",
    "            },\n",
    "            'feature_count': len(X.columns),\n",
    "            'sample_size': len(df),\n",
    "            'training_date': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open('../models/training_summary.json', 'w') as f:\n",
    "            json.dump(training_summary, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"     - ../models/training_summary.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving model: {e}\")\n",
    "    \n",
    "    print(f\"\\nüéâ MODEL DEVELOPMENT COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‚úÖ Best model: {best_model_name}\")\n",
    "    print(f\"‚úÖ Performance: ROC-AUC = {results[best_model_name]['metrics']['roc_auc']:.3f}\")\n",
    "    print(f\"‚úÖ Business impact: ${net_financial_impact:,.0f}\" if 'net_financial_impact' in locals() else \"\")\n",
    "    print(f\"‚úÖ Model saved and ready for deployment\")\n",
    "    \n",
    "    print(f\"\\nüìã NEXT STEPS:\")\n",
    "    print(f\"   1. Deploy model to Streamlit dashboard\")\n",
    "    print(f\"   2. Set up model monitoring and retraining pipeline\")\n",
    "    print(f\"   3. Create API endpoints for real-time predictions\")\n",
    "    print(f\"   4. Deploy to production environment\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No model results available for saving\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (credit-risk-env)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}